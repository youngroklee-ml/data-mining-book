[
["index.html", "데이터마이닝 with R 개요", " 데이터마이닝 with R 전치혁, 이혜선, 이종석, 이영록 2018-10-06 개요 본 사이트는 전치혁 교수님의 책 을 기반으로 한 R 예제를 제공할 목적으로 만들어졌으며, 지속적으로 업데이트될 예정입니다. 본 사이트의 R 예제들은 R 3.5.1 version에서 수행되었으며, R 프로그램은 CRAN에서 다운로드받아 설치할 수 있습니다. 본 사이트는 R을 이용한 데이터마이닝 수행에 초점을 두고 있으며, 예제 수행을 위해서는 기본적인 R 프로그래밍 지식이 필요합니다. R 프로그래밍에 대한 지식은 아래와 같은 자료들로부터 얻을 수 있습니다. DataCamp’s free R Tutorial: https://www.datacamp.com/courses/free-introduction-to-r Advanced R (by Hadley Wickham): https://adv-r.hadley.nz "],
["logistic-regression.html", "Chapter 1 로지스틱 회귀분석 1.1 필요 R 패키지 설치 1.2 이분 로지스틱 회귀모형 1.3 명목 로지스틱 회귀모형 1.4 서열 로지스틱 회귀모형", " Chapter 1 로지스틱 회귀분석 로지스틱 회귀분석(logistic regression)은 종속변수가 통상 2개의 범주(있음/없음, 불량/양호, 합격/불합격 등)를 다루는 모형을 지칭하나, 3개 이상의 범주를 다루기도 한다. 후자의 경우는 다시 서열형(ordinal) 데이터와 명목형(nominal) 데이터인 경우에 따라 서로 다른 모형이 사용된다. 1.1 필요 R 패키지 설치 본 장에서 필요한 R 패키지들은 아래와 같다. package version tidyverse 1.2.1 stats 3.5.1 1.2 이분 로지스틱 회귀모형 1.2.1 기본 R 스크립트 train_df &lt;- tribble( ~id, ~x1, ~x2, ~x3, ~y, 1, 0, 8, 2, &quot;우수&quot;, 2, 1, 7, 1, &quot;우수&quot;, 3, 0, 9, 0, &quot;우수&quot;, 4, 1, 6, 4, &quot;우수&quot;, 5, 1, 8, 2, &quot;우수&quot;, 6, 0, 7, 3, &quot;우수&quot;, 7, 0, 7, 0, &quot;보통&quot;, 8, 1, 6, 1, &quot;보통&quot;, 9, 0, 7, 2, &quot;보통&quot;, 10, 0, 8, 1, &quot;보통&quot;, 11, 0, 5, 2, &quot;보통&quot;, 12, 1, 8, 0, &quot;보통&quot;, 13, 0, 6, 3, &quot;보통&quot;, 14, 1, 7, 2, &quot;보통&quot;, 15, 0, 6, 1, &quot;보통&quot; ) %&gt;% mutate(y = factor(y, levels = c(&quot;보통&quot;, &quot;우수&quot;))) knitr::kable(train_df, booktabs = TRUE, align = c(&#39;r&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;), col.names = c(&#39;객체번호&#39;, &#39;아침식사여부($x_1$)&#39;, &#39;수면시간($x_2$)&#39;, &#39;서클활동시간($x_3$)&#39;, &#39;범주(y)&#39;), caption = &#39;우수/보통 학생에 대한 설문조사 결과&#39;) Table 1.1: 우수/보통 학생에 대한 설문조사 결과 객체번호 아침식사여부(\\(x_1\\)) 수면시간(\\(x_2\\)) 서클활동시간(\\(x_3\\)) 범주(y) 1 0 8 2 우수 2 1 7 1 우수 3 0 9 0 우수 4 1 6 4 우수 5 1 8 2 우수 6 0 7 3 우수 7 0 7 0 보통 8 1 6 1 보통 9 0 7 2 보통 10 0 8 1 보통 11 0 5 2 보통 12 1 8 0 보통 13 0 6 3 보통 14 1 7 2 보통 15 0 6 1 보통 Table 1.1와 같이 세 개의 독립변수 \\(x_1\\), \\(x_2\\), \\(x_3\\)와 이분형 종속변수 \\(y\\)의 관측값(보통 = 0, 우수 = 1)으로 이루어진 15개의 학습표본을 train_df라는 data frame에 저장한다. 아래와 같이 glm 함수를 이용하여 로지스틱 회귀모형을 간편하게 추정할 수 있다. glm_fit &lt;- glm(y ~ x1 + x2 + x3, family = binomial(link = &quot;logit&quot;), data = train_df) knitr::kable( glm_fit %&gt;% broom::tidy(), booktabs = TRUE, caption = &#39;Table \\\\1.1에 대한 Logistic Regression 결과&#39; ) Table 1.2: Table 1.1에 대한 Logistic Regression 결과 term estimate std.error statistic p.value (Intercept) -30.510836 18.018256 -1.693329 0.0903929 x1 2.031278 1.983692 1.023989 0.3058406 x2 3.470671 2.074978 1.672631 0.0944000 x3 2.414387 1.396372 1.729043 0.0838015 Table 1.2은 추정된 회귀계수 추정치 estmate과 그 표준오차 std.error, 표준화(standardized)된 회귀계수값 statistic (= estmate / std.error), 그리고 귀무가설 \\(H_0\\): statistic = 0 에 대한 유의확률 p.value를 보여준다. 1.2.2 회귀모형 이분 로지스틱 회귀모형은 종속변수가 2가지 범주를 취하는 경우에 사용된다. \\(N\\)개의 객체로 이루어진 학습데이터 \\(\\{(\\mathbf{x}_i, y_i)\\}_{i = 1, \\cdots, N}\\)를 아래와 같이 정의하자. \\(\\mathbf{x}_i \\in \\mathbb{R}^p\\): \\(p\\)개의 독립변수로 이루어진 벡터 (\\(\\mathbf{x}_i = [x_{i1} \\, x_{i2} \\, \\cdots \\, x_{ip}]^\\top\\)) \\(y_i\\): 0 혹은 1의 값을 갖는 이분형 지시변수 (indicator variable) \\(\\mathbf{x}_i\\) 관측값을 이용하여 \\(y_i\\)의 기대값 \\(P_i\\)을 추정하는 모형을 아래와 같이 로지스틱 함수로 정의하자. \\[\\begin{eqnarray} P_i &amp;=&amp; P(y_i = 1 \\,|\\, \\mathbf{x}_i)\\\\ &amp;=&amp; E[y_i | \\mathbf{x}_i]\\\\ &amp;=&amp; \\frac{\\exp(\\beta_0 + \\boldsymbol\\beta^\\top \\mathbf{x}_i)}{1 + \\exp(\\beta_0 + \\boldsymbol\\beta^\\top \\mathbf{x}_i)} \\tag{1.1} \\end{eqnarray}\\] 여기에서 \\(\\boldsymbol\\beta \\in \\mathbb{R}^{p}\\)는 \\(\\mathbf{x}_i\\)와 동일한 차원의 벡터이다 (\\(\\boldsymbol\\beta = [\\beta_1 \\, \\beta_2 \\, \\cdots \\, \\beta_p]^\\top\\)). 식 (1.1)는 모든 \\(\\mathbf{x}_i\\)값에 대해 0에서 1 사이의 값을 갖게 되므로 각 범주에 속할 확률을 추정하는 데 적합한 반면, 변수 \\(\\mathbf{x}\\) 및 계수들에 대해 선형이 아니므로 추정이 어렵다. 그러나 아래와 같이 로짓(logit) 변환을 통해 선형회귀식으로 변환할 수 있다. \\[\\begin{eqnarray} logit(P_i) &amp;=&amp; \\ln \\left[ \\frac{P_i}{1 - P_i} \\right]\\\\ &amp;=&amp; \\ln(\\exp(\\beta_0 + \\boldsymbol\\beta^\\top \\mathbf{x}_i))\\\\ &amp;=&amp; \\beta_0 + \\boldsymbol\\beta^\\top \\mathbf{x}_i \\tag{1.2} \\end{eqnarray}\\] 식 (1.2)에서 확률 \\(P_i\\)는 직접적으로 관측되는 것이 아니고 0 또는 1을 갖는 \\(y_i\\)가 관측되므로, \\(P_i\\)를 일종의 잠재변수(latent variable)로 해석할 수 있다. \\[\\begin{equation} y_i = \\begin{cases} 1 &amp; \\text{ if } \\beta_0 + \\boldsymbol\\beta^\\top \\mathbf{x}_i + \\varepsilon_i &gt; 0 \\\\ 0 &amp; \\text{ otherwise } \\end{cases} \\tag{1.3} \\end{equation}\\] 식 (1.3)에서 \\(\\varepsilon_i\\)는 표준로지스틱분포(standard logistic distribution)을 따른다. 1.2.3 회귀계수 추정 로지스틱 모형에서 회귀계수의 추정을 위해서 주로 최우추정법(maximum likelihood estimation)이 사용된다. \\(N\\)개의 객체로 이루어진 학습데이터에 대해 우도함수는 다음과 같다. \\[\\begin{equation*} L = \\prod_{i = 1}^{N} P_i^{y_i} (1 - P_i)^{1 - y_i} \\end{equation*}\\] 그리고 우도함수에 자연로그를 취하면 아래와 같이 전개된다. \\[\\begin{eqnarray} \\log L &amp;=&amp; \\sum_{i = 1}^{N} y_i \\log P_i + \\sum_{i = 1}^{N} (1 - y_i) \\log (1 - P_i)\\\\ &amp;=&amp; \\sum_{i = 1}^{N} y_i \\log \\frac{P_i}{1 - P_i} + \\sum_{i = 1}^{N} \\log (1 - P_i)\\\\ &amp;=&amp; \\sum_{i = 1}^{N} y_i (\\beta_0 + \\boldsymbol\\beta^\\top \\mathbf{x}_i) - \\sum_{i = 1}^{N} \\log (1 + \\exp (\\beta_0 + \\boldsymbol\\beta^\\top \\mathbf{x}_i) )\\\\ &amp;=&amp; \\sum_{i = 1}^{N} y_i \\left(\\beta_0 + \\sum_{j = 1}^{p} \\beta_j x_{ij} \\right) - \\sum_{i = 1}^{N} \\log \\left(1 + \\exp\\left(\\beta_0 + \\sum_{j = 1}^{p} \\beta_j x_{ij}\\right)\\right) \\tag{1.4} \\end{eqnarray}\\] 식 (eq:binary-logistic-reg-loglik)을 각 회귀계수 \\(\\beta_0, \\beta_1, \\cdots, \\beta_p\\)에 대해 편미분하여 최적해를 얻는다. 이를 위해 주로 뉴턴-랩슨 알고리즘(Newton-Raphson algorithm)이나 quasi-Newton 알고리즘이 사용되나 (전치혁 2012), 본 장에서는 우선 안정성은 떨어지지만 보다 간편한 방법으로 경사하강법(gradient descent)을 소개한다. 1.2.3.1 경사하강법 식 (1.1)과 \\(P(y_i = 0 \\,|\\, \\mathbf{x}_i) = 1 - P_i\\), 그리고 \\[\\begin{equation*} \\frac{\\exp(z)}{1 + \\exp(z)} = \\frac{1}{1 + \\exp(-z)} \\end{equaiton*} 임을 고려하면 아래와 같이 범주확률모형을 정의할 수 있다. \\begin{equation*} P(y = y_i \\,|\\, \\mathbf{x}_i, \\beta_0, \\boldsymbol\\beta) = \\frac{1}{1 + \\exp\\left((1 - 2y_i)(\\beta_0 + \\sum_{j = 1}^{p} \\beta_j x_{ij})\\right)} \\end{equation*}\\] 이에 따라 로그우도함수 (1.4)는 아래와 같이 정리된다. \\[\\begin{equation*} \\log \\prod_{i = 1}^{N} P(y = y_i \\,|\\, \\mathbf{x}_i, \\beta_0, \\boldsymbol\\beta) = - \\sum_{i = 1}^{N} \\log \\left(1 + \\exp\\left((1 - 2y_i)(\\beta_0 + \\sum_{j = 1}^{p} \\beta_j x_{ij})\\right)\\right) \\end{equation*}\\] 위 로그우도함수를 최대화하는 문제는 아래 함수를 최소화하는 문제와 동일하다. \\[\\begin{equation} f(\\beta_0, \\boldsymbol\\beta) = \\sum_{i = 1}^{N} \\log \\left(1 + \\exp\\left((1 - 2y_i)(\\beta_0 + \\sum_{j = 1}^{p} \\beta_j x_{ij})\\right)\\right) \\tag{1.5} \\end{equation}\\] 경사하강법에 따라 아래의 과정을 통해 회귀계수를 추정할 수 있다. 임의의 값으로 \\(\\beta_0, \\beta_1, \\cdots, \\beta_j\\)의 초기 추정값을 설정한다. 식 (1.5)을 각 회귀변수에 대해 편미분한 미분값을 구한다. 2의 값에 학습률(step size)을 곱한 만큼 회귀계수 추정값을 이동시킨다. 방향은 미분값의 반대방향. 수렴할 때까지 2-3의 과정을 반복한다. 여기에서 식 (1.5)의 각 회귀변수에 대한 편미분식은 아래와 같다. \\[\\begin{eqnarray*} \\frac{\\partial f}{\\partial \\beta_0} &amp;=&amp; \\sum_{i = 1}^{N} (1 - 2y_i) \\frac{\\exp\\left((1 - 2y_i)(\\beta_0 + \\sum_{j = 1}^{p} \\beta_j x_{ij})\\right)}{1 + \\exp\\left((1 - 2y_i)(\\beta_0 + \\sum_{j = 1}^{p} \\beta_j x_{ij})\\right)}\\\\ &amp;=&amp; \\sum_{i = 1}^{N} \\frac{1 - 2y_i}{1 + \\exp\\left((2y_i - 1)(\\beta_0 + \\sum_{j = 1}^{p} \\beta_j x_{ij})\\right)} \\end{eqnarray*}\\] \\[\\begin{eqnarray*} \\frac{\\partial f}{\\partial \\beta_j} &amp;=&amp; \\sum_{i = 1}^{N} (1 - 2y_i)x_{ij} \\frac{\\exp\\left((1 - 2y_i)(\\beta_0 + \\sum_{j = 1}^{p} \\beta_j x_{ij})\\right)}{1 + \\exp\\left((1 - 2y_i)(\\beta_0 + \\sum_{j = 1}^{p} \\beta_j x_{ij})\\right)}\\\\ &amp;=&amp; \\sum_{i = 1}^{N} \\frac{(1 - 2y_i)x_{ij}}{1 + \\exp\\left((2y_i - 1)(\\beta_0 + \\sum_{j = 1}^{p} \\beta_j x_{ij})\\right)} \\end{eqnarray*}\\] 따라서, 회귀계수 추정값을 이동시키는 함수 update_beta를 아래와 같이 구현할 수 있다. update_beta &lt;- function(x, y, beta0 = 0, beta = rep(0, dim(x)[2]), alpha = 0.01) { # 변미분식의 분모 denominator &lt;- 1 + exp((2 * y - 1) * (beta0 + (x %*% beta))) # intercept 이동량 계산 beta0_numerator &lt;- 1 - 2 * y beta0_update = sum(beta0_numerator / denominator) # intercept 외 회귀계수 이동량 계산 beta_numerator &lt;- sweep(x, MARGIN = 1, STATS = 1 - 2 * y, FUN = &quot;*&quot;) beta_update = apply(beta_numerator, MARGIN = 2, function(x) sum(x / denominator)) # 회귀계수 이동 beta0 &lt;- beta0 - alpha * beta0_update beta &lt;- beta - alpha * beta_update return(list(beta0 = beta0, beta = beta)) } 위의 함수를 이용하여 아래 estimate_beta처엄 수렴할 때까지 회귀계수 추정값을 계속 이동시킨다. 본 경사하강법은 학습률 파라미터 alpha값에 따라 민감한 단점이 있으며, 특히 alpha값을 크게 설정할 경우에는 추정값이 수렴하지 않고 오히려 실제값에서 계속 멀어지는 현상이 발생하기도 한다. 이러한 단점을 보완하기 위한 여러 방법이 있으나, 본 장에서 자세한 설명은 생략하기로 한다. caltculate_loglik &lt;- function(x, y, beta0 = 0, beta = rep(0, dim(x)[2])) { sum(y * (beta0 + (x %*% beta))) - sum(log(1 + exp(beta0 + (x %*% beta)))) } estimate_beta &lt;- function(x, y, beta0 = 0, beta = rep(0, dim(x)[2]), alpha = 0.01, conv_threshold = 1e-5, max_iter = 1e+5) { new_beta0 &lt;- beta0 new_beta &lt;- beta conv &lt;- FALSE i_iter &lt;- 0 while(i_iter &lt; max_iter) { res &lt;- update_beta(x, y, new_beta0, new_beta, alpha) if(abs(caltculate_loglik(x, y, beta0, beta) - caltculate_loglik(x, y, res$beta0, res$beta)) &lt; conv_threshold) { conv &lt;- TRUE break } new_beta0 &lt;- res$beta0 new_beta &lt;- res$beta i_iter &lt;- i_iter + 1 } return(list(conv = conv, beta0 = new_beta0, beta = new_beta)) } 위에서 정의한 함수를 이용하여 Table 1.1의 학습표본에 대한 로지스틱 회귀모형을 추정해보자. res &lt;- estimate_beta(train_df[, c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;)] %&gt;% as.matrix(), train_df$y %&gt;% as.numeric() - 1, alpha = 0.015, conv_threshold = 1e-6) print(res) ## $conv ## [1] FALSE ## ## $beta0 ## [1] -30.39189 ## ## $beta ## x1 x2 x3 ## 2.022808 3.457019 2.405969 위 회귀계수 추정값은 R 함수 glm을 이용한 추정값(Table 1.2)과 유사함을 볼 수 있다. 1.2.3.2 반복재가중최소제곱법 R의 glm 함수는 반복재가중최소제곱법(iteratively rewighted least squares; IRLS 혹은 IWLS)을 사용한다. 이는 선형회귀식 \\[\\begin{equation*} logit(y_i) = \\beta_0 + \\boldsymbol\\beta^\\top \\mathbf{x}_i + \\varepsilon_i \\end{equation*}\\] 을 추정하는 방법인데, 여기에서 \\(y_i\\)는 0 혹은 1이므로, \\(logit(y_i)\\)는 \\(-\\infty\\) 혹은 \\(\\infty\\)가 되어 회귀식을 추정할 수 없다. 따라서, 식 (1.1)에 설명된 로지스틱 함수 \\[\\begin{equation*} P = \\frac{\\exp(\\beta_0 + \\boldsymbol\\beta^\\top \\mathbf{x})}{1 + \\exp(\\beta_0 + \\boldsymbol\\beta^\\top \\mathbf{x})} \\end{equation*}\\] 와 테일러 급수(Taylor series)를 이용하여 \\(logit(y)\\)에 대한 근사함수를 아래와 같이 얻는다. \\[\\begin{eqnarray*} g(y) &amp;=&amp; logit(P) + (y - P) \\frac{\\partial logit(P)}{\\partial P}\\\\ &amp;=&amp; \\log \\frac{P}{1 - P} + (y - P) \\left( \\frac{1}{P} + \\frac{1}{1 - P} \\right) \\end{eqnarray*}\\] 그리고 아래 선형회귀식을 추정한다. \\[\\begin{equation*} g(y_i) = \\beta_0 + \\boldsymbol\\beta^\\top \\mathbf{x}_i + \\varepsilon_i \\end{equation*}\\] 여기에서 오차항 \\(\\varepsilon_i\\)의 분산은 추정된 확률 \\(P_i\\)에 따라 다르므로, 통상적 최소자승법(ordinary least squares; OLS) 대신 오차항의 분산이 동일해지도록 객체마다 가중치를 부여하는 가중최소자승법(weighted least squares; WLS)을 사용한다. 로지스틱 회귀모형에서 각 객체의 가중치는 \\[\\begin{equation*} w_i = P_i (1 - P_i) \\end{equation*}\\] 가중치와 회귀계수 추정값은 상호 영향을 미치므로, 수렴할 때까지 반복적으로 가중치와 회귀계수 추정값을 변화시키면서 최종 추정값을 찾아가는 방법이다. 우선 회귀계수 추정값이 주어졌을 때 각 객체에 대한 확률값 \\(P_i\\)와 가중치 \\(w_i\\)를 구하는 함수 calculate_weight를 아래와 같이 구현해보자. calculate_weight &lt;- function(x, beta0 = 0, beta = rep(0, dim(x)[2])) { # 각 객체의 y값이 1일 확률 P &lt;- (1 + exp(- beta0 - (x %*% beta)))^(-1) %&gt;% drop() # 가중치 계산 w &lt;- P * (1 - P) return(list(P = P, w = w)) } 그리고 확률추정값과 가중치가 주어졌을 때 회귀계수를 구하는 함수 calculate_beta를 아래와 같이 구현해보자. 여기서 회귀계수를 구하는 부분은 R의 선형회귀분석함수 lm을 사용한다. calculate_beta &lt;- function(x, y, P, w) { # 추정확률값이 0 이나 1인 경우 여전히 logit 함수가 정의되지 않으므로 회귀계수 결정에서 제외 logit_derivative &lt;- 1/P + 1/(1 - P) is_good &lt;- !is.nan(logit_derivative) # 모든 객체에 대한 추정확률이 0 이나 1인 경우 회귀계수 추정 불가능 if(all(!is_good)) return(NULL) # 테일러 급수 계산 g_y &lt;- log(P[is_good]) - log(1 - P[is_good]) + (y[is_good] - P[is_good]) * logit_derivative # 가중치최소자승법을 이용한 추정 df &lt;- bind_cols(as_tibble(x) %&gt;% `colnames&lt;-`(colnames(x)), tibble(g_y = g_y)) lm(g_y ~ ., data = df, subset = is_good, weights = w) } 위에서 정의한 두 함수 calculate_weight과 calculate_beta를 반복적으로 사용하여 Table 1.1의 학습표본에 대한 로지스틱 회귀모형을 추정해보자. 모든 객체의 가중치 변화량이 1/10000 보다 작을 경우 모형추정이 수렴한 것으로 간주하도록 하자. X &lt;- train_df[, c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;)] %&gt;% as.matrix() y &lt;- train_df$y %&gt;% as.numeric() - 1 weight &lt;- calculate_weight(X) for(i in 1:10) { wls_fit &lt;- calculate_beta(X, y, weight$P, weight$w) if(is.null(wls_fit)) {break} new_weight &lt;- calculate_weight(x = X, beta0 = coef(wls_fit)[1], beta = coef(wls_fit)[-1]) if(max(abs(new_weight$w - weight$w)) &lt; 1e-4) {break} weight &lt;- new_weight } coef(wls_fit) ## (Intercept) x1 x2 x3 ## -30.510837 2.031278 3.470671 2.414387 위 스크립트를 실행시킨 결과 7번째 반복수행에서 결과가 수렴하였으며, 해당 결과는 glm 함수를 사용하였을 때의 결과 (Table 1.2)과 매우 근사함을 확인할 수 있다. 1.3 명목 로지스틱 회귀모형 1.4 서열 로지스틱 회귀모형 References "],
["da.html", "Chapter 2 판별분석 2.1 개요 2.2 필요 R 패키지 설치 2.3 피셔 방법 2.4 의사결정론에 의한 선형분류규칙 2.5 오분류비용을 고려한 분류규칙 2.6 이차판별분석 2.7 세 범주 이상의 분류", " Chapter 2 판별분석 2.1 개요 판별분석(discriminant analysis)은 범주들을 가장 잘 구별하는 변수들의 하나 또는 다수의 함수를 도출하여 이를 기반으로 분류규칙을 제시한다. 본 장에서는 변수의 분포에 대한 가정이 필요 없는 피셔(Fisher) 방법과 다변량 정규분포를 가정하는 선형 및 비선형 판별분석을 설명한다. 2.2 필요 R 패키지 설치 본 장에서 필요한 R 패키지들은 아래와 같다. package version tidyverse 1.2.1 MASS 7.3-50 mvtnorm 1.0-8 2.3 피셔 방법 2.3.1 기본 R 스크립트 train_df &lt;- tibble( id = c(1:9), x1 = c(5, 4, 7, 8, 3, 2, 6, 9, 5), x2 = c(7, 3, 8, 6, 6, 5, 6, 6, 4), class = factor(c(1, 2, 2, 2, 1, 1, 1, 2, 2), levels = c(1, 2)) ) knitr::kable(train_df, booktabs = TRUE, align = c(&#39;r&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;), col.names = c(&#39;객체번호&#39;, &#39;$x_1$&#39;, &#39;$x_2$&#39;, &#39;범주&#39;), caption = &#39;판별분석 학습표본 데이터&#39;) Table 2.1: 판별분석 학습표본 데이터 객체번호 \\(x_1\\) \\(x_2\\) 범주 1 5 7 1 2 4 3 2 3 7 8 2 4 8 6 2 5 3 6 1 6 2 5 1 7 6 6 1 8 9 6 2 9 5 4 2 Table 2.1와 같이 두 독립변수 x1, x2와 이분형 종속변수 class의 관측값으로 이루어진 9개의 학습표본을 train_df라는 data frame에 저장한다. fisher_da &lt;- MASS::lda(class ~ x1 + x2, train_df) print(fisher_da) ## Call: ## lda(class ~ x1 + x2, data = train_df) ## ## Prior probabilities of groups: ## 1 2 ## 0.4444444 0.5555556 ## ## Group means: ## x1 x2 ## 1 4.0 6.0 ## 2 6.6 5.4 ## ## Coefficients of linear discriminants: ## LD1 ## x1 0.6850490 ## x2 -0.7003859 2.3.2 피셔 판별함수 각 객체는 변수벡터 \\(\\mathbf{x} \\in \\mathbb{R}^p\\)와 범주 \\(y \\in \\{1, 2\\}\\)로 이루어진다고 하자. 아래는 변수 \\(\\mathbf{x}\\)의 기대치와 분산-공분산행렬(varinace-covariance matrix)을 나타낸다. \\[\\begin{eqnarray*} \\boldsymbol\\mu_1 = E(\\mathbf{x} | y = 1)\\\\ \\boldsymbol\\mu_2 = E(\\mathbf{x} | y = 2)\\\\ \\boldsymbol\\Sigma = Var(\\mathbf{x} | y = 1) = Var(\\mathbf{x} | y = 2) \\end{eqnarray*}\\] 다음과 같이 변수들의 선형조합으로 새로운 변수 \\(z\\)를 형성하는 함수를 피셔 판별함수(Fisher’s discriminant function)라 한다. \\[\\begin{equation} z = \\mathbf{w}^\\top \\mathbf{x} \\tag{2.1} \\end{equation}\\] 여기서 계수벡터 \\(\\mathbf{w} \\in \\mathbb{R}^p\\)는 통상 아래와 같이 변수 \\(z\\)의 범주간 평균 차이 대 변수 \\(z\\)의 분산의 비율을 최대화하는 것으로 결정한다. \\[\\begin{equation} {\\arg\\!\\min}_{\\mathbf{w}} \\frac{\\mathbf{w}^\\top ( \\boldsymbol\\mu_1 - \\boldsymbol\\mu_2 )}{\\mathbf{w}^\\top \\boldsymbol\\Sigma \\mathbf{w}} \\tag{2.2} \\end{equation}\\] 위 식 (2.2)의 해는 \\[\\begin{equation*} \\mathbf{w} \\propto \\boldsymbol\\Sigma^{-1}(\\boldsymbol\\mu_1 - \\boldsymbol\\mu_2) \\end{equation*}\\] 의 조건을 만족하며, 편의상 비례상수를 1로 두면 아래와 같은 해가 얻어진다. \\[\\begin{equation} \\mathbf{w} = \\boldsymbol\\Sigma^{-1}(\\boldsymbol\\mu_1 - \\boldsymbol\\mu_2) \\tag{2.3} \\end{equation}\\] 실제 모집단의 평균 및 분산을 알지 못하는 경우, 학습표본으로부터 \\(\\boldsymbol\\mu_1, \\boldsymbol\\mu_2, \\boldsymbol\\Sigma\\)의 추정치를 얻어 식 (2.3)에 대입하는 방식으로 판별계수를 추정한다. 자세한 내용은 교재 (전치혁 2012) 참조. Table 2.1에 주어진 학습표본을 이용하여 피셔 판별함수를 구해보도록 하자. 우선 각 범주별 평균벡터 \\(\\hat{\\boldsymbol\\mu}_1, \\hat{\\boldsymbol\\mu}_2\\)를 아래와 같이 구한다. mu_hat &lt;- train_df %&gt;% group_by(class) %&gt;% summarize(x1 = mean(x1), x2 = mean(x2)) %&gt;% arrange(class) print(mu_hat) ## # A tibble: 2 x 3 ## class x1 x2 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 6 ## 2 2 6.6 5.4 또한 범주별 표본 분산-공분산행렬 \\(\\mathbf{S}_1, \\mathbf{S}_2\\)를 다음과 같이 구한다. 리스트 S_within_group의 첫번째 원소는 범주 1의 분산-공분산행렬 \\(\\mathbf{S}_1\\), 두번째 원소는 범주 2의 분산-공분산행렬 \\(\\mathbf{S}_2\\)를 나타낸다. S_within_group &lt;- lapply( unique(train_df$class) %&gt;% sort(), function(x) { train_df %&gt;% filter(class == x) %&gt;% select(x1, x2) %&gt;% var() } ) print(S_within_group) ## [[1]] ## x1 x2 ## x1 3.333333 1.0000000 ## x2 1.000000 0.6666667 ## ## [[2]] ## x1 x2 ## x1 4.30 2.95 ## x2 2.95 3.80 위에서 얻은 범주별 표본 분산-공분산행렬을 이용하여 합동 분산-공분산행렬을 아래와 같이 추정한다. \\[\\begin{equation*} \\hat{\\boldsymbol\\Sigma} = \\mathbf{S}_p = \\frac{(n_1 - 1)\\mathbf{S}_1 + (n_2 - 1)\\mathbf{S}_2}{n_1 + n_2 - 2} \\end{equation*}\\] 이 때 \\(n_1, n_2\\)는 각각 범주 1, 2에 속한 학습표본 객체의 수를 나타낸다. 아래 R 스크립트에서는 임의의 범주 표본수 벡터 n과 범주별 표본 분산-공분산행렬 리스트 S에 대해 합동 분산-공분산행렬을 구하는 함수 pooled_variance를 정의하고, 주어진 학습표본에 대한 입력값을 대입하여 합동 분산-공분산행렬 추정치 Sigma_hat을 구한다. pooled_variance &lt;- function(n, S) { lapply(1:length(n), function(i) (n[i] - 1)*S[[i]]) %&gt;% Reduce(`+`, .) %&gt;% `/`(sum(n) - length(n)) } n_obs &lt;- train_df %&gt;% group_by(class) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(pi = n / sum(n)) %&gt;% arrange(class) Sigma_hat &lt;- pooled_variance(n_obs$n, S_within_group) print(Sigma_hat) ## x1 x2 ## x1 3.885714 2.114286 ## x2 2.114286 2.457143 위에서 구한 추정치들을 이용하여 아래와 같이 판별함수 계수 추정치 \\(\\hat{\\mathbf{w}}\\)를 구한다. \\[\\begin{equation*} \\hat{\\mathbf{w}} = \\hat{\\boldsymbol\\Sigma}^{-1}(\\hat{\\boldsymbol\\mu}_1 - \\hat{\\boldsymbol\\mu}_2) \\end{equation*}\\] w_hat &lt;- solve(Sigma_hat) %*% t(mu_hat[1, c(&#39;x1&#39;, &#39;x2&#39;)] - mu_hat[2, c(&#39;x1&#39;, &#39;x2&#39;)]) print(w_hat) ## [,1] ## x1 -1.508039 ## x2 1.541801 2.3.3 분류 규칙 피셔 판별함수에 따른 분류 경계값은 학습표본에 대한 판별함수값의 평균으로 아래와 같이 구할 수 있다. \\[\\begin{equation*} \\bar{z} = \\frac{1}{N} \\sum_i^N \\hat{\\mathbf{w}}^\\top \\mathbf{x}_i \\end{equation*}\\] z_mean &lt;- t(w_hat) %*% (train_df %&gt;% select(x1, x2) %&gt;% colMeans()) %&gt;% drop() print(z_mean) ## [1] 0.526438 위 결과를 통해, 분류규칙은 다음과 같이 주어진다. \\(\\hat{\\mathbf{w}}^\\top \\mathbf{x} \\ge \\bar{z}\\) 이면, \\(\\mathbf{x}\\)를 범주 1로 분류 \\(\\hat{\\mathbf{w}}^\\top \\mathbf{x} &lt; \\bar{z}\\) 이면, \\(\\mathbf{x}\\)를 범주 2로 분류 train_prediction_df &lt;- train_df %&gt;% mutate( z = w_hat[1]*x1 + w_hat[2]*x2, predicted_class = factor(if_else(z &gt;= z_mean, 1, 2), levels = c(1, 2)) ) knitr::kable(train_prediction_df, booktabs = TRUE, align = c(&#39;r&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;), col.names = c(&#39;객체번호&#39;, &#39;$x_1$&#39;, &#39;$x_2$&#39;, &#39;실제범주&#39;, &#39;$z$&#39;, &#39;추정범주&#39;), caption = &#39;학습표본에 대한 피셔 분류 결과&#39;) Table 2.2: 학습표본에 대한 피셔 분류 결과 객체번호 \\(x_1\\) \\(x_2\\) 실제범주 \\(z\\) 추정범주 1 5 7 1 3.2524116 1 2 4 3 2 -1.4067524 2 3 7 8 2 1.7781350 1 4 8 6 2 -2.8135048 2 5 3 6 1 4.7266881 1 6 2 5 1 4.6929260 1 7 6 6 1 0.2025723 2 8 9 6 2 -4.3215434 2 9 5 4 2 -1.3729904 2 위 결과 객체 3, 7가 오분류된다. 2.3.4 R 패키지를 이용한 분류규칙 도출 패키지 MASS내의 함수 lda 수행 시 얻어지는 판별계수 \\(\\hat{\\mathbf{w}}\\)는 위 결과와는 사뭇 다른데, lda 함수의 경우 아래와 같이 1) 제약식을 포함하여 비례계수를 구하기 때문에 계수의 크기가 달라지며, 2) 목적함수를 최소화하는 대신 최대화하는 값을 찾기 때문에 부호가 달라진다. \\[\\begin{equation*} \\begin{split} \\max \\text{ } &amp; \\mathbf{w}^\\top ( \\boldsymbol\\mu_1 - \\boldsymbol\\mu_2 )\\\\ \\text{s.t. } &amp; \\mathbf{w}^\\top \\boldsymbol\\Sigma \\mathbf{w} = 1 \\end{split} \\end{equation*}\\] 이에 따른 lda 함수의 계수 추정 결과는 아래와 같다. fisher_da &lt;- MASS::lda(class ~ x1 + x2, train_df) w_hat_lda &lt;- fisher_da$scaling print(w_hat_lda) ## LD1 ## x1 0.6850490 ## x2 -0.7003859 z_mean_lda &lt;- t(fisher_da$scaling) %*% (train_df %&gt;% select(x1, x2) %&gt;% colMeans()) %&gt;% drop() print(z_mean_lda) ## LD1 ## -0.2391423 위 결과는 아래와 같은 계산을 통해 앞 장에서 보았던 결과와 동일한 분류 경계식으로 표현될 수 있음을 볼 수 있다. scale_adjust &lt;- t(w_hat) %*% Sigma_hat %*% w_hat %&gt;% drop() %&gt;% sqrt() sign_adjust &lt;- -1 w_hat &lt;- w_hat_lda * scale_adjust * sign_adjust print(w_hat) ## LD1 ## x1 -1.508039 ## x2 1.541801 z_mean &lt;- z_mean_lda * scale_adjust * sign_adjust print(z_mean) ## LD1 ## 0.526438 아래 스크립트는 위 lda 함수로부터의 경계식 추정을 기반으로 아래 수식값을 계산한다. \\[\\begin{equation*} \\hat{\\mathbf{w}}^\\top \\mathbf{x} - \\bar{z} \\end{equation*}\\] predict(fisher_da, train_df)$x ## LD1 ## 1 -1.2383140 ## 2 0.8781805 ## 3 -0.5686020 ## 4 1.5172187 ## 5 -1.9080261 ## 6 -1.8926892 ## 7 0.1471208 ## 8 2.2022677 ## 9 0.8628436 피셔 분류규칙에 따라 해당 값이 0보다 작으면 범주 1, 0보다 크면 범주 2로 분류한다. train_df %&gt;% mutate( centered_z = predict(fisher_da, .)$x, predicted_class = factor(if_else(centered_z &lt;= 0, 1, 2), levels = c(1, 2)) ) %&gt;% knitr::kable(booktabs = TRUE, align = c(&#39;r&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;), col.names = c(&#39;객체번호&#39;, &#39;$x_1$&#39;, &#39;$x_2$&#39;, &#39;실제범주&#39;, &#39;$z - \\\\bar{z}$&#39;, &#39;추정범주&#39;), caption = &#39;학습표본에 대한 피셔 분류 결과 - `MASS::lda` 분류 경계식 기준&#39;) Table 2.3: 학습표본에 대한 피셔 분류 결과 - MASS::lda 분류 경계식 기준 객체번호 \\(x_1\\) \\(x_2\\) 실제범주 \\(z - \\bar{z}\\) 추정범주 1 5 7 1 -1.2383140 1 2 4 3 2 0.8781805 2 3 7 8 2 -0.5686020 1 4 8 6 2 1.5172187 2 5 3 6 1 -1.9080261 1 6 2 5 1 -1.8926892 1 7 6 6 1 0.1471208 2 8 9 6 2 2.2022677 2 9 5 4 2 0.8628436 2 Table 2.3는 Table 2.2와 동일한 범주 추정 결과를 보인다. 2.4 의사결정론에 의한 선형분류규칙 다음과 같이 객체가 각 범주에 속할 사전확률과 각 범주 내에서의 분류변수의 확률밀도함수에 대한 기호를 정의한다. \\(\\pi_k\\): 임의의 객체가 범주 \\(k\\)에 속할 사전확률 \\(f_k(\\mathbf{x})\\): 범주 \\(k\\)에 대한 변수의 확률밀도함수 이 때 통상적으로 \\(\\mathbf{x}\\)는 다변량 정규분포를 따르는 것으로 가정하여 아래와 같이 평균벡터 \\(\\boldsymbol\\mu_k\\)와 분산-공분산행렬 \\(\\boldsymbol\\Sigma\\)로 확률밀도함수를 정의할 수 있다. 이 때 분산-공분산행렬 \\(\\boldsymbol\\Sigma\\)는 모든 범주에 대해 동일하다고 가정한다. \\[\\begin{equation} f_k(\\mathbf{x}) = \\frac{1}{(2\\pi)^{p/2}|\\boldsymbol\\Sigma|^{1/2}} \\exp \\{ -\\frac{1}{2} \\left(\\mathbf{x} - \\boldsymbol\\mu_k\\right)^\\top \\boldsymbol\\Sigma^{-1} \\left(\\mathbf{x} - \\boldsymbol\\mu_k\\right) \\} \\tag{2.4} \\end{equation}\\] 본 장에서는 두 범주(\\(k = 1, 2\\)) 분류 문제만 다루며, 세 범주 이상에 대한 분류 문제는 뒷 장에서 추가적으로 다루기로 한다. 2.4.1 기본 R 스크립트 Table 2.1의 학습표본에 대해 선형판별분석을 적용하는 R 스크립트는 아래에 보이는 것처럼 피셔 판별함수를 구하기 위한 동일하며, prior 파라미터를 정의하지 않음으로써 \\(\\pi_1\\)과 \\(\\pi_2\\)를 학습표본의 범주 1, 2의 비율로 설정한다. lda_fit &lt;- MASS::lda(class ~ x1 + x2, train_df) print(lda_fit) ## Call: ## lda(class ~ x1 + x2, data = train_df) ## ## Prior probabilities of groups: ## 1 2 ## 0.4444444 0.5555556 ## ## Group means: ## x1 x2 ## 1 4.0 6.0 ## 2 6.6 5.4 ## ## Coefficients of linear discriminants: ## LD1 ## x1 0.6850490 ## x2 -0.7003859 2.4.2 선형판별함수 두 범주 문제에 있어서, 범주를 알지 못하는 변수 \\(\\mathbf{x}\\)에 대한 확률밀도함수는 아래와 같다. \\[\\begin{equation*} f(\\mathbf{x}) = \\pi_1 f_1(\\mathbf{x}) + \\pi_2 f_2(\\mathbf{x}) \\end{equation*}\\] 베이즈 정리(Bayes’s theorem)에 따라 변수 \\(\\mathbf{x}\\)값이 주어졌을 때 범주 \\(k\\)에 속할 사후확률(posterior)은 아래와 같이 구할 수 있다. \\[\\begin{equation} P(y = k \\, | \\, \\mathbf{x}) = \\frac{\\pi_k f_k(\\mathbf{x})}{f(\\mathbf{x})} \\tag{2.5} \\end{equation}\\] 각 범주에 대한 사후확률을 계산하여, 확률이 높은 쪽으로 범주를 추정한다. \\[\\begin{equation} \\hat{y} = \\begin{cases} 1, &amp; \\text{if } P(y = 1 \\, | \\, \\mathbf{x}) \\ge P(y = 2 \\, | \\, \\mathbf{x})\\\\ 2, &amp; \\text{otherwise} \\end{cases} \\tag{2.6} \\end{equation}\\] 이를 다시 정리하면 아래와 같다. \\[\\begin{equation*} \\hat{y} = \\begin{cases} 1, &amp; \\text{if } \\frac{f_1(\\mathbf{x})}{f_2(\\mathbf{x})} \\ge \\frac{\\pi_2}{\\pi_1}\\\\ 2, &amp; \\text{otherwise} \\end{cases} \\end{equation*}\\] 위 분류규칙에 식 (2.4)을 대입하여 정리하면 다음과 같다. 보다 자세한 내용은 교재 (전치혁 2012) 참조. \\[\\begin{equation*} \\hat{y} = \\begin{cases} 1, &amp; \\text{if } \\boldsymbol\\mu_1^\\top \\boldsymbol\\Sigma^{-1}\\mathbf{x} - \\frac{1}{2} \\boldsymbol\\mu_1^\\top \\boldsymbol\\Sigma^{-1} \\boldsymbol\\mu_1 + \\ln \\pi_1 \\ge \\boldsymbol\\mu_2^\\top \\boldsymbol\\Sigma^{-1}\\mathbf{x} - \\frac{1}{2} \\boldsymbol\\mu_2^\\top \\boldsymbol\\Sigma^{-1} \\boldsymbol\\mu_2 + \\ln \\pi_2 \\\\ 2, &amp; \\text{otherwise} \\end{cases} \\end{equation*}\\] 따라서, 각 범주에 대한 판별함수를 \\[\\begin{equation} u_k(\\mathbf{x}) = \\boldsymbol\\mu_k^\\top \\boldsymbol\\Sigma^{-1}\\mathbf{x} - \\frac{1}{2} \\boldsymbol\\mu_k^\\top \\boldsymbol\\Sigma^{-1} \\boldsymbol\\mu_k + \\ln \\pi_k \\tag{2.7} \\end{equation}\\] 라 하면, 아래와 같이 분류규칙을 정의할 수 있다. \\[\\begin{equation} \\hat{y} = \\begin{cases} 1, &amp; \\text{if } u_1(\\mathbf{x}) \\ge u_2(\\mathbf{x}) \\\\ 2, &amp; \\text{otherwise} \\end{cases} \\tag{2.8} \\end{equation}\\] Table 2.1의 학습표본에 대해 판별함수값을 계산하고 범주를 추정하면 아래와 같다. discriminant_func &lt;- function(X, mu, Sigma, pi) { Sigma_inv &lt;- solve(Sigma) (t(mu) %*% Sigma_inv %*% X %&gt;% drop()) - 0.5 * (t(mu) %*% Sigma_inv %*% mu %&gt;% drop()) + log(pi) } lda_discriminant_result_df &lt;- train_df %&gt;% mutate( u1 = discriminant_func( .[c(&quot;x1&quot;, &quot;x2&quot;)] %&gt;% t(), mu_hat[1, c(&quot;x1&quot;, &quot;x2&quot;)] %&gt;% unlist(), Sigma_hat, n_obs$pi[1] ), u2 = discriminant_func( .[c(&quot;x1&quot;, &quot;x2&quot;)] %&gt;% t(), mu_hat[2, c(&quot;x1&quot;, &quot;x2&quot;)] %&gt;% unlist(), Sigma_hat, n_obs$pi[2] ) ) %&gt;% mutate( predicted_class = factor(if_else(u1 &gt;= u2, 1, 2), levels = c(1, 2)) ) knitr::kable( lda_discriminant_result_df, booktabs = TRUE, align = rep(&#39;r&#39;, dim(lda_discriminant_result_df)[2]), col.names = c(&#39;객체번호&#39;, &#39;$x_1$&#39;, &#39;$x_2$&#39;, &#39;실제범주&#39;, &#39;$u_1(\\\\mathbf{x})$&#39;, &#39;$u_2(\\\\mathbf{x})$&#39;, &#39;추정범주&#39;), caption = &#39;학습표본에 대한 LDA 적용 결과: 판별함수값 및 추정범주&#39;) Table 2.4: 학습표본에 대한 LDA 적용 결과: 판별함수값 및 추정범주 객체번호 \\(x_1\\) \\(x_2\\) 실제범주 \\(u_1(\\mathbf{x})\\) \\(u_2(\\mathbf{x})\\) 추정범주 1 5 7 1 9.2051470 6.971538 1 2 4 3 2 -1.9363321 0.489223 2 3 7 8 2 11.0057900 10.246458 1 4 8 6 2 4.5909990 8.423307 2 5 3 6 1 7.4045039 3.696619 1 6 2 5 1 5.0411598 1.367036 1 7 6 6 1 5.7164010 6.532631 2 8 9 6 2 4.0282981 9.368644 2 9 5 4 2 0.4270119 2.818805 2 또한 식 (2.5)에 따른 사후확률과 식 (2.6)에 따른 추정범주는 아래와 같이 얻어진다. lda_posterior_result_df &lt;- train_df %&gt;% mutate( f1 = mvtnorm::dmvnorm( .[c(&quot;x1&quot;, &quot;x2&quot;)], mu_hat[1, c(&quot;x1&quot;, &quot;x2&quot;)] %&gt;% unlist(), Sigma_hat), f2 = mvtnorm::dmvnorm( .[c(&quot;x1&quot;, &quot;x2&quot;)], mu_hat[2, c(&quot;x1&quot;, &quot;x2&quot;)] %&gt;% unlist(), Sigma_hat), f = n_obs$pi[1] * f1 + n_obs$pi[2] * f2 ) %&gt;% mutate( p1 = n_obs$pi[1] * f1 / f, p2 = n_obs$pi[2] * f2 / f ) %&gt;% mutate( predicted_class = factor(if_else(p1 &gt;= p2, 1, 2), levels = c(1, 2)) ) %&gt;% select( id, x1, x2, class, p1, p2, predicted_class ) knitr::kable( lda_posterior_result_df, booktabs = TRUE, align = rep(&#39;r&#39;, dim(lda_posterior_result_df)[2]), col.names = c(&#39;객체번호&#39;, &#39;$x_1$&#39;, &#39;$x_2$&#39;, &#39;실제범주&#39;, &#39;$P(y = 1 | \\\\mathbf{x})$&#39;, &#39;$P(y = 2 | \\\\mathbf{x})$&#39;, &#39;추정범주&#39;), caption = &#39;학습표본에 대한 LDA 적용 결과: 사후확률 및 추정범주&#39;) Table 2.5: 학습표본에 대한 LDA 적용 결과: 사후확률 및 추정범주 객체번호 \\(x_1\\) \\(x_2\\) 실제범주 \\(P(y = 1 | \\mathbf{x})\\) \\(P(y = 2 | \\mathbf{x})\\) 추정범주 1 5 7 1 0.9032273 0.0967727 1 2 4 3 2 0.0812446 0.9187554 2 3 7 8 2 0.6812088 0.3187912 1 4 8 6 2 0.0212004 0.9787996 2 5 3 6 1 0.9760579 0.0239421 1 6 2 5 1 0.9752562 0.0247438 1 7 6 6 1 0.3065644 0.6934356 2 8 9 6 2 0.0047713 0.9952287 2 9 5 4 2 0.0838007 0.9161993 2 패키지 MASS내의 함수 lda를 통해 위 Table 2.5 결과를 간편하게 얻을 수 있다. lda_fit &lt;- MASS::lda(class ~ x1 + x2, train_df) train_df %&gt;% bind_cols( predict(lda_fit, train_df)$posterior %&gt;% `colnames&lt;-`(paste0(&quot;p&quot;, colnames(.))) %&gt;% as_data_frame() ) %&gt;% mutate( predicted_class = predict(lda_fit, .)$class ) ## # A tibble: 9 x 7 ## id x1 x2 class p1 p2 predicted_class ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 5 7 1 0.903 0.0968 1 ## 2 2 4 3 2 0.0812 0.919 2 ## 3 3 7 8 2 0.681 0.319 1 ## 4 4 8 6 2 0.0212 0.979 2 ## 5 5 3 6 1 0.976 0.0239 1 ## 6 6 2 5 1 0.975 0.0247 1 ## 7 7 6 6 1 0.307 0.693 2 ## 8 8 9 6 2 0.00477 0.995 2 ## 9 9 5 4 2 0.0838 0.916 2 위 결과들은 교재 (전치혁 2012)의 예제 결과와는 다소 차이가 있는데, 이는 교재에서는 사전확률을 학습표본 내 비율 대신 \\(\\pi_1 = \\pi_2 = 0.5\\)로 지정하였기 때문이다. 교재와 동일한 결과는 아래의 스크립트처럼 lda 함수 실행 시 사전확률 파리미터 prior의 값을 지정함으로써 얻을 수 있다. lda_fit_equal_prior &lt;- MASS::lda(class ~ x1 + x2, train_df, prior = c(1/2, 1/2)) train_df %&gt;% bind_cols( predict(lda_fit_equal_prior, train_df)$posterior %&gt;% `colnames&lt;-`(paste0(&quot;p&quot;, colnames(.))) %&gt;% as_data_frame() ) %&gt;% mutate( predicted_class = predict(lda_fit_equal_prior, .)$class ) %&gt;% knitr::kable( booktabs = TRUE, align = rep(&#39;r&#39;, dim(lda_posterior_result_df)[2]), col.names = c(&#39;객체번호&#39;, &#39;$x_1$&#39;, &#39;$x_2$&#39;, &#39;실제범주&#39;, &#39;$P(y = 1 | \\\\mathbf{x})$&#39;, &#39;$P(y = 2 | \\\\mathbf{x})$&#39;, &#39;추정범주&#39;), caption = &#39;학습표본에 대한 LDA 적용 결과: 사후확률 및 추정범주 (사전확률 = 0.5)&#39;) Table 2.6: 학습표본에 대한 LDA 적용 결과: 사후확률 및 추정범주 (사전확률 = 0.5) 객체번호 \\(x_1\\) \\(x_2\\) 실제범주 \\(P(y = 1 | \\mathbf{x})\\) \\(P(y = 2 | \\mathbf{x})\\) 추정범주 1 5 7 1 0.9210538 0.0789462 1 2 4 3 2 0.0995341 0.9004659 2 3 7 8 2 0.7275992 0.2724008 1 4 8 6 2 0.0263608 0.9736392 2 5 3 6 1 0.9807542 0.0192458 1 6 2 5 1 0.9801065 0.0198935 1 7 6 6 1 0.3559269 0.6440731 2 8 9 6 2 0.0059571 0.9940429 2 9 5 4 2 0.1026013 0.8973987 2 2.5 오분류비용을 고려한 분류규칙 위 Table 2.5의 객체 3, 7와 같이 선형분류함수가 모든 객체의 범주를 정확하게 추정하지 못하고 오분류가 발생하는 경우가 있다. 이 때 다음과 같이 두 종류의 오분류 비용이 있다고 가정하자. \\(C(1 \\, | \\, 2)\\): 범주 2를 1로 잘못 분류 시 초래 비용 \\(C(2 \\, | \\, 1)\\): 범주 1를 2로 잘못 분류 시 초래 비용 이 때 총 기대 오분류 비용은 다음과 같다. \\[\\begin{equation} C(1 \\, | \\, 2) \\pi_2 \\int_{\\mathbf{x} \\in R_1} f_2(\\mathbf{x}) d\\mathbf{x} + C(2 \\, | \\, 1) \\pi_1 \\int_{\\mathbf{x} \\in R_2} f_1(\\mathbf{x}) d\\mathbf{x} \\tag{2.9} \\end{equation}\\] 여기에서 \\(R_1 \\subset \\mathbb{R}^p, R_2 = \\mathbb{R}^p - R_{1}\\)는 판별함수에 의해 각각 범주 1, 2로 분류되는 판별변수 영역을 나타낸다. 즉, \\[\\begin{equation*} \\hat{y} = \\begin{cases} 1, &amp; \\text{if } \\mathbf{x} \\in R_1 \\\\ 2, &amp; \\text{otherwise} \\end{cases} \\end{equation*}\\] 식 (2.9)을 최소화하는 영역 \\(R_1, R_2\\)는 아래와 같다. \\[\\begin{eqnarray*} R_1 &amp;=&amp; \\left\\{\\mathbf{x} \\in \\mathbb{R}^p \\, : \\, \\frac{f_1(\\mathbf{x})}{f_2(\\mathbf{x})} \\ge \\frac{\\pi_2}{\\pi_1} \\left( \\frac{C(1 \\, | \\, 2)}{C(2 \\, | \\, 1)} \\right) \\right\\}\\\\ R_2 &amp;=&amp; \\left\\{\\mathbf{x} \\in \\mathbb{R}^p \\, : \\, \\frac{f_1(\\mathbf{x})}{f_2(\\mathbf{x})} &lt; \\frac{\\pi_2}{\\pi_1} \\left( \\frac{C(1 \\, | \\, 2)}{C(2 \\, | \\, 1)} \\right) \\right\\} \\end{eqnarray*}\\] 위 중 \\(R_1\\)에 대한 식을 아래와 같이 단계적으로 전개할 수 있다. \\[\\begin{eqnarray*} R_1 &amp;=&amp; \\left\\{\\mathbf{x} \\in \\mathbb{R}^p \\, : \\, \\frac{\\pi_1 f_1(\\mathbf{x})}{\\pi_2 f_2(\\mathbf{x})} \\ge \\frac{C(1 \\, | \\, 2)}{C(2 \\, | \\, 1)} \\right\\}\\\\ &amp;=&amp; \\left\\{\\mathbf{x} \\in \\mathbb{R}^p \\, : \\, \\frac{\\frac{\\pi_1 f_1(\\mathbf{x})}{\\pi_1 f_1(\\mathbf{x}) + \\pi_2 f_2(\\mathbf{x})}}{\\frac{\\pi_2 f_2(\\mathbf{x})}{\\pi_1 f_1(\\mathbf{x}) + \\pi_2 f_2(\\mathbf{x})}} \\ge \\frac{C(1 \\, | \\, 2)}{C(2 \\, | \\, 1)} \\right\\}\\\\ &amp;=&amp; \\left\\{\\mathbf{x} \\in \\mathbb{R}^p \\, : \\, \\frac{P(y = 1 \\, | \\, \\mathbf{x})}{P(y = 2 \\, | \\, \\mathbf{x})} \\ge \\frac{C(1 \\, | \\, 2)}{C(2 \\, | \\, 1)} \\right\\}\\\\ &amp;=&amp; \\left\\{\\mathbf{x} \\in \\mathbb{R}^p \\, : \\, C(2 \\, | \\, 1) P(y = 1 \\, | \\, \\mathbf{x}) \\ge C(1 \\, | \\, 2) P(y = 2 \\, | \\, \\mathbf{x}) \\right\\} \\end{eqnarray*}\\] 따라서 오분류비용을 고려한 분류규칙은 1) 사후확률에 오분류 비용을 곱한 뒤, 2) 그 값이 큰 범주로 분류하여 오분류비용을 최소화한다. Table 2.5에 오분류비용 \\(C(1 \\, | \\, 2) = 1, C(2 \\, | \\, 1) = 5\\)를 적용한 결과는 아래와 같이 구할 수 있다. lda_fit &lt;- MASS::lda(class ~ x1 + x2, train_df) misclassification_cost &lt;- c(5, 1) lda_unequal_cost_result_df &lt;- train_df %&gt;% bind_cols( predict(lda_fit, train_df)$posterior %*% diag(misclassification_cost) %&gt;% as_data_frame() %&gt;% `names&lt;-`(paste0(&quot;s&quot;, lda_fit$lev)) ) %&gt;% mutate( predicted_class = factor(if_else(s1 &gt;= s2, 1, 2), levels = c(1, 2)) ) knitr::kable( lda_unequal_cost_result_df, booktabs = TRUE, align = rep(&#39;r&#39;, dim(lda_unequal_cost_result_df)[2]), col.names = c(&#39;객체번호&#39;, &#39;$x_1$&#39;, &#39;$x_2$&#39;, &#39;실제범주&#39;, &#39;$C(2 \\\\, | \\\\, 1) P(y = 1 | \\\\mathbf{x})$&#39;, &#39;$C(1 \\\\, | \\\\, 2) P(y = 2 | \\\\mathbf{x})$&#39;, &#39;추정범주&#39;), caption = &#39;학습표본에 대한 오분류 비용을 고려한 LDA 적용 결과&#39;) Table 2.7: 학습표본에 대한 오분류 비용을 고려한 LDA 적용 결과 객체번호 \\(x_1\\) \\(x_2\\) 실제범주 \\(C(2 \\, | \\, 1) P(y = 1 | \\mathbf{x})\\) \\(C(1 \\, | \\, 2) P(y = 2 | \\mathbf{x})\\) 추정범주 1 5 7 1 4.5161363 0.0967727 1 2 4 3 2 0.4062232 0.9187554 2 3 7 8 2 3.4060438 0.3187912 1 4 8 6 2 0.1060019 0.9787996 2 5 3 6 1 4.8802897 0.0239421 1 6 2 5 1 4.8762808 0.0247438 1 7 6 6 1 1.5328222 0.6934356 1 8 9 6 2 0.0238567 0.9952287 2 9 5 4 2 0.4190033 0.9161993 2 위 Table 2.7에서 보는 바와 같이 오분류 객체는 3로, 이전 장의 Table 2.5에 비해 실제범주가 1인 객체를 더 정확하게 분류함을 확인할 수 있다. 범주 1인 객체를 범주 2로 분류할 때 발생하는 비용이 범주 2인 객체를 범주 1로 분류할 때 발생하는 비용보다 다섯 배나 크기 때문에, 오분류비용을 고려한 분류규칙은 실제 범주가 2인 객체를 범주 2로 정확하게 분류할 확률이 줄어든다 할지라도, 실제 범주가 1인 객체를 범주 1로 정확하게 분류하는 확률을 높이는 방향으로 학습된다. 2.6 이차판별분석 이차판별분석은 판별함수가 변수들에 대한 이차함수로 표현되는 경우인데, 각 범주에 대한 변수벡터 \\(\\mathbf{x}\\)가 서로 다른 분산-공분산행렬을 갖는 다변량 정규분포를 따를 때 의사결정론에 의한 분류규칙으로부터 유도된다. 2.6.1 기본 R 스크립트 Table 2.1의 학습표본에 대해 이차판별분석을 적용하는 R 스크립트는 아래에 보이는 것과 같이 MASS 패키지의 qda 함수를 사용한다. qda_fit &lt;- MASS::qda(class ~ x1 + x2, train_df) print(qda_fit) ## Call: ## qda(class ~ x1 + x2, data = train_df) ## ## Prior probabilities of groups: ## 1 2 ## 0.4444444 0.5555556 ## ## Group means: ## x1 x2 ## 1 4.0 6.0 ## 2 6.6 5.4 2.6.2 이차 판별함수 각 범주의 확률밀도함수는 아래와 같이 다변량 정규분포로 정의된다. \\[\\begin{equation} f_k(\\mathbf{x}) = \\frac{1}{(2\\pi)^{p/2}|\\boldsymbol\\Sigma_k|^{1/2}} \\exp \\{ -\\frac{1}{2} \\left(\\mathbf{x} - \\boldsymbol\\mu_k\\right)^\\top \\boldsymbol\\Sigma_k^{-1} \\left(\\mathbf{x} - \\boldsymbol\\mu_k\\right) \\} \\tag{2.10} \\end{equation}\\] 위 식 (2.10)이 선형판별함수에서 사용한 식 (2.4)과 다른 부분은 분산-공분산분포 \\(\\boldsymbol\\Sigma_k\\)가 범주 \\(k\\)에 대해 각각 정의된다는 점이다. 이 경우 각 범주에 대한 판별함수는 아래와 같이 정의된다. \\[\\begin{equation} u_k(\\mathbf{x}) = - \\frac{1}{2} (\\mathbf{x} - \\boldsymbol\\mu_k)^\\top \\boldsymbol\\Sigma^{-1} (\\mathbf{x} - \\boldsymbol\\mu_k) - \\frac{1}{2} \\ln \\left| \\boldsymbol\\Sigma_k \\right| + \\ln \\pi_k \\tag{2.11} \\end{equation}\\] 데이터 행렬 \\(X = (\\mathbf{x}_1, \\mathbf{x}_2, \\cdots , \\mathbf{x}_N)\\)의 각 객체에 대한 판별함수값을 얻는 함수를 아래와 같이 구현할 수 있다. qda_discriminant_func &lt;- function(X, mu, Sigma, pi) { Sigma_inv_sqrt &lt;- chol(solve(Sigma)) - 0.5 * rowSums((t(X - mu) %*% t(Sigma_inv_sqrt))^2) - 0.5 * log(det(Sigma)) + log(pi) } 2.6.3 이차판별함수에 의한 분류 분류기준은 선형판별분석과 마찬가지로 판별함수값이 큰 범주로 분류한다. \\[\\begin{equation*} \\hat{y} = \\begin{cases} 1, &amp; \\text{if } u_1(\\mathbf{x}) \\ge u_2(\\mathbf{x}) \\\\ 2, &amp; \\text{otherwise} \\end{cases} \\end{equation*}\\] Table 2.1의 학습표본에 대해 이차판별함수값을 계산하고 범주를 추정하면 아래와 같다. qda_discriminant_result_df &lt;- train_df %&gt;% mutate( u1 = qda_discriminant_func( .[c(&quot;x1&quot;, &quot;x2&quot;)] %&gt;% t(), mu_hat[1, c(&quot;x1&quot;, &quot;x2&quot;)] %&gt;% unlist(), S_within_group[[1]], n_obs$pi[1] ), u2 = qda_discriminant_func( .[c(&quot;x1&quot;, &quot;x2&quot;)] %&gt;% t(), mu_hat[2, c(&quot;x1&quot;, &quot;x2&quot;)] %&gt;% unlist(), S_within_group[[2]], n_obs$pi[2] ) ) %&gt;% mutate( predicted_class = factor(if_else(u1 &gt;= u2, 1, 2), levels = c(1, 2)) ) knitr::kable( qda_discriminant_result_df, booktabs = TRUE, align = rep(&#39;r&#39;, dim(qda_discriminant_result_df)[2]), col.names = c(&#39;객체번호&#39;, &#39;$x_1$&#39;, &#39;$x_2$&#39;, &#39;실제범주&#39;, &#39;$u_1(\\\\mathbf{x})$&#39;, &#39;$u_2(\\\\mathbf{x})$&#39;, &#39;추정범주&#39;), caption = &#39;학습표본에 대한 QDA 적용 결과: 판별함수값 및 추정범주&#39;) Table 2.8: 학습표본에 대한 QDA 적용 결과: 판별함수값 및 추정범주 객체번호 \\(x_1\\) \\(x_2\\) 실제범주 \\(u_1(\\mathbf{x})\\) \\(u_2(\\mathbf{x})\\) 추정범주 1 5 7 1 -1.729447 -3.950639 1 2 4 3 2 -13.183993 -2.497284 2 3 7 8 2 -3.911266 -3.145402 2 4 8 6 2 -5.274902 -1.868806 2 5 3 6 1 -1.183993 -5.764060 1 6 2 5 1 -1.729447 -6.202685 1 7 6 6 1 -2.002175 -1.934273 2 8 9 6 2 -7.729447 -2.582391 2 9 5 4 2 -8.274902 -1.927726 2 위 Table 2.8에서 보듯이 모든 학습객체가 올바로 분류되고 있다. 또한 선형판별분석의 경우와 마찬가지로 사후확률 비교를 통한 범주 분류를 수행할 수 있다. qda_posterior_result_df &lt;- train_df %&gt;% mutate( f1 = mvtnorm::dmvnorm( .[c(&quot;x1&quot;, &quot;x2&quot;)], mu_hat[1, c(&quot;x1&quot;, &quot;x2&quot;)] %&gt;% unlist(), S_within_group[[1]]), f2 = mvtnorm::dmvnorm( .[c(&quot;x1&quot;, &quot;x2&quot;)], mu_hat[2, c(&quot;x1&quot;, &quot;x2&quot;)] %&gt;% unlist(), S_within_group[[2]]), f = n_obs$pi[1] * f1 + n_obs$pi[2] * f2 ) %&gt;% mutate( p1 = n_obs$pi[1] * f1 / f, p2 = n_obs$pi[2] * f2 / f ) %&gt;% mutate( predicted_class = factor(if_else(p1 &gt;= p2, 1, 2), levels = c(1, 2)) ) %&gt;% select( id, x1, x2, class, p1, p2, predicted_class ) knitr::kable( qda_posterior_result_df, booktabs = TRUE, align = rep(&#39;r&#39;, dim(qda_posterior_result_df)[2]), col.names = c(&#39;객체번호&#39;, &#39;$x_1$&#39;, &#39;$x_2$&#39;, &#39;실제범주&#39;, &#39;$P(y = 1 | \\\\mathbf{x})$&#39;, &#39;$P(y = 2 | \\\\mathbf{x})$&#39;, &#39;추정범주&#39;), caption = &#39;학습표본에 대한 QDA 적용 결과: 사후확률 및 추정범주&#39;) Table 2.9: 학습표본에 대한 QDA 적용 결과: 사후확률 및 추정범주 객체번호 \\(x_1\\) \\(x_2\\) 실제범주 \\(P(y = 1 | \\mathbf{x})\\) \\(P(y = 2 | \\mathbf{x})\\) 추정범주 1 5 7 1 0.9021365 0.0978635 1 2 4 3 2 0.0000228 0.9999772 2 3 7 8 2 0.3173746 0.6826254 2 4 8 6 2 0.0321055 0.9678945 2 5 3 6 1 0.9898499 0.0101501 1 6 2 5 1 0.9887184 0.0112816 1 7 6 6 1 0.4830310 0.5169690 2 8 9 6 2 0.0057829 0.9942171 2 9 5 4 2 0.0017486 0.9982514 2 이 또한 MASS 패키지의 predict.qda 함수를 통해 아래와 같이 동일한 결과값을 보다 간편하게 얻을 수 있다. train_df %&gt;% bind_cols( predict(qda_fit, train_df)$posterior %&gt;% `colnames&lt;-`(paste0(&quot;p&quot;, colnames(.))) %&gt;% as_data_frame() ) %&gt;% mutate( predicted_class = predict(qda_fit, .)$class ) ## # A tibble: 9 x 7 ## id x1 x2 class p1 p2 predicted_class ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 5 7 1 0.902 0.0979 1 ## 2 2 4 3 2 0.0000228 1.000 2 ## 3 3 7 8 2 0.317 0.683 2 ## 4 4 8 6 2 0.0321 0.968 2 ## 5 5 3 6 1 0.990 0.0102 1 ## 6 6 2 5 1 0.989 0.0113 1 ## 7 7 6 6 1 0.483 0.517 2 ## 8 8 9 6 2 0.00578 0.994 2 ## 9 9 5 4 2 0.00175 0.998 2 2.7 세 범주 이상의 분류 2.7.1 기본 R 스크립트 3개의 범주를 지닌 붓꽃(iris) 데이터에 대해 선형판별분석을 적용하는 R 스크립트는 아래와 같다. 본 예제에서는 각 범주별 50개 데이터 중 첫 30개 관측치만을 학습표본으로 삼아 판별함수를 유도한다. iris_train_df &lt;- datasets::iris %&gt;% rename(x1 = Sepal.Length, x2 = Sepal.Width, x3 = Petal.Length, x4 = Petal.Width, class = Species) %&gt;% group_by(class) %&gt;% slice(1:30) %&gt;% ungroup() %&gt;% mutate(id = row_number()) iris_lda_fit &lt;- MASS::lda(class ~ . -id, iris_train_df) print(iris_lda_fit) ## Call: ## lda(class ~ . - id, data = iris_train_df) ## ## Prior probabilities of groups: ## setosa versicolor virginica ## 0.3333333 0.3333333 0.3333333 ## ## Group means: ## x1 x2 x3 x4 ## setosa 5.026667 3.450000 1.473333 0.2466667 ## versicolor 6.070000 2.790000 4.333333 1.3533333 ## virginica 6.583333 2.933333 5.603333 2.0066667 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## x1 0.5711419 -1.2397647 ## x2 1.8752911 3.0223980 ## x3 -1.7361767 0.3159667 ## x4 -3.4672646 1.3954748 ## ## Proportion of trace: ## LD1 LD2 ## 0.9929 0.0071 2.7.2 일반화된 판별함수 \\(K (&gt; 2)\\)개의 범주가 있는 경우에 대한 판별분석은 아래와 같이 일반화된다. \\(\\pi_k\\): 범주 \\(k\\)에 속할 사전확률, \\(k = 1, 2, \\cdots, K\\) \\(C(k&#39; \\, | \\, k) \\ge 0\\): 실제 범주 \\(k\\)에 속하는 데 범주 \\(k&#39;\\)로 분류할 때 소요 비용 (\\(C(k&#39; \\, | \\, k) = 0 \\text{ if } k&#39; = k\\)) \\(f_k(\\mathbf{x})\\): 범주 \\(k\\)에 속하는 \\(\\mathbf{x}\\)의 확률밀도함수 \\(R_k \\subset \\mathbb{R}^p\\): 범주 \\(k\\)로 분류되는 \\(\\mathbf{x}\\)의 영역 \\(P(k&#39; \\, | \\, k) = \\int_{\\mathbf{x} \\in R_{k&#39;}} f_k(\\mathbf{x}) d\\mathbf{x}\\): 실제범주 \\(k\\)에 속하는 데 범주 \\(k&#39;\\)로 분류할 확률 이 때, 총 기대 오분류 비용은 아래와 같다. \\[\\begin{equation*} \\sum_{k = 1}^{K} \\pi_k \\sum_{k&#39; \\neq k} C(k&#39; \\, | \\, k) \\int_{\\mathbf{x} \\in R_{k&#39;}} f_k(\\mathbf{x}) d\\mathbf{x} \\end{equation*}\\] 따라서 분류문제는 위 총 기대 오분류 비용을 최소화하는 \\(R_1, \\cdots, R_K\\)를 찾는 것이다. 우선, 범주를 고려하지 않은 \\(\\mathbf{x}\\)의 확률밀도함수는 아래와 같이 정의된다. \\[\\begin{equation*} f(\\mathbf{x}) = \\sum_{k=1}^{K} \\pi_k f_k(\\mathbf{x}) \\end{equation*}\\] 베이즈 정리에 의하여, 변수 \\(\\mathbf{x}\\)가 주어졌을 때 범주 \\(k\\)에 속할 사후확률은 아래와 같다. \\[\\begin{equation*} P(y = k \\,|\\, \\mathbf{x}) = \\frac{\\pi_k f_k(\\mathbf{x})}{f(\\mathbf{x})} \\end{equation*}\\] 오분류비용이 동일한 경우에는 각 객체에 대해 위의 사후확률이 가장 큰 범주로 추정한다. 위 식에서 \\[\\begin{equation*} P(y = k \\,|\\, \\mathbf{x}) \\propto \\pi_k f_k(\\mathbf{x}) \\end{equation*}\\] 이므로, 아래와 같이 범주가 추정된다. \\[\\begin{equation*} \\hat{y} = {arg\\,max}_{k} \\pi_k f_k(\\mathbf{x}) \\end{equation*}\\] 앞 장들에서 살펴본 것과 마찬가지로, 선형판별분석의 경우 각 범주의 확률밀도함수 \\(f_k(\\mathbf{x})\\)가 동일 분산-공분산행렬을 가정하며, 이차판별분석의 경우 서로 다른 분산-공분산행렬을 가정한다. 아래 스크립트는 MASS 패키지의 lda 함수를 통해 각 범주에 속할 사후확률과 범주 추정값을 얻는 과정을 보여준다. iris_lda_fit &lt;- MASS::lda(class ~ x1 + x2 + x3 + x4, iris_train_df) iris_lda_result &lt;- iris_train_df %&gt;% bind_cols( predict(iris_lda_fit, .)$posterior %&gt;% as_data_frame() ) %&gt;% mutate( predicted_class = predict(iris_lda_fit, .)$class ) print(iris_lda_result) ## # A tibble: 90 x 10 ## x1 x2 x3 x4 class id setosa versicolor virginica ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 seto… 1 1 5.57e-22 6.34e-42 ## 2 4.9 3 1.4 0.2 seto… 2 1 3.32e-17 5.67e-36 ## 3 4.7 3.2 1.3 0.2 seto… 3 1 2.75e-19 2.14e-38 ## 4 4.6 3.1 1.5 0.2 seto… 4 1 8.12e-17 5.62e-35 ## 5 5 3.6 1.4 0.2 seto… 5 1 1.14e-22 1.25e-42 ## 6 5.4 3.9 1.7 0.4 seto… 6 1 3.20e-21 4.38e-40 ## 7 4.6 3.4 1.4 0.3 seto… 7 1 8.74e-19 4.07e-37 ## 8 5 3.4 1.5 0.2 seto… 8 1 3.27e-20 1.62e-39 ## 9 4.4 2.9 1.4 0.2 seto… 9 1.000 2.22e-15 3.42e-33 ## 10 4.9 3.1 1.5 0.1 seto… 10 1 9.36e-19 4.85e-38 ## # ... with 80 more rows, and 1 more variable: predicted_class &lt;fct&gt; knitr::kable( iris_lda_result %&gt;% select(id, class, predicted_class, setosa, versicolor, virginica) %&gt;% filter(class != predicted_class), booktabs = TRUE, align = rep(&#39;r&#39;, 6), col.names = c(&#39;객체번호&#39;, &#39;실제범주&#39;, &#39;추정범주&#39;, &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;), caption = &#39;붓꽃 학습표본에 대한 LDA 적용 결과 - 오분류 객체 사후 확률&#39;) Table 2.10: 붓꽃 학습표본에 대한 LDA 적용 결과 - 오분류 객체 사후 확률 객체번호 실제범주 추정범주 setosa versicolor virginica 51 versicolor virginica 0 0.3088912 0.6911088 아래 스크립트는 MASS 패키지의 qda 함수를 통해 각 범주에 속할 사후확률과 범주 추정값을 얻는 과정을 보여준다. iris_qda_fit &lt;- MASS::qda(class ~ x1 + x2 + x3 + x4, iris_train_df) iris_qda_result &lt;- iris_train_df %&gt;% bind_cols( predict(iris_qda_fit, .)$posterior %&gt;% as_data_frame() ) %&gt;% mutate( predicted_class = predict(iris_qda_fit, .)$class ) knitr::kable( iris_qda_result %&gt;% select(id, class, predicted_class, setosa, versicolor, virginica) %&gt;% filter(class != predicted_class), booktabs = TRUE, align = rep(&#39;r&#39;, 6), col.names = c(&#39;객체번호&#39;, &#39;실제범주&#39;, &#39;추정범주&#39;, &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;), caption = &#39;붓꽃 학습표본에 대한 QDA 적용 결과 - 오분류 객체 사후 확률&#39;) Table 2.11: 붓꽃 학습표본에 대한 QDA 적용 결과 - 오분류 객체 사후 확률 객체번호 실제범주 추정범주 setosa versicolor virginica 51 versicolor virginica 0 0.4274712 0.5725288 위 결과에서 선형판별분석과 이차판별분석은 동일한 객체를 오분류한다. 해당 객체의 실제 범주에 대한 사후확률은 이차판별분석 결과에서 보다 높게 나타난다. References "],
["tree-based-method.html", "Chapter 3 트리기반 기법 3.1 CART 개요 3.2 필요 R package 설치 3.3 CART 트리 생성 3.4 가지치기 및 최종 트리 선정 3.5 R패키지 내 분류 트리 방법", " Chapter 3 트리기반 기법 3.1 CART 개요 CART(Classification and Regression Trees)는 Breiman et al. (1984) 에 의하여 개발된 것인데, 각 (독립)변수를 이분화(binary split)하는 과정을 반복하여 트리 형태를 형성함으로써 분류(종속변수가 범주형일 때) 또는 회귀분석(종속변수가 연속형일 때)을 수행하는 것이다. 이 때 독립변수들은 범주형 또는 연속형 모두에 적용될 수 있다. 본 장에서는 분류를 위한 목적만을 설명하도록 한다. 3.2 필요 R package 설치 본 장에서 필요한 R 패키지들은 아래와 같다. package version tidyverse 1.2.1 rpart 4.1-13 rpart.plot 3.0.1 3.3 CART 트리 생성 3.3.1 기본 R 스크립트 train_df &lt;- tibble( x1 = c(1,2,2,2,2,3,4,4,4,5), x2 = c(4,6,5,4,3,6,6,5,4,3), class = as.factor(c(1,1,1,2,2,1,1,2,2,2)) ) Table 3.1: 학습표본 데이터 x1 x2 class 1 4 1 2 6 1 2 5 1 2 4 2 2 3 2 3 6 1 4 6 1 4 5 2 4 4 2 5 3 2 Table 3.1와 같이 두 독립변수 x1, x2와 이분형 종속변수 class의 관측값으로 이루어진 10개의 학습표본을 train_df라는 data frame에 저장한다. library(rpart) library(rpart.plot) cart.est &lt;- rpart( class ~ x1 + x2 , data = train_df , method = &quot;class&quot; , parms = list(split = &quot;gini&quot;) , control = rpart.control(minsplit = 2 , minbucket = 1 , cp = 0 , xval = 0 , maxcompete = 0) ) rpart.plot(cart.est) Figure 3.1: CART 트리 rpart 라는 package를 기반으로, 두 변수 x1과 x2를 이용하여 이분형 종속변수 class를 분류하는 CART 트리를 생성할 수 있으며, rpart.plot package를 이용하여 Figure 3.1과 같이 시각화할 수 있다. 3.3.2 기호 정의 본 장에서 사용될 수학적 기호는 아래와 같다. \\(T\\): 트리 \\(A(T)\\): 트리 \\(T\\)의 최종노드의 집합 \\(J\\): 범주수 \\(N\\): 학습표본의 총 객체수 \\(N_j\\): 범주 \\(j\\)에 속한 객체 수 \\(N(t)\\): 노드 \\(t\\)에서의 객체수 \\(N_j(t)\\): 노드 \\(t\\)에서 범주 \\(j\\)에 속한 객체수 \\(p(j,t)\\): 임의의 객체가 범주 \\(j\\)와 노드 \\(t\\)에 속할 확률 \\(p(t)\\): 임의의 객체가 노드 \\(t\\)에 속할 확률 \\[p(t) = \\sum_{j=1}^{J} p(j,t)\\] \\(p(j|t)\\): 임의의 객체가 노드 \\(t\\)에 속할 때 범주 \\(j\\)에 속할 조건부 확률 \\[p(j|t) = \\frac{p(j,t)}{p(t)}, \\quad \\sum_{j=1}^{J} p(j|t) = 1\\] 이 때, 각 확률은 학습표본에서 아래와 같이 추정할 수 있다. \\[\\begin{align} p(j,t) &amp;\\approx \\frac{N_j(t)}{N}\\\\ p(t) &amp;\\approx \\frac{N(t)}{N}\\\\ p(j|t) &amp;\\approx \\frac{N_j(t)}{N(t)} \\end{align}\\] 3.3.3 노드 및 트리의 불순도 3.3.3.1 노드의 불순도 CART는 지니 지수(Gini index)를 불순도 함수로 사용한다. 총 \\(J\\)개의 범주별 객체비율을 \\(p_1, \\cdots , p_J\\)라 할 때 (\\(\\sum_{j=1}^{J} p_j = 1\\)), 지니 지수는 식 (3.1)와 같다. \\[\\begin{equation} G(p_1, \\cdots, p_J) = \\sum_{j=1}^{J} p_j(1-p_j) = 1 - \\sum_{j=1}^{J}p_j^2 \\tag{3.1} \\end{equation}\\] 노드 \\(t\\)에서의 범주별 객체비율은 \\(p(1|t), \\cdots, p(J|t)\\)이므로, 노드 \\(t\\)의 불순도는 식 (3.2)와 같이 산출된다. \\[\\begin{equation} \\begin{split} i(t) &amp;= 1 - \\sum_{j=1}^{J} p(j|t)^2\\\\ &amp;\\approx 1 - \\sum_{j=1}^{J} \\left[\\frac{N_j(t)}{N(t)}\\right]^2 \\end{split} \\tag{3.2} \\end{equation}\\] 3.3.3.2 트리 불순도 트리 \\(T\\)의 불순도는 식 (3.3)와 같이 최종노드들의 불순도의 가중평균으로 정의된다. \\[\\begin{equation} I(T) = \\sum_{t \\in A(T)} i(t)p(t) \\tag{3.3} \\end{equation}\\] 여기서 \\[ I(t) = i(t)p(t) \\] 라 하면, 다음이 성립한다. \\[ I(T) = \\sum_{t \\in A(T)} I(t) \\] 3.3.4 분지기준 뿌리 노드에서의 분지만을 살펴보기 위해 control parameter maxdepth의 값을 1으로 설정한다. 이 경우, CART 알고리즘은 뿌리노드에서의 양 갈래 분지만을 선택한 뒤 종료된다. 아래 스크립트를 이용하여 뿌리노드에서 최적분지된 트리를 얻는다. cart.firstsplit &lt;- rpart(class ~ x1 + x2 , data = train_df , method = &quot;class&quot; , parms = list(split = &quot;gini&quot;) , control = rpart.control(minsplit = 2 , minbucket = 1 , maxdepth = 1 , cp = 0 , xval = 0 , maxcompete = 0 ) ) rpart.plot(cart.firstsplit) Figure 3.2: 뿌리노드 분지 또한 분지 결과 트리는 Table 3.2와 같이 frame이라는 이름의 data frame에 설명된다. 각 행 앞의 번호는 노드 인덱스 \\(t\\)를 나타내며, 각 열에 대한 설명은 아래와 같다. var: 노드 \\(t\\)를 분지하는 데 이용된 변수. 값이 &lt;leaf&gt;인 경우에는 노드 \\(t\\)가 최종 노드임을 나타낸다. n: 노드 내 객체 수 \\(N(t)\\) wt: 가중치 적용 후 객체 수 (추후 appendix에서 설명) dev: 오분류 객체 수 yval: 노드 \\(t\\)를 대표하는 범주 complexity: 노드 \\(t\\)에서 추가로 분지할 때 감소하는 relative error값; 본 분류트리 예제에서 error는 오분류율이며, 뿌리 노드의 relative error값을 1으로 한다. Table 3.2: 뿌리노드 분지 상세 (frame) var n wt dev yval complexity ncompete nsurrogate 1 x2 10 10 5 1 0.6 0 0 2 &lt;leaf&gt; 3 3 0 1 0.0 0 0 3 &lt;leaf&gt; 7 7 2 2 0.0 0 0 또한 frame에는 트리 내 각 노드에 속한 객체와 범주에 대한 정보를 나타내는 yval2라는 행렬이 Table 3.3와 같이 존재한다. 실제 yval2의 열의 개수는 전체 학습 대상 범주 수에 따라 달라지며, 본 예는 이분 분류 트리(범주개수 = 2)에 해당하는 열 구성을 보여준다. 각 행 앞의 번호는 노드 인덱스 \\(t\\)를 나타내며, 각 열에 대한 설명은 아래와 같다. 열1: 노드 \\(t\\)에서의 최적 추정 범주 \\(j^*\\) 열2: 노드 \\(t\\) 내 범주 class=1 객체 수 \\(N_1(t)\\) 열3: 노드 \\(t\\) 내 범주 class=2 객체 수 \\(N_2(t)\\) 열4: 노드 \\(t\\) 내 범주 class=1 관측 확률 \\(p(1|t) \\approx \\tfrac{N_1(t)}{N(t)}\\) 열5: 노드 \\(t\\) 내 범주 class=2 관측 확률 \\(p(2|t) \\approx \\tfrac{N_2(t)}{N(t)}\\) nodeprob: 노드 \\(t\\) 확률 \\(p(t) \\approx \\tfrac{N(t)}{N}\\) ## Warning: Setting row names on a tibble is deprecated. Table 3.3: 노드 내 객체 및 범주 정보 (yval2) 열1 열2 열3 열4 열5 nodeprob 1 1 5 5 0.50 0.50 1.0 2 1 3 0 1.00 0.00 0.3 3 2 2 5 0.29 0.71 0.7 위 CART 모델 데이터를 이용하여 트리의 불순도를 계산해보자. 우선 노드 상세 정보 행렬 yval2의 x번째 노드의 불순도(\\(i(t)\\))를 계산하는 함수 rpartNodeImpurity를 아래와 같이 구현한다. rpartNodeImpurity &lt;- function(x, yval2) { node_vec &lt;- yval2[x, ] n.columns &lt;- length(node_vec) class.prob &lt;- node_vec[((n.columns/2)+1):(n.columns-1)] return(1 - sum(class.prob^2)) } CART tree 객체의 각 leaf node에 함수 rpartNodeImpurity를 적용하여 노드 불순도 \\(i(t)\\)를 계산한 뒤, 노드 확률 \\(p(t)\\)을 이용한 가중합을 통해 트리 불순도 \\(I(T)\\)를 계산하는 함수 rpartImpurity를 아래와 같이 구현한다. rpartImpurity &lt;- function(rpart.obj) { leaf.nodes &lt;- which(rpart.obj$frame$var==&quot;&lt;leaf&gt;&quot;) node.impurity &lt;- sapply(leaf.nodes, rpartNodeImpurity, yval2 = rpart.obj$frame$yval2) node.prob &lt;- rpart.obj$frame$yval2[leaf.nodes, &#39;nodeprob&#39;] return(sum(node.prob * node.impurity)) } 위 함수를 이용하여 계산한 트리 Figure 3.1의 불순도는 0.29이다. rpartImpurity(cart.firstsplit) ## [1] 0.2857143 분지를 추가할수록 불순도는 감소한다. 분지를 추가하기 위해서는 maxdepth라는 control parameter 값을 증가시키면 된다. maxdepth: 뿌리노드부터 임의의 최종노드에 도달하는 최대 가능 분지 수 (default=30) maxdepth 파라미터의 값을 1부터 4까지 증가시키며 불순도의 변화를 살펴보자. library(ggplot2) tree.impurity &lt;- sapply(c(1:4), function(depth) { rpart(class ~ x1 + x2 , data = train_df , method = &quot;class&quot; , parms = list(split = &quot;gini&quot;) , control = rpart.control(minsplit = 2 , minbucket = 1 , maxdepth = depth , cp = 0 , xval = 0 , maxcompete = 0)) %&gt;% rpartImpurity() }) tibble(maxdepth=c(1:4), impurity=tree.impurity) %&gt;% ggplot(aes(x=maxdepth, y=impurity)) + geom_line() Figure 3.3: 파라미터 maxdepth값에 따른 트리불순도 변화 위 예에서, 트리의 분지가 증가함에 따라 불순도는 0.29, 0.17, 0.17, 0로 감소한다. maxdepth값이 3일 때 불순도가 감소하지 않는 이유는, 세 번째 분지 결과가 전체적인 오분류를 감소시키지 않아 rpart 함수가 해당 분지를 취소하기 때문이다. 여기에 작용하는 파라미터는 cp라는 control parameter이다. cp: 노드가 분지되기 위한 최소 relative error 감소치 (default = 0.01). 값이 0일 경우 최대트리를 생성한다. 위 예제에서는 cp값을 0으로 설정하여, 해당 분지가 트리 불순도를 감소시킨다 하더라도 전체 트리의 오분류를 감소시키는 데 기여하지 않는다면 시도하지 않도록 하였다. 3.4 가지치기 및 최종 트리 선정 3.4.1 가지치기 앞 장의 최대 트리 그림 3.1은 학습 데이터를 오분류 없이 완벽하게 분류하기 위해 복잡한 분류 구조를 형성하였다. 이러한 복잡한 분류 구조는 학습 데이터가 아닌 새로운 데이터에 대한 분류 정확도를 떨어뜨릴 수 있다. 이는 bias-variance tradeoff라 부르는 현상으로, 비단 분류트리 뿐 아니라 모든 데이터마이닝 방법에 일반적으로 적용된다. 분류 트리는 가지치기라는 방식을 통해, 분류 구조를 단순화함으로써 분류 트리가 새로운 데이터에도 정확한 분류를 제공하기를 추구한다. 가지치기란 트리 내 특정 내부노드를 기준으로 그 하위에 발생한 분지를 모두 제거하고, 해당 내부노드를 최종노드로 치환하는 방식이다. Table 3.4: 최대 트리 분지 상세 (frame) var n wt dev yval complexity ncompete nsurrogate 1 x2 10 10 5 1 0.6 0 0 2 &lt;leaf&gt; 3 3 0 1 0.0 0 0 3 x1 7 7 2 2 0.2 0 0 6 &lt;leaf&gt; 1 1 0 1 0.0 0 0 7 x2 6 6 1 2 0.1 0 0 14 x1 2 2 1 1 0.1 0 0 28 &lt;leaf&gt; 1 1 0 1 0.0 0 0 29 &lt;leaf&gt; 1 1 0 2 0.0 0 0 15 &lt;leaf&gt; 4 4 0 2 0.0 0 0 Table 3.4에서 생성 가능한 가지치기는 최종 노드(var값이 &lt;leaf&gt;)가 아닌 모든 노드(1, 3, 7, 14)에서 가능하며, 함수 snip.rpart를 이용하여 가지치기 된 트리를 생성할 수 있다. 각 내부 노드에서 가지치기된 트리들은 아래와 같이 얻어진다. internal.node.index &lt;- rownames(cart.est$frame)[which(cart.est$frame$var != &#39;&lt;leaf&gt;&#39;)] %&gt;% as.numeric() snipped &lt;- lapply(internal.node.index, function(x){snip.rpart(cart.est, x)}) n.trees &lt;- length(snipped) par(mfrow=c(2,2)) invisible(lapply(c(1:n.trees), function(x) { rpart.plot(snipped[[x]])} )) Figure 3.4: 각 내부노드 기준으로 가지치기된 트리 위 각 가지치기 후보 노드의 오분류 비용은 함수 nodeCost를 아래와 같이 구현하여 계산할 수 있다. nodeCost &lt;- function(node, tree) { node_vec &lt;- tree$frame$yval2[as.character(node) == row.names(tree$frame), ] n.columns &lt;- length(node_vec) class.prob.max &lt;- max(node_vec[((n.columns/2)+1):(n.columns-1)]) node.prob &lt;- node_vec[n.columns] node.misclassification.cost &lt;- (1-class.prob.max)*node.prob return(node.misclassification.cost) } tibble( pruning_node = internal.node.index, node_cost = sapply(internal.node.index, nodeCost, tree=cart.est) ) %&gt;% knitr::kable() pruning_node node_cost 1 0.5 3 0.2 7 0.1 14 0.1 각 가지치기 노드에 해당하는 하부 트리의 오분류비용 및 복잡도를 구하기 위해 subtreeEval라는 함수를 아래와 같이 구현한다. subtreeEval &lt;- function(node, tree) { snipped &lt;- snip.rpart(tree, node)$frame leaf.nodes &lt;- setdiff(rownames(tree$frame[tree$frame$var==&quot;&lt;leaf&gt;&quot;,]), rownames(snipped)) %&gt;% as.numeric() tibble( pruning_node = node, node.cost = nodeCost(node, tree), subtree.cost = sapply(leaf.nodes, nodeCost, tree=tree) %&gt;% sum(), subtree.size = length(leaf.nodes) ) %&gt;% mutate(alpha = (node.cost - subtree.cost) / (subtree.size - 1)) } 각 노드에 대하여 알파값을 다음과 같이 계산할 수 있다. df.cost &lt;- lapply(internal.node.index, subtreeEval, tree=cart.est) %&gt;% bind_rows() Table 3.5: 내부노드 가지치기 평가 (df.cost) pruning_node node.cost subtree.cost subtree.size alpha 1 0.5 0 5 0.12 3 0.2 0 4 0.07 7 0.1 0 3 0.05 14 0.1 0 2 0.10 위 Table 3.5 에서 최소 알파값에 해당하는 노드 7에서 가지치기를 한다. pruned.tree.1 &lt;- snip.rpart(cart.est, df.cost$pruning_node[which.min(df.cost$alpha)]) rpart.plot(pruned.tree.1) Figure 3.5: 1단계 가지치기 결과 가지치기로 형성된 트리에서 다시 각 가지치기 노드의 오분류비용, 복잡도 및 알파값을 구한다. df.cost &lt;- rownames(pruned.tree.1$frame)[pruned.tree.1$frame$var!=&quot;&lt;leaf&gt;&quot;] %&gt;% as.numeric() %&gt;% lapply(subtreeEval, tree=pruned.tree.1) %&gt;% bind_rows() knitr::kable(df.cost) pruning_node node.cost subtree.cost subtree.size alpha 1 0.5 0.1 3 0.2 3 0.2 0.1 2 0.1 위 결과에서 다시 최소 알파값에 해당하는 노드 3에서 가지치기를 하면 아래와 같은 트리가 형성된다. pruned.tree.2 &lt;- snip.rpart(pruned.tree.1, df.cost$pruning_node[which.min(df.cost$alpha)]) rpart.plot(pruned.tree.2) Figure 3.6: 2단계 가지치기 결과 3.4.2 최적 트리의 선정 위 가지치기 과정에서 얻는 가지친 트리들이 최종 트리의 후보가 되며, 이 중 테스트 표본에 대한 오분류율이 가장 작은 트리를 최적 트리로 선정하게 된다. 트리를 학습할 때 사용된 학습데이터 Table 3.1 외에, Table 3.6과 같은 6개의 테스트 데이터가 있다고 하자. test_df &lt;- tibble( x1 = c(1,0,3,4,2,1), x2 = c(5,5,4,3,7,4), class = factor(c(1,1,2,2,1,2), levels=c(1,2)) ) Table 3.6: 테스트 데이터 x1 x2 class 1 5 1 0 5 1 3 4 2 4 3 2 2 7 1 1 4 2 테스트 데이터에 위에서 학습된 세 개의 트리, 즉 최대 트리 cart.est와 두 개의 가지치기 트리 pruned.tree.1 &amp; pruned.tree.2를 적용하여 각 트리가 각각의 테스트 데이터를 어떻게 분류하는지 살펴보자. test_pred &lt;- test_df %&gt;% bind_cols( pred_maxtree = predict(cart.est, test_df, type=&quot;class&quot;), pred_prune1 = predict(pruned.tree.1, test_df, type=&quot;class&quot;), pred_prune2 = predict(pruned.tree.2, test_df, type=&quot;class&quot;) ) Table 3.7: 테스트 데이터에 대한 예측 결과 x1 x2 class pred_maxtree pred_prune1 pred_prune2 1 5 1 1 1 2 0 5 1 1 1 2 3 4 2 2 2 2 4 3 2 2 2 2 2 7 1 1 1 1 1 4 2 1 1 2 결과 Table 3.7에서 최대트리가 오분류한 테스트 표본은 1개, 첫번째 가지치기 트리가 오분류한 테스트 표본은 1개, 그리고 두 번째 가지치기 트리가 오분류한 테스트 표본은 2개이다. 위 결과를 토대로, 최적의 트리를 선정하는 과정은 아래와 같다. 각각의 트리에 의해 오분류된 테스트 표본의 개수를 전체 테스트 표본의 개수로 나누어 오분류율 \\(R^{ts}\\)를 구한다. 테스트 표본 수를 \\(n_{test}\\)라 할 때, 오분류의 표준편차를 아래와 같이 계산한다. \\[SE = \\sqrt{\\frac{R^{ts}(1 - R^{ts})}{n_{test}}}\\] 1에서 구한 오분류율에 2에서 구한 표준편차를 더하여 \\(R^{ts} + SE\\)를 각 트리의 평가척도로 계산한다. 후보 트리들 중 해당 평가척도가 가장 작은 트리를 최종 트리로 선정한다. test.summary &lt;- test_pred %&gt;% summarize(n.test = n(), cart.est = sum(pred_maxtree != class) / n.test, pruned.tree.1 = sum(pred_prune1 != class) / n.test, pruned.tree.2 = sum(pred_prune2 != class) / n.test) %&gt;% gather(&quot;tree&quot;,&quot;R.ts&quot;,-n.test) %&gt;% mutate(SE = sqrt((R.ts*(1 - R.ts))/n.test), score = R.ts + SE) %&gt;% select(-n.test) Table 3.8: 분류 성능 트리 오분류율(\\(R^{ts}\\)) 표준편차(\\(SE\\)) 척도(\\(R^{ts} + SE\\)) cart.est 0.17 0.15 0.32 pruned.tree.1 0.17 0.15 0.32 pruned.tree.2 0.33 0.19 0.53 위 결과, 최적 트리는 최대 트리 혹은 첫 번째 가지치기 트리가 된다. 위 절차를 임의의 데이터에 대해 수행하는 함수를 구현해보자. rpart_learn &lt;- function(formula, train_df, test_df) { # 최대 트리 생성 max_tree &lt;- rpart(formula , data = train_df , method = &quot;class&quot; , parms = list(split = &quot;gini&quot;) , control = rpart.control(minsplit = 2 , minbucket = 1 , cp = 0 , xval = 0 , maxcompete = 0 ) ) # 가지치기 curr_tree &lt;- list() k &lt;- 1 curr_tree[[k]] &lt;- max_tree while(dim(curr_tree[[k]]$frame)[1] &gt; 1) { internal.node.index &lt;- rownames(curr_tree[[k]]$frame)[which(curr_tree[[k]]$frame$var != &#39;&lt;leaf&gt;&#39;)] %&gt;% as.numeric() df.cost &lt;- lapply(internal.node.index, subtreeEval, tree=curr_tree[[k]]) %&gt;% bind_rows() curr_tree[[k + 1]] &lt;- snip.rpart(curr_tree[[k]], df.cost$pruning_node[which.min(df.cost$alpha)]) k &lt;- k + 1 } # 최적 가지치기 트리 선정 n.test &lt;- dim(test_df)[1] R.ts &lt;- lapply(curr_tree, function(x) { sum(predict(x, test_df, type=&quot;class&quot;) != test_df$class) / n.test }) %&gt;% unlist() score &lt;- R.ts + sqrt((R.ts*(1 - R.ts))/n.test) return(curr_tree[[max(which(score == min(score)))]]) } optimal_tree &lt;- rpart_learn(class ~ x1 + x2, train_df, test_df) rpart.plot(optimal_tree) 3.5 R패키지 내 분류 트리 방법 앞 장에서는 rpart의 결과를 이용하여 교재 8.2 - 8.3장의 예제를 재현해보았다. 실제로 rpart 내부의 기본 트리 방법은 교재의 예제와는 다소 다른 부분이 있다. 이 장에서는 실제 rpart 패키지의 분류 트리 방법에 대해 알아본다. 3.5.1 트리 확장 트리 내 임의의 노드 \\(t\\)에 대한 불순도는 아래와 같이 정의된다. \\[i(t) = \\sum_{j=1}^{J} f\\left(p(j|t)\\right)\\] 여기에서 \\(p(j|t)\\)는 노드 \\(t\\) 내 전체 샘플 \\(N(t)\\) 중 범주 \\(j\\)의 샘플 \\(N_j(t)\\)의 비율로 추정된다. \\[p(j|t) \\approx \\frac{N_j(t)}{N(t)}\\] 또한 함수 \\(f\\)는 concave 함수로, \\(f(0) = f(1) = 0\\)의 조건을 만족시켜야 한다. rpart 에서 설정할 수 있는 함수 \\(f\\)의 종류에 대해서는 아래에서 좀 더 자세히 살펴보기로 한다. 트리 내 임의의 노드 \\(t\\)가 분지규칙 \\(s\\)에 따라 두 개의 노드 \\(t_L\\)과 \\(t_R\\)로 분지된다고 할 때, 불순도의 감소량은 아래와 같이 계산된다. \\[\\begin{eqnarray} \\Delta I(s,t) &amp;=&amp; I(t) - I(t_L) - I(t_R)\\\\ &amp;=&amp; p(t)i(t) - p(t_L)i(t_L) - p(t_R)i(t_R) \\end{eqnarray}\\] rpart는 위 \\(\\Delta I(s,t)\\)값이 최대가 되는 분지 기준 \\(s^*\\)를 찾아 노드 \\(t\\)를 분지하여 트리를 확장하고, 확장된 트리의 최종 노드에서 다시 최적 분지를 찾는 과정을 반복한다. 3.5.1.1 분지 함수 함수 rpart 사용 시 parms 파라미터에 split 값으로 분지 방법을 설정할 수 있다. Gini index (parms=list(split=‘gini’)) 교재의 예제에 사용된 방법으로, 우선 아래와 같은 함수 \\(f\\)를 사용한다. \\[f(p) = p(1-p)\\] information index (parms=list(split=‘information’)) 교재에 엔트로피 지수(Entropy index)로 설명된 지수로, 아래와 같은 함수를 사용한다. \\[f(p) = -p\\log(p)\\] user-defined function 사용자가 임의로 함수를 정의하여 사용할 수 있다. 본 장에서는 자세한 설명은 생략한다. 3.5.2 가지치기 임의의 노드 \\(t\\)에 대한 위험도(오분류 비용의 기대치)는 아래와 같이 계산된다. \\[r(t) = \\sum_{j \\neq \\tau(t)} p(j|t)C\\left(\\tau(t)|j\\right)\\] 여기에서 함수 \\(C(i|j)\\)는 범주 \\(j\\)에 속하는 객체를 범주 \\(i\\)로 분류할 때의 오분류 비용이며, \\(\\tau(t)\\)는 노드 \\(t\\) 내의 오분류 비용을 최소화하도록 노드 \\(t\\)에 지정된 범주값이다. rpart의 오분류 비용 \\(C(i|j)\\)의 기본값은 \\[C(i|j) = \\begin{cases} 1, &amp; \\text{ if } i \\neq j\\\\ 0, &amp; \\text{ if } i = j \\end{cases} \\] 으로 설정되어 있으며, parms 파라미터에 loss 값으로 오분류 비용 \\(C(i|j)\\)를 재설정할 수 있다. 본 장에서는 기본값을 사용하도록 하자. \\(A(T)\\)를 트리 \\(T\\)의 최종 노드의 집합이라 정의하고, 트리의 최종 노드의 개수를 \\(|T|\\)라 할 때, 트리 \\(T\\)의 위험도 \\(R(T)\\)는 아래와 같이 정의된다. \\[R(T) = \\sum_{t \\in A(T)} p(t)r(t)\\] 복잡도 계수(complexity parameter) \\(\\alpha \\in [0, \\infty)\\)를 이용하여, 트리의 비용-복합도 척도를 다음과 같이 정의한다. \\[R_\\alpha(T) = R(T) + \\alpha|T|\\] 이 때, 임의의 계수 \\(\\alpha\\)에 대해 비용 \\(R_\\alpha(T)\\)가 최소가 되게하는 가지치기 트리를 \\(T_\\alpha\\)라 하면, 아래와 같은 관계들이 성립한다. \\(T_0\\): 최대 트리 \\(T_\\infty\\): 뿌리 노드 트리 (분지 없음) \\(\\alpha &gt; \\beta\\)일 때, \\(T_\\alpha\\)는 \\(T_\\beta\\)와 동일하거나 혹은 \\(T_\\beta\\)에서 가지치기된 트리이다. 3.5.3 파라미터값 결정 함수 rpart를 사용할 때 여러가지 사용자 정의 파라미터값을 설정할 수 있으며, 그 파라미터 값에 따라 생성되는 트리의 결과가 달라진다. 대표적인 파라미터 값으로는 아래와 같은 것들이 있다. minsplit: 분지를 시도하기 위해 필요한 노드 내 최소 관측객체 수 (default=20) cp: 노드가 분지되기 위한 최소 relative error 감소치 (default = 0.01). 값이 0일 경우 최대트리를 생성한다. maxdepth: 뿌리노드부터 임의의 최종노드에 도달하는 최대 가능 분지 수 (default=30) References "],
["svm.html", "Chapter 4 서포트 벡터 머신 4.1 개요 4.2 필요 R package 설치 4.3 선형 SVM - 분리 가능 경우 4.4 선형 SVM - 분리 불가능 경우 4.5 비선형 SVM 4.6 R패키지 내 SVM", " Chapter 4 서포트 벡터 머신 4.1 개요 서포트 벡터 머신(suuport vector machine; 이하 SVM)은 기본적으로 두 범주를 갖는 객체들을 분류하는 방법이다. 물론 세 범주 이상의 경우로 확장이 가능하다. 4.2 필요 R package 설치 본 장에서 필요한 R 패키지들은 아래와 같다. package version tidyverse 1.2.1 e1071 1.6-8 Matrix 1.2-14 quadprog 1.5-5 4.3 선형 SVM - 분리 가능 경우 4.3.1 기본 R 스크립트 train_df &lt;- tibble( x1 = c(5, 4, 7, 8, 3, 2, 6, 9, 5), x2 = c(7, 3, 8, 6, 6, 5, 6, 6, 4), class = c(1, -1, 1, 1, -1, -1, 1, 1, -1) ) knitr::kable(train_df, booktabs = TRUE, align = c(&#39;r&#39;, &#39;r&#39;, &#39;r&#39;), caption = &#39;선형분리가능 학습표본 데이터&#39;) Table 4.1: 선형분리가능 학습표본 데이터 x1 x2 class 5 7 1 4 3 -1 7 8 1 8 6 1 3 6 -1 2 5 -1 6 6 1 9 6 1 5 4 -1 Table 4.1와 같이 두 독립변수 x1, x2와 이분형 종속변수 class의 관측값으로 이루어진 9개의 학습표본을 train_df라는 data frame에 저장한다. library(e1071) svm_model &lt;- svm(as.factor(class) ~ x1 + x2, data = train_df, kernel = &quot;linear&quot;, scale = FALSE) plot(svm_model, data = train_df, formula = x2 ~ x1, grid = 200) Figure 4.1: 선형 SVM 분리 하이퍼플레인 그림 4.1에서 각 객체의 기호는 서포트 벡터 여부(“X”이면 서포트 벡터), 각 객체의 색상은 범주값(검정 = -1, 빨강 = 1)을 나타내며, 분리 하이퍼플레인은 아래와 같다. \\[ 0.6666667 x_{1} + 0.6666667 x_{2} = 7 \\] 4.3.2 기호 정의 본 장에서 사용될 수학적 기호는 아래와 같다. \\(\\mathbf{x} \\in \\mathbb{R}^p\\): p차원 변수벡터 \\(y \\in \\{-1, 1\\}\\): 범주 \\(N\\): 객체 수 \\((\\mathbf{x}_i, y_i)\\): \\(i\\)번째 객체의 변수벡터와 범주값 4.3.3 최적 하이퍼플레인 선형 SVM은 주어진 객체들의 두 범주를 완벽하게 분리하는 하이퍼플레인 중 각 범주의 서포트 벡터들로부터의 거리가 최대가 되는 하이퍼플레인을 찾는 문제로 귀착된다. 우선 아래와 같이 하이퍼플레인을 정의한다. \\[\\begin{equation} \\mathbf{w}^\\top \\mathbf{x} + b = 0 \\tag{4.1} \\end{equation}\\] 여기서 \\(\\mathbf{w} \\in \\mathbb{R}^p\\)와 \\(b \\in \\mathbb{R}\\)이 하이퍼플레인의 계수이다. 범주값이 1인 객체들 중 하이퍼플레인에서 가장 가까운 객체에 대해 다음과 같은 조건이 만족한다고 가정하자. \\[ H_1: \\mathbf{w}^\\top \\mathbf{x} + b = 1 \\] 또한 범주값이 -1인 객체들 중 하이퍼플레인에서 가장 가까운 객체에 대해 다음과 같은 조건이 만족한다고 가정하자. \\[ H_2: \\mathbf{w}^\\top \\mathbf{x} + b = -1 \\] 이 때 두 하이퍼플레인 \\(H_1\\)과 \\(H_2\\) 간의 거리(margin)는 \\(2 / \\lVert \\mathbf{w} \\rVert\\)이다. 선형 SVM은 아래와 같이 \\(H_1\\)과 \\(H_2\\) 간의 거리를 최대로 하는 최적화 문제가 된다. \\[\\begin{equation*} \\begin{split} \\max \\text{ } &amp; \\frac{2}{\\mathbf{w}^\\top \\mathbf{w}}\\\\ \\text{s.t.}&amp; \\\\ &amp; \\mathbf{w}^\\top \\mathbf{x}_i + b \\ge 1 \\text{ for } y_i = 1\\\\ &amp; \\mathbf{w}^\\top \\mathbf{x}_i + b \\le -1 \\text{ for } y_i = -1 \\end{split} \\end{equation*}\\] 이를 간략히 정리하면 \\[\\begin{equation*} \\begin{split} \\min \\text{ } &amp; \\frac{\\mathbf{w}^\\top \\mathbf{w}}{2}\\\\ \\text{s.t.}&amp; \\\\ &amp; y_i \\left( \\mathbf{w}^\\top \\mathbf{x}_i + b \\right) \\ge 1 \\end{split} \\end{equation*}\\] 과 같이 정리할 수 있으며, 각 객체 \\(i\\)에 대한 제약조건에 라그랑지 계수(Lagrange multiplier) \\(\\alpha_i \\ge 0\\)를 도입하여 라그랑지 함수를 유도하면 식 (4.2)과 같은 최적화 문제가 된다. 이를 원문제(primal problem)라 하자. \\[\\begin{equation} \\begin{split} \\min \\text{ } &amp; L_P = \\frac{1}{2} \\mathbf{w}^\\top \\mathbf{w} + \\sum_{i = 1}^{N} \\alpha_i \\left[ y_i \\left( \\mathbf{w}^\\top \\mathbf{x}_i + b \\right) - 1 \\right]\\\\ \\text{s.t. } &amp; \\alpha_i \\ge 0, \\text{ } i = 1, \\cdots, N \\end{split} \\tag{4.2} \\end{equation}\\] 원문제 식 (4.2)에 대한 울프쌍대문제(Wolfe dual problem)는 아래 식 (4.3)과 같이 도출된다. 보다 자세한 내용은 교재(전치혁 2012) 참고. \\[\\begin{equation} \\begin{split} \\max \\text{ } &amp; L_D = \\sum_{i = 1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^\\top \\mathbf{x}_j\\\\ \\text{s.t. } &amp;\\\\ &amp; \\sum_{i = 1}^{N} \\alpha_i y_i = 0\\\\ &amp; \\alpha_i \\ge 0, \\text{ } i = 1, \\cdots, N \\end{split} \\tag{4.3} \\end{equation}\\] 식 (4.3)은 이차계획(quadratic programming) 문제로, 각종 소프트웨어와 알고리즘을 이용하여 구할 수 있다. 본 장에서는 quadprog 패키지를 이용하여 해를 구하기로 한다. 이는 실제로 e1071의 svm 함수 호출 시 사용하는 방법은 아니며, 실제 svm 함수가 호출하는 알고리즘은 다음 장에서 다시 설명하기로 한다. quadprog의 solve.QP 함수는 아래와 같은 형태로 formulation된 문제(Goldfarb and Idnani 1983)에 대한 최적해를 구한다. \\[\\begin{equation} \\begin{split} \\min \\text{ } &amp; -\\mathbf{d}^{\\top}\\boldsymbol{\\alpha} + \\frac{1}{2} \\boldsymbol{\\alpha}^{\\top}\\mathbf{D}\\boldsymbol{\\alpha}\\\\ \\text{s.t. } &amp; \\mathbf{A}^{\\top}\\boldsymbol{\\alpha} \\ge \\mathbf{b}_0 \\end{split} \\tag{4.4} \\end{equation}\\] 식 (4.4)과 식 (4.3)이 동일한 문제를 나타내도록 아래와 같이 목적함수에 필요한 벡터 및 행렬을 정의한다. \\[\\begin{eqnarray*} \\mathbf{d} &amp;=&amp; \\mathbf{1}_{N \\times 1}\\\\ \\mathbf{D} &amp;=&amp; \\mathbf{y}\\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{X}^{\\top} \\end{eqnarray*}\\] where \\[\\begin{eqnarray*} \\mathbf{y} &amp;=&amp; \\left[ \\begin{array}{c c c c} y_1 &amp; y_2 &amp; \\cdots &amp; y_N \\end{array} \\right]^\\top\\\\ \\mathbf{X} &amp;=&amp; \\left[ \\begin{array}{c c c c} \\mathbf{x}_1 &amp; \\mathbf{x}_2 &amp; \\cdots &amp; \\mathbf{x}_N \\end{array} \\right]^{\\top} \\end{eqnarray*}\\] N &lt;- dim(train_df)[1] X &lt;- train_df[c(&#39;x1&#39;, &#39;x2&#39;)] %&gt;% as.matrix() y &lt;- train_df[[&#39;class&#39;]] %&gt;% as.numeric() d &lt;- rep(1, N) D &lt;- (y %*% t(y)) * (X %*% t(X)) 여기에서 행렬 \\(\\mathbf{D}\\)의 determinant 값은 0으로, Goldfarb and Idnani (1983) 가 가정하는 symmetric positive definite matrix 조건에 위배되어 solve.QP 함수 실행 시 오류가 발생한다. 이를 방지하기 위해 아래 예에서는 Matrix 패키지의 nearPD함수를 이용하여 행렬 \\(\\mathbf{D}\\)와 근사한 symmetric positive definite matrix를 아래와 같이 찾는다. D_pd &lt;- Matrix::nearPD(D, doSym = T)$mat %&gt;% as.matrix() 식 (4.4)의 제약식은 모두 inequality 형태로, 식 (4.3)의 equality constraint \\(\\sum_{i = 1}^{N} \\alpha_i y_i = 0\\)를 표현하기 위해서 두 개의 제약식 \\(\\sum_{i = 1}^{N} \\alpha_i y_i \\ge 0\\)와 \\(\\sum_{i = 1}^{N} - \\alpha_i y_i \\ge 0\\)를 생성한다. \\[\\begin{equation*} \\mathbf{A}^\\top = \\left[ \\begin{array}{c c c c} y_1 &amp; y_2 &amp; \\cdots &amp; y_N\\\\ -y_1 &amp; -y_2 &amp; \\cdots &amp; -y_N\\\\ 1 &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; 1 &amp; \\cdots &amp; 0\\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{array} \\right], \\mathbf{b}_0 = \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\cdots \\\\ 0 \\end{array} \\right] \\end{equation*}\\] A &lt;- cbind( y, -y, diag(N) ) b_zero &lt;- rep(0, 2 + N) 이제 위에서 구한 행렬과 벡터들을 solve.QP 함수에 입력하여 최적해를 구한다. res &lt;- quadprog::solve.QP(D_pd, d, A, b_zero) alpha_sol &lt;- res$solution obj_val &lt;- -res$value Table 4.2: 이차계획문제의 최적해 variable solution alpha_1 0.2234 alpha_2 0.0000 alpha_3 0.0000 alpha_4 0.0000 alpha_5 0.2228 alpha_6 0.0000 alpha_7 0.2210 alpha_8 0.0000 alpha_9 0.2216 표 4.2의 결과는 교재(전치혁 2012)에 나타난 최적해와는 다소 차이가 있으나, 결과적으로 목적함수값은 0.4444로 동일하다. 위의 과정으로 최적해 \\(\\alpha_{i}^{*}\\)를 구한 뒤, 아래와 같이 분리 하이퍼플레인의 계수를 결정할 수 있다. \\[\\begin{eqnarray*} \\mathbf{w} &amp;=&amp; \\sum_{i = 1}^{N} \\alpha_{i}^{*} y_{i} \\mathbf{x}_{i}\\\\ b &amp;=&amp; \\sum_{i: \\alpha_{i}^{*} &gt; 0} \\frac{1 - y_{i} \\mathbf{w}^{\\top} \\mathbf{x}_{i}}{y_{i}} \\left/ \\sum_{i: \\alpha_{i}^{*} &gt; 0} 1 \\right. \\end{eqnarray*}\\] w &lt;- colSums(alpha_sol * y * X) print(w) ## x1 x2 ## 0.6666658 0.6666657 sv_ind &lt;- which(round(alpha_sol, digits = 4) &gt; 0) b &lt;- mean((1 - y[sv_ind] * (X[sv_ind, ] %*% w)) / y[sv_ind]) print(b) ## [1] -6.99999 위 결과와 같이, 분리 하이퍼플레인은 교재와 동일하게 얻어진다. 4.4 선형 SVM - 분리 불가능 경우 본 장에서는 학습표본 내의 두 범주가 어떠한 선형 하이퍼플레인으로도 완전하게 분리되지 않아 식 (4.2)이 해를 갖지 못하는 경우에 대한 문제를 다룬다. 4.4.1 기본 R 스크립트 앞 장에서 사용한 학습표본에 아래와 같이 하나의 객체를 추가하여 전체 학습표본이 선형 하이퍼플레인으로 분리될 수 없도록 하자. inseparable_train_df &lt;- bind_rows(train_df, tibble(x1 = 7, x2 = 6, class = -1)) knitr::kable(inseparable_train_df, booktabs = TRUE, align = c(&#39;r&#39;, &#39;r&#39;, &#39;r&#39;), caption = &#39;선형분리불가능 학습표본 데이터&#39;) Table 4.3: 선형분리불가능 학습표본 데이터 x1 x2 class 5 7 1 4 3 -1 7 8 1 8 6 1 3 6 -1 2 5 -1 6 6 1 9 6 1 5 4 -1 7 6 -1 library(e1071) svm_model &lt;- svm(as.factor(class) ~ x1 + x2, data = inseparable_train_df, kernel = &quot;linear&quot;, cost = 1, scale = FALSE) plot(svm_model, data = inseparable_train_df, formula = x2 ~ x1, grid = 200) Figure 4.2: 선형 SVM 분리 불가능 경우의 하이퍼플레인 Figure 4.2에서 보이듯, 하나의 검정 객체(범주 = -1)가 범주 1로 분류되는 영역에 존재하여 오분류가 발생한다. 이처럼 선형 하이퍼플레인으로 두 범주 학습표본의 분리가 불가능한 경우, 오분류 학습표본에 대한 페널티를 적용하여 최적 분리 하이퍼플레인을 도출하게 된다. 위 예에서의 최적 하이퍼플레인은 아래와 같다. \\[ 0.6 x_{1} + 0.8 x_{2} = 7.6 \\] 4.4.2 최적 하이퍼플레인 여유변수(slack variable) \\(\\xi_i\\) 를 각 학습객체 \\(i = 1, \\cdots, N\\)에 대해 아래와 같이 정의한다. \\[\\begin{equation*} \\xi_i = \\max \\left\\{ 0, 1 - y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\right\\} \\end{equation*}\\] 이는 객체가 자신의 범주의 서포트 벡터를 지나는 하이퍼플레인(범주 1인 경우 \\(H_1\\), 범주 -1인 경우 \\(H_2\\))으로 부터 다른 범주 방향으로 떨어진 거리를 나타낸다. 이 여유변수 \\(\\xi_i\\)에 단위당 페널티 단가 \\(C\\)를 부여하여 아래와 같은 최적화 문제를 정의한다. \\[\\begin{equation*} \\begin{split} \\min \\text{ } &amp; \\frac{\\mathbf{w}^\\top \\mathbf{w}}{2} + C \\sum_{i = 1}^{N} \\xi_i \\\\ \\text{s.t.}&amp; \\\\ &amp; y_i \\left( \\mathbf{w}^\\top \\mathbf{x}_i + b \\right) \\ge 1 - \\xi_i, \\text{ } i = 1, \\cdots, N \\\\ &amp; \\xi \\ge 0, \\text{ } i = 1, \\cdots, N \\end{split} \\end{equation*}\\] 이에 대한 울프쌍대문제를 앞 4.3.3장과 같은 과정으로 도출하면 아래 식 (4.5)와 같다. \\[\\begin{equation} \\begin{split} \\max \\text{ } &amp; L_D = \\sum_{i = 1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^\\top \\mathbf{x}_j\\\\ \\text{s.t. } &amp;\\\\ &amp; \\sum_{i = 1}^{N} \\alpha_i y_i = 0\\\\ &amp; 0 \\le \\alpha_i \\le C, \\text{ } i = 1, \\cdots, N \\end{split} \\tag{4.5} \\end{equation}\\] 이는 분리 가능 경우의 식 (4.3)에 변수 \\(\\alpha_i\\)에 대한 상한값 \\(C\\)의 제약이 추가된 문제로, 이는 e1071 패키지의 svm 함수가 기본 방법으로 사용하는 LIBSVM 라이브러리(Chang and Lin 2011)의 \\(C\\)-support vector classification(\\(C\\)-SVC)이 사용하는 문제식이며, LIBSVM 라이브러리는 특정 알고리즘(Fan, Chen, and Lin 2005)을 이용하여 해를 제공한다. 아래 svm 함수의 입력 변수에서 type = &quot;C-classification&quot;은 식 (4.3)를 최적화하겠다는 것을 나타내며, cost = 1은 페널티 단가 \\(C\\)의 값을 1로 설정하겠다는 것을 나타낸다. svm_model &lt;- svm(as.factor(class) ~ x1 + x2, data = inseparable_train_df, kernel = &quot;linear&quot;, scale = FALSE, type = &quot;C-classification&quot;, cost = 1) 위 결과 모델 객체 svm_model의 원소 중 index는 학습표본 중 서포트 벡터에 해당하는 인덱스 \\(i\\)를 나타내며, coefs는 각 서포트 벡터의 \\(\\alpha_i y_i\\) 값을 나타낸다. 따라서, coefs를 각 서포트 벡터의 범주값 \\(y_i\\)로 나누면 식 (4.5)의 최적해를 아래와 같이 볼 수 있다. N &lt;- dim(inseparable_train_df)[1] X &lt;- inseparable_train_df[c(&#39;x1&#39;, &#39;x2&#39;)] %&gt;% as.matrix() y &lt;- inseparable_train_df[[&#39;class&#39;]] %&gt;% as.numeric() sv_ind &lt;- svm_model$index alpha_sol &lt;- vector(&quot;numeric&quot;, N) alpha_sol[sv_ind] &lt;- drop(svm_model$coefs[, 1]) / y[sv_ind] Table 4.4: 이차계획문제의 최적해: 선형 분리 불가능 경우 variable solution alpha_1 0.8 alpha_2 0.0 alpha_3 0.0 alpha_4 0.0 alpha_5 0.8 alpha_6 0.0 alpha_7 1.0 alpha_8 0.0 alpha_9 0.0 alpha_10 1.0 하이퍼플레인의 계수 \\(\\mathbf{w}\\)는 분리 가능의 경우와 동일하게 구할 수 있다. \\[\\begin{equation*} \\mathbf{w} = \\sum_{i = 1}^{N} \\alpha_{i}^{*} y_{i} \\mathbf{x}_{i} \\end{equation*}\\] w &lt;- colSums(alpha_sol * y * X) print(w) ## x1 x2 ## 0.6 0.8 상수 \\(b\\)는 아래와 같이 \\(0 &lt; \\alpha_{i}^{*} &lt; C\\)인 객체들을 이용해 산출할 수 있다. \\[\\begin{equation*} b = \\sum_{i: 0 &lt; \\alpha_{i}^{*} &lt; C} \\frac{1 - y_{i} \\mathbf{w}^{\\top} \\mathbf{x}_{i}}{y_{i}} \\left/ \\sum_{i: 0 &lt; \\alpha_{i}^{*} &lt; C} 1 \\right. \\end{equation*}\\] ind &lt;- sv_ind[alpha_sol[sv_ind] &lt; svm_model$cost] b &lt;- mean((1 - y[ind] * (X[ind, ] %*% w)) / y[ind]) print(b) ## [1] -7.6 위와 같은 하이퍼플레인의 계수 \\(\\mathbf{w}\\)와 상수 \\(b\\)값은 svm 객체에 원소들을 이용하여 보다 쉽게 확인할 수 있다. w &lt;- t(svm_model$coefs) %*% svm_model$SV print(w) ## x1 x2 ## [1,] 0.6 0.8 b &lt;- -svm_model$rho print(b) ## [1] -7.6 선형 하이퍼플레인으로 분리 불가능한 경우, 페널티 단가 \\(C\\)의 값에 따라 도출되는 분리 하이퍼플레인이 달라진다. \\(C\\)의 값이 1, 5, 100일 때의 하이퍼플레인을 비교해보자. svm_models &lt;- lapply(c(1, 5, 100), function(C) svm(as.factor(class) ~ x1 + x2, data = inseparable_train_df, kernel = &quot;linear&quot;, scale = FALSE, type = &quot;C-classification&quot;, cost = C)) getHyperplane &lt;- function(model) { list(C = model$cost, w = paste(round(t(model$coefs) %*% model$SV, digits = 2), collapse = &quot;, &quot;), b = -round(model$rho, digits = 2), sv = paste(model$index, collapse = &quot;, &quot;), misclassified = paste(which(model$fitted != as.factor(inseparable_train_df$class)), collapse = &quot;, &quot;)) } svm_summary &lt;- lapply(svm_models, getHyperplane) %&gt;% bind_rows() Table 4.5: 페널티 단가 C에 따른 하이퍼플레인 계수 및 결과 페널티 단가 \\(C\\) \\((w_1, w_2)\\) \\(b\\) 서포트 벡터 객체 오분류 객체 1 0.6, 0.8 -7.6 1, 7, 5, 10 10 5 0.4, 1.2 -9.4 1, 4, 7, 5, 10 10 100 0.4, 1.2 -9.4 1, 4, 7, 5, 10 10 Table 4.5에서 보이는 바와 같이, 페널티 단가 \\(C\\)의 값이 1과 5일 때 분리 하이퍼플레인이 변하는 것을 볼 수 있다. \\(C\\)값이 5와 100일 때의 분리 하이퍼플레인은 거의 동일하다. plot(svm_models[[2]], data = inseparable_train_df, formula = x2 ~ x1, grid = 200) Figure 4.3: 선형 SVM 분리 불가능 경우의 하이퍼플레인 (\\(C = 5\\)) Figure 4.3의 하이퍼플레인(\\(C = 5\\)인 경우)은 Figure 4.2의 하이퍼플레인(\\(C = 1\\)인 경우)보다 오분류 객체에 가깝게 위치함을 확인할 수 있다. 4.5 비선형 SVM 본 장에서는 선형으로 분리 성능이 좋지 않은 경우에 대해 원 입력변수에 대해 비선형인 하이퍼플레인을 찾는 문제를 다룬다. 이는 원 입력변수에 대해 비선형인 기저함수 공간으로 객체를 이동시킨 후 해당 공간에서 선형 분리 하이퍼플레인을 찾는 과정이다. 4.5.1 기본 R 스크립트 nonlinear_train_df &lt;- tibble( x1 = c(5, 4, 7, 8, 3, 2, 6, 9, 5), x2 = c(7, 3, 8, 6, 6, 5, 6, 6, 4), class = c(1, -1, -1, -1, 1, 1, 1, -1, -1) ) knitr::kable(nonlinear_train_df, booktabs = TRUE, align = c(&#39;r&#39;, &#39;r&#39;, &#39;r&#39;), caption = &#39;비선형 SVM 학습표본 데이터&#39;) Table 4.6: 비선형 SVM 학습표본 데이터 x1 x2 class 5 7 1 4 3 -1 7 8 -1 8 6 -1 3 6 1 2 5 1 6 6 1 9 6 -1 5 4 -1 library(e1071) svm_model &lt;- svm(as.factor(class) ~ x1 + x2, data = nonlinear_train_df, kernel = &quot;polynomial&quot;, coef0 = 1, gamma = 1, degree = 2, cost = 5, scale = FALSE) plot(svm_model, data = nonlinear_train_df, formula = x2 ~ x1, grid = 200) Figure 4.4: 비선형 SVM 하이퍼플레인 4.5.2 최적 하이퍼플레인 식 (4.1)을 일반화한 다음과 같은 하이퍼플레인을 고려하자. \\[\\begin{equation} f(\\mathbf{x}) = \\Phi(\\mathbf{x})^\\top \\mathbf{w} + b \\tag{4.6} \\end{equation}\\] 여기서 벡터함수 \\(\\Phi: \\mathbb{R}^p \\rightarrow \\mathbb{R}^m\\)는 \\(\\mathbf{x}\\)에 대한 새로운 특징(feature)을 추출하는 변환함수라 할 수 있는데, 통상 추출되는 특징의 차원 \\(m\\)이 원 변수 \\(\\mathbf{x}\\)의 차원 \\(p\\)보다 높다. 이를 \\(\\mathbf{x}\\)의 기저함수(basis function)라 부르며, 하이퍼플레인 계수 또한 \\(m\\)차원의 벡터가 된다 (\\(\\mathbf{w} \\in \\mathbb{R}^m\\)). 이 때, 비선형 SVM 문제는 선형 SVM 문제 식 (4.5)에서 변수를 기저변수로 치환한 형태로 아래 식 (4.7)과 같이 나타낼 수 있다. \\[\\begin{equation} \\begin{split} \\max \\text{ } &amp; L_D = \\sum_{i = 1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} \\alpha_i \\alpha_j y_i y_j \\Phi(\\mathbf{x}_i)^\\top \\Phi(\\mathbf{x}_j)\\\\ \\text{s.t. } &amp;\\\\ &amp; \\sum_{i = 1}^{N} \\alpha_i y_i = 0\\\\ &amp; 0 \\le \\alpha_i \\le C, \\text{ } i = 1, \\cdots, N \\end{split} \\tag{4.7} \\end{equation}\\] 식 (4.7)의 목적함수에서 기저함수의 내적 \\(\\Phi(\\mathbf{x}_i)^\\top \\Phi(\\mathbf{x}_j)\\)을 아래와 같이 커널함수(kernel function)로 나타낼 수 있으며, 이는 두 객체 \\(\\mathbf{x}_i, \\mathbf{x}_j\\)간의 일종의 유사성 척도(similarity measure)로 해석될 수 있다. \\[\\begin{equation*} K(\\mathbf{x}_i, \\mathbf{x}_j) = \\Phi(\\mathbf{x}_i)^\\top \\Phi(\\mathbf{x}_j) \\end{equation*}\\] 널리 사용되는 커널함수로는 아래와 같은 함수들이 있다. \\[\\begin{eqnarray*} \\text{Gaussian RBF:} &amp; &amp; K(\\mathbf{x}_i, \\mathbf{x}_j) &amp;=&amp; \\exp \\left( \\frac{- \\left\\lVert \\mathbf{x}_i - \\mathbf{x}_j \\right\\rVert^2}{2 \\sigma^2} \\right)\\\\ \\text{$r$-th order polynomial:} &amp; &amp; K(\\mathbf{x}_i, \\mathbf{x}_j) &amp;=&amp; \\left( \\mathbf{x}_i^\\top \\mathbf{x}_j + 1 \\right)^r \\\\ \\text{Sigmoid:} &amp; &amp; K(\\mathbf{x}_i, \\mathbf{x}_j) &amp;=&amp; \\tanh \\left(\\kappa \\mathbf{x}_i^\\top \\mathbf{x}_j - \\delta \\right) \\end{eqnarray*}\\] 커널함수를 이용하여 분리 하이퍼플레인을 찾기 위한 식을 아래와 같이 나타낸다. \\[\\begin{equation} \\begin{split} \\max \\text{ } &amp; L_D = \\sum_{i = 1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} \\alpha_i \\alpha_j y_i y_j k_{ij}\\\\ \\text{s.t. } &amp;\\\\ &amp; \\sum_{i = 1}^{N} \\alpha_i y_i = 0\\\\ &amp; 0 \\le \\alpha_i \\le C, \\text{ } i = 1, \\cdots, N \\end{split} \\tag{4.8} \\end{equation}\\] 이 때 \\(k_{ij}\\)는 \\(K(\\mathbf{x}_i, \\mathbf{x}_j)\\)를 나타낸다. 식 (4.8)의 최적해 \\(\\boldsymbol\\alpha^*\\)는 선형 SVM과 마찬가지로 이차계획(quadratic programming) 소프트웨어/알고리즘을 이용하여 구할 수 있다. Table 4.6의 학습데이터에 대해 e1071 패키지의 svm 함수를 이용하여 이차 다항 커널에 기반한 분리 하이퍼플레인을 구해보자. svm 함수에 파라미터값 kernel = &quot;polynomial&quot;를 설정함으로써 다항 커널을 사용할 수 있다. svm 함수의 다항 커널은 위에서 설명된 것보다 일반화된 형태로 아래와 같이 정의된다. \\[\\begin{equation*} K(\\mathbf{x}_i, \\mathbf{x}_j) = \\left( \\gamma \\mathbf{x}_i^\\top \\mathbf{x}_j + \\beta_0 \\right)^r \\end{equation*}\\] 위 커널함수의 파라미터 \\(\\gamma, \\beta_0, r\\)은 svm 함수에 파라미터 gamma, coef0, degree로 각각 정의된다. 따라서 이차 커널 \\[\\begin{equation*} K(\\mathbf{x}_i, \\mathbf{x}_j) = \\left( \\mathbf{x}_i^\\top \\mathbf{x}_j + 1 \\right)^2 \\end{equation*}\\] 에 기반한 SVM을 학습하기 위해서 아래와 같이 svm 함수를 호출한다. svm_model &lt;- svm(as.factor(class) ~ x1 + x2, data = nonlinear_train_df, kernel = &quot;polynomial&quot;, coef0 = 1, gamma = 1, degree = 2, scale = FALSE) 위 함수 호출 결과 서포트 벡터 객체는 1, 7, 2, 3, 9이다. 비선형 SVM의 분리 하이퍼플레인 또한 페널티 단가 \\(C\\)의 값에 따라 달라진다. 선형 SVM의 경우와 같이 \\(C = 1, 5, 100\\)에 대해 각각 비선형 SVM을 구해보자. svm_models &lt;- lapply( c(1, 5, 100), function(C) svm(as.factor(class) ~ x1 + x2, data = nonlinear_train_df, kernel = &quot;polynomial&quot;, coef0 = 1, gamma = 1, degree = 2, cost = C, scale = FALSE) ) getSummary &lt;- function(model) { list(C = model$cost, sv = paste(model$index, collapse = &quot;, &quot;), misclassified = paste(which(model$fitted != as.factor(nonlinear_train_df$class)), collapse = &quot;, &quot;)) } svm_summary &lt;- lapply(svm_models, getSummary) %&gt;% bind_rows() Table 4.7: 페널티 단가 C에 따른 비선형 SVM 결과 페널티 단가 \\(C\\) 서포트 벡터 객체 오분류 객체 1 1, 7, 2, 3, 9 7 5 6, 7, 2, 3, 9 100 6, 7, 2, 3, 9 4.6 R패키지 내 SVM 4.6.1 커널함수 앞 장에서는 선형 커널과 다항 커널함수의 예를 살펴보았다. 본 장에서는 가우시안 커널 및 시그모이드 커널을 사용하는 법을 살펴보자. 가우시안 커널의 경우 \\[\\begin{equation*} K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp \\left( -\\gamma \\left\\lVert \\mathbf{x}_i - \\mathbf{x}_j \\right\\rVert^2 \\right) \\end{equation*}\\] 과 같이 \\(\\gamma\\) 파라미터를 이용하여 함수를 정의하며, svm 함수에 gamma 파라미터값을 통해 설정할 수 있다. svm_model &lt;- svm(as.factor(class) ~ x1 + x2, data = nonlinear_train_df, kernel = &quot;radial&quot;, gamma = 1, cost = 5, scale = FALSE) plot(svm_model, data = nonlinear_train_df, formula = x2 ~ x1, grid = 200) Figure 4.5: 가우시안 커널을 이용한 비선형 SVM 하이퍼플레인 시그모이드 커널의 경우 \\[\\begin{equation*} K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh \\left(\\gamma \\mathbf{x}_i^\\top \\mathbf{x}_j + \\beta_0 \\right) \\end{equation*}\\] 와 같이 두 파라미터 \\(\\gamma, \\beta_0\\)의 값에 대응하는 svm 함수의 파라미터 gamma, coef0 값을 통해 설정할 수 있다. svm_model &lt;- svm(as.factor(class) ~ x1 + x2, data = nonlinear_train_df, kernel = &quot;sigmoid&quot;, gamma = 0.01, coef0 = -1, cost = 5, scale = FALSE) plot(svm_model, data = nonlinear_train_df, formula = x2 ~ x1, grid = 200) Figure 4.6: 시그모이드 커널을 이용한 비선형 SVM 하이퍼플레인 커널 함수의 종류 kernel, 커널 함수의 파라미터 gamma, coef0, degree, 페널티 단가 cost등의 svm 함수 파라미터는 학습 표본과는 별도의 테스트 데이터에 대해 오분류율을 최소화하는 값을 선택하는 것이 일반적이다. 4.6.2 \\(\\nu\\)-SVC \\(\\nu\\)-support vector classification(\\(\\nu\\)-SVC) (Schölkopf et al. 2000, Chang and Lin (2001))은 \\(C\\)-SVC의 이차계획식 (4.8)과 다른 형태로, 페널티 단가 \\(C\\) 대신 \\(\\nu\\)라는 파라미터를 이용한 아래 최적화 문제의 해를 구한다. \\[\\begin{equation} \\begin{split} \\min \\text{ } &amp; L_D = \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} \\alpha_i \\alpha_j y_i y_j k_{ij}\\\\ \\text{s.t. } &amp;\\\\ &amp; \\sum_{i = 1}^{N} \\alpha_i y_i = 0\\\\ &amp; 0 \\le \\alpha_i \\le \\frac{1}{N}, \\text{ } i = 1, \\cdots, N\\\\ &amp; \\sum_{i = 1}^{N} \\alpha_i = \\nu \\end{split} \\tag{4.9} \\end{equation}\\] 이 때, 각 \\(\\alpha_i\\)의 최대값은 \\(1/N\\)으로, \\(\\nu\\)를 포함한 제약식을 무시할 때 모든 객체에 대한 \\(\\alpha_i\\)값의 합의 이론적 최대치는 1이 되며, \\(\\nu \\in (0, 1]\\)은 전체 객체 중 서포트 벡터 객체의 개수를 제한하는 개념으로 생각할 수 있다. 식 (4.9)이 실제로 최적해를 가지기 위한 \\(\\nu\\)값의 범위는 \\[\\begin{equation*} 0 &lt; \\nu \\le \\frac{2}{N} \\min \\left( \\sum_i I(y_i = 1), \\sum_i I(y_i = -1) \\right) \\end{equation*}\\] 으로 (Chang and Lin 2001), 에를 들어 범주 1에 속하는 학습표본 객체 수가 전체의 10% 라면, \\(\\nu\\) 값은 최대 0.2 까지 설정할 수 있다. 또한 svm 함수가 호출하는 LIBSVM 라이브러리는 위 식 (4.9)을 \\(N\\)이 큰(학습 표본 수가 매우 많은) 경우에도 안정된 결과를 얻을 수 있도록 아래와 같이 변환한 문제를 다룬다. \\[\\begin{equation} \\begin{split} \\min \\text{ } &amp; L_D = \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} \\bar{\\alpha}_i \\bar{\\alpha}_j y_i y_j k_{ij}\\\\ \\text{s.t. } &amp;\\\\ &amp; \\sum_{i = 1}^{N} \\bar{\\alpha}_i y_i = 0\\\\ &amp; 0 \\le \\bar{\\alpha}_i \\le 1, \\text{ } i = 1, \\cdots, N\\\\ &amp; \\sum_{i = 1}^{N} \\bar{\\alpha}_i = \\nu N \\end{split} \\tag{4.10} \\end{equation}\\] 이 때 \\(\\bar{\\alpha}_i = \\alpha_i N\\)이다. \\(\\nu\\)-SVC은 아래와 같이 svm 함수를 호출할 때 type = &quot;nu-classification&quot;과 파라미터 nu 값을 설정함으로써 학습할 수 있다. svm_model &lt;- svm(as.factor(class) ~ x1 + x2, data = nonlinear_train_df, type = &quot;nu-classification&quot;, kernel = &quot;radial&quot;, gamma = 1, nu = 0.5, scale = FALSE) plot(svm_model, data = nonlinear_train_df, formula = x2 ~ x1, grid = 200) Figure 4.7: 가우시안 커널을 이용한 \\(\\nu\\)-SVC 하이퍼플레인 References "],
["references.html", "References", " References "]
]
