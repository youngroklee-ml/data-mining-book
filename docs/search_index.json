[
["index.html", "데이터마이닝 with R 개요", " 데이터마이닝 with R 전치혁, 이혜선, 이종석, 이영록 2018-09-21 개요 본 사이트는 전치혁 교수님의 책 을 기반으로 한 R 예제를 제공할 목적으로 만들어졌으며, 지속적으로 업데이트될 예정입니다. 본 사이트의 R 예제들은 R 3.5.1 version에서 수행되었으며, R 프로그램은 CRAN에서 다운로드받아 설치할 수 있습니다. 본 사이트는 R을 이용한 데이터마이닝 수행에 초점을 두고 있으며, 예제 수행을 위해서는 기본적인 R 프로그래밍 지식이 필요합니다. R 프로그래밍에 대한 지식은 아래와 같은 자료들로부터 얻을 수 있습니다. DataCamp’s free R Tutorial: https://www.datacamp.com/courses/free-introduction-to-r Advanced R (by Hadley Wickham): https://adv-r.hadley.nz "],
["da.html", "Chapter 1 판별분석 1.1 개요 1.2 필요 R 패키지 설치 1.3 피셔 방법 1.4 의사결정론에 의한 분류규칙 1.5 오분류비용을 고려한 분류규칙 1.6 이차판별분석 1.7 세 범주 이상의 분류", " Chapter 1 판별분석 1.1 개요 판별분석(discriminant analysis)은 범주들을 가장 잘 구별하는 변수들의 하나 또는 다수의 함수를 도출하여 이를 기반으로 분류규칙을 제시한다. 본 장에서는 변수의 분포에 대한 가정이 필요 없는 피셔(Fisher) 방법과 다변량 정규분포를 가정하는 선형 및 비선형 판별분석을 설명한다. 1.2 필요 R 패키지 설치 본 장에서 필요한 R 패키지들은 아래와 같다. package version tidyverse 1.2.1 MASS 7.3-50 1.3 피셔 방법 1.3.1 기본 R 스크립트 train_df &lt;- tibble( id = c(1:9), x1 = c(5, 4, 7, 8, 3, 2, 6, 9, 5), x2 = c(7, 3, 8, 6, 6, 5, 6, 6, 4), class = factor(c(1, 2, 2, 2, 1, 1, 1, 2, 2), levels = c(1, 2)) ) knitr::kable(train_df, booktabs = TRUE, align = c(&#39;r&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;), col.names = c(&#39;객체번호&#39;, &#39;$x_1$&#39;, &#39;$x_2$&#39;, &#39;범주&#39;), caption = &#39;판별분석 학습표본 데이터&#39;) Table 1.1: 판별분석 학습표본 데이터 객체번호 \\(x_1\\) \\(x_2\\) 범주 1 5 7 1 2 4 3 2 3 7 8 2 4 8 6 2 5 3 6 1 6 2 5 1 7 6 6 1 8 9 6 2 9 5 4 2 Table 1.1와 같이 두 독립변수 x1, x2와 이분형 종속변수 class의 관측값으로 이루어진 9개의 학습표본을 train_df라는 data frame에 저장한다. fisher_da &lt;- MASS::lda(class ~ x1 + x2, train_df) print(fisher_da) ## Call: ## lda(class ~ x1 + x2, data = train_df) ## ## Prior probabilities of groups: ## 1 2 ## 0.4444444 0.5555556 ## ## Group means: ## x1 x2 ## 1 4.0 6.0 ## 2 6.6 5.4 ## ## Coefficients of linear discriminants: ## LD1 ## x1 0.6850490 ## x2 -0.7003859 1.3.2 피셔 판별함수 각 객체는 변수벡터 \\(\\mathbf{x} \\in \\mathbb{R}^p\\)와 범주 \\(y \\in \\{1, 2\\}\\)로 이루어진다고 하자. 아래는 변수 \\(\\mathbf{x}\\)의 기대치와 분산-공분산행렬(varinace-covariance matrix)을 나타낸다. \\[\\begin{eqnarray*} \\boldsymbol\\mu_1 = E(\\mathbf{x} | y = 1)\\\\ \\boldsymbol\\mu_2 = E(\\mathbf{x} | y = 2)\\\\ \\boldsymbol\\Sigma = Var(\\mathbf{x} | y = 1) = Var(\\mathbf{x} | y = 2) \\end{eqnarray*}\\] 다음과 같이 변수들의 선형조합으로 새로운 변수 \\(z\\)를 형성하는 함수를 피셔 판별함수(Fisher’s discriminant function)라 한다. \\[\\begin{equation} z = \\mathbf{w}^\\top \\mathbf{x} \\tag{1.1} \\end{equation}\\] 여기서 계수벡터 \\(\\mathbf{w} \\in \\mathbb{R}^p\\)는 통상 아래와 같이 변수 \\(z\\)의 범주간 평균 차이 대 변수 \\(z\\)의 분산의 비율을 최대화하는 것으로 결정한다. \\[\\begin{equation} {\\arg\\!\\min}_{\\mathbf{w}} \\frac{\\mathbf{w}^\\top ( \\boldsymbol\\mu_1 - \\boldsymbol\\mu_2 )}{\\mathbf{w}^\\top \\boldsymbol\\Sigma \\mathbf{w}} \\tag{1.2} \\end{equation}\\] 위 식 (1.2)의 해는 \\[\\begin{equation*} \\mathbf{w} \\propto \\boldsymbol\\Sigma^{-1}(\\boldsymbol\\mu_1 - \\boldsymbol\\mu_2) \\end{equation*}\\] 의 조건을 만족하며, 편의상 비례상수를 1로 두면 아래와 같은 해가 얻어진다. \\[\\begin{equation} \\mathbf{w} = \\boldsymbol\\Sigma^{-1}(\\boldsymbol\\mu_1 - \\boldsymbol\\mu_2) \\tag{1.3} \\end{equation}\\] 실제 모집단의 평균 및 분산을 알지 못하는 경우, 학습표본으로부터 \\(\\boldsymbol\\mu_1, \\boldsymbol\\mu_2, \\boldsymbol\\Sigma\\)의 추정치를 얻어 식 (1.3)에 대입하는 방식으로 판별계수를 추정한다. 자세한 내용은 교재 (전치혁 2012) 참조. Table 1.1에 주어진 학습표본을 이용하여 피셔 판별함수를 구해보도록 하자. 우선 각 범주별 평균벡터 \\(\\hat{\\boldsymbol\\mu}_1, \\hat{\\boldsymbol\\mu}_2\\)를 아래와 같이 구한다. mu_hat &lt;- train_df %&gt;% group_by(class) %&gt;% summarize(x1 = mean(x1), x2 = mean(x2)) %&gt;% arrange(class) print(mu_hat) ## # A tibble: 2 x 3 ## class x1 x2 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 6 ## 2 2 6.6 5.4 또한 범주별 표본 분산-공분산행렬 \\(\\mathbf{S}_1, \\mathbf{S}_2\\)를 다음과 같이 구한다. 리스트 S_within_group의 첫번째 원소는 범주 1의 분산-공분산행렬 \\(\\mathbf{S}_1\\), 두번째 원소는 범주 2의 분산-공분산행렬 \\(\\mathbf{S}_2\\)를 나타낸다. S_within_group &lt;- lapply( unique(train_df$class) %&gt;% sort(), function(x) { train_df %&gt;% filter(class == x) %&gt;% select(x1, x2) %&gt;% var() } ) print(S_within_group) ## [[1]] ## x1 x2 ## x1 3.333333 1.0000000 ## x2 1.000000 0.6666667 ## ## [[2]] ## x1 x2 ## x1 4.30 2.95 ## x2 2.95 3.80 위에서 얻은 범주별 표본 분산-공분산행렬을 이용하여 합동 분산-공분산행렬을 아래와 같이 추정한다. \\[\\begin{equation*} \\hat{\\boldsymbol\\Sigma} = \\mathbf{S}_p = \\frac{(n_1 - 1)\\mathbf{S}_1 + (n_2 - 1)\\mathbf{S}_2}{n_1 + n_2 - 2} \\end{equation*}\\] 이 때 \\(n_1, n_2\\)는 각각 범주 1, 2에 속한 학습표본 객체의 수를 나타낸다. 아래 R 스크립트에서는 임의의 범주 표본수 벡터 n과 범주별 표본 분산-공분산행렬 리스트 S에 대해 합동 분산-공분산행렬을 구하는 함수 pooled_variance를 정의하고, 주어진 학습표본에 대한 입력값을 대입하여 합동 분산-공분산행렬 추정치 Sigma_hat을 구한다. pooled_variance &lt;- function(n, S) { lapply(1:length(n), function(i) (n[i] - 1)*S[[i]]) %&gt;% Reduce(`+`, .) %&gt;% `/`(sum(n) - length(n)) } n_obs &lt;- train_df %&gt;% group_by(class) %&gt;% count() %&gt;% arrange(class) Sigma_hat &lt;- pooled_variance(n_obs$n, S_within_group) print(Sigma_hat) ## x1 x2 ## x1 3.885714 2.114286 ## x2 2.114286 2.457143 위에서 구한 추정치들을 이용하여 아래와 같이 판별함수 계수 추정치 \\(\\hat{\\mathbf{w}}\\)를 구한다. \\[\\begin{equation*} \\hat{\\mathbf{w}} = \\hat{\\boldsymbol\\Sigma}^{-1}(\\hat{\\boldsymbol\\mu}_1 - \\hat{\\boldsymbol\\mu}_2) \\end{equation*}\\] w_hat &lt;- solve(Sigma_hat) %*% t(mu_hat[1, c(&#39;x1&#39;, &#39;x2&#39;)] - mu_hat[2, c(&#39;x1&#39;, &#39;x2&#39;)]) print(w_hat) ## [,1] ## x1 -1.508039 ## x2 1.541801 1.3.3 분류 규칙 피셔 판별함수에 따른 분류 경계값은 학습표본에 대한 판별함수값의 평균으로 아래와 같이 구할 수 있다. \\[\\begin{equation*} \\bar{z} = \\sum_i^N \\hat{\\mathbf{w}}^\\top \\mathbf{x}_i \\end{equation*}\\] z_mean &lt;- t(w_hat) %*% (train_df %&gt;% select(x1, x2) %&gt;% colMeans()) %&gt;% drop() print(z_mean) ## [1] 0.526438 위 결과를 통해, 분류규칙은 다음과 같이 주어진다. \\(\\hat{\\mathbf{w}}^\\top \\mathbf{x} \\ge \\bar{z}\\) 이면, \\(\\mathbf{x}\\)를 범주 1로 분류 \\(\\hat{\\mathbf{w}}^\\top \\mathbf{x} &lt; \\bar{z}\\) 이면, \\(\\mathbf{x}\\)를 범주 2로 분류 train_prediction_df &lt;- train_df %&gt;% mutate( z = w_hat[1]*x1 + w_hat[2]*x2, predicted_class = factor(if_else(z &gt;= z_mean, 1, 2), levels = c(1, 2)) ) knitr::kable(train_prediction_df, booktabs = TRUE, align = c(&#39;r&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;), col.names = c(&#39;객체번호&#39;, &#39;$x_1$&#39;, &#39;$x_2$&#39;, &#39;실제범주&#39;, &#39;$z$&#39;, &#39;추정범주&#39;), caption = &#39;학습표본에 대한 피셔 분류 결과&#39;) Table 1.2: 학습표본에 대한 피셔 분류 결과 객체번호 \\(x_1\\) \\(x_2\\) 실제범주 \\(z\\) 추정범주 1 5 7 1 3.2524116 1 2 4 3 2 -1.4067524 2 3 7 8 2 1.7781350 1 4 8 6 2 -2.8135048 2 5 3 6 1 4.7266881 1 6 2 5 1 4.6929260 1 7 6 6 1 0.2025723 2 8 9 6 2 -4.3215434 2 9 5 4 2 -1.3729904 2 위 결과 객체 3, 7가 오분류된다. 1.3.4 R 패키지를 이용한 분류규칙 도출 패키지 MASS내의 함수 lda 수행 시 얻어지는 판별계수 \\(\\hat{\\mathbf{w}}\\)는 위 결과와는 사뭇 다른데, lda 함수의 경우 아래와 같이 1) 제약식을 포함하여 비례계수를 구하기 때문에 계수의 크기가 달라지며, 2) 목적함수를 최소화하는 대신 최대화하는 값을 찾기 때문에 부호가 달라진다. \\[\\begin{equation*} \\begin{split} \\max \\text{ } &amp; \\mathbf{w}^\\top ( \\boldsymbol\\mu_1 - \\boldsymbol\\mu_2 )\\\\ \\text{s.t. } &amp; \\mathbf{w}^\\top \\boldsymbol\\Sigma \\mathbf{w} = 1 \\end{split} \\end{equation*}\\] 이에 따른 lda 함수의 계수 추정 결과는 아래와 같다. w_hat_lda &lt;- fisher_da$scaling print(w_hat_lda) ## LD1 ## x1 0.6850490 ## x2 -0.7003859 z_mean_lda &lt;- t(fisher_da$scaling) %*% (train_df %&gt;% select(x1, x2) %&gt;% colMeans()) %&gt;% drop() print(z_mean_lda) ## LD1 ## -0.2391423 위 결과는 아래와 같은 계산을 통해 앞 장에서 보았던 결과와 동일한 분류 경계식으로 표현될 수 있음을 볼 수 있다. scale_adjust &lt;- t(w_hat) %*% Sigma_hat %*% w_hat %&gt;% drop() %&gt;% sqrt() sign_adjust &lt;- -1 w_hat &lt;- w_hat_lda * scale_adjust * sign_adjust print(w_hat) ## LD1 ## x1 -1.508039 ## x2 1.541801 z_mean &lt;- z_mean_lda * scale_adjust * sign_adjust print(z_mean) ## LD1 ## 0.526438 아래 스크립트는 위 lda 함수로부터의 경계식 추정을 기반으로 아래 수식값을 계산한다. \\[\\begin{equation*} \\hat{\\mathbf{w}}^\\top \\mathbf{x} - \\bar{z} \\end{equation*}\\] predict(fisher_da, train_df)$x ## LD1 ## 1 -1.2383140 ## 2 0.8781805 ## 3 -0.5686020 ## 4 1.5172187 ## 5 -1.9080261 ## 6 -1.8926892 ## 7 0.1471208 ## 8 2.2022677 ## 9 0.8628436 피셔 분류규칙에 따라 해당 값이 0보다 작으면 범주 1, 0보다 크면 범주 2로 분류한다. train_df %&gt;% mutate( centered_z = predict(fisher_da, .)$x, predicted_class = factor(if_else(centered_z &lt;= 0, 1, 2), levels = c(1, 2)) ) %&gt;% knitr::kable(booktabs = TRUE, align = c(&#39;r&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;), col.names = c(&#39;객체번호&#39;, &#39;$x_1$&#39;, &#39;$x_2$&#39;, &#39;실제범주&#39;, &#39;$z - \\\\bar{z}$&#39;, &#39;추정범주&#39;), caption = &#39;학습표본에 대한 피셔 분류 결과 - `MASS::lda` 분류 경계식 기준&#39;) Table 1.3: 학습표본에 대한 피셔 분류 결과 - MASS::lda 분류 경계식 기준 객체번호 \\(x_1\\) \\(x_2\\) 실제범주 \\(z - \\bar{z}\\) 추정범주 1 5 7 1 -1.2383140 1 2 4 3 2 0.8781805 2 3 7 8 2 -0.5686020 1 4 8 6 2 1.5172187 2 5 3 6 1 -1.9080261 1 6 2 5 1 -1.8926892 1 7 6 6 1 0.1471208 2 8 9 6 2 2.2022677 2 9 5 4 2 0.8628436 2 Table 1.3는 Table 1.2와 동일한 범주 추정 결과를 보인다. 1.4 의사결정론에 의한 분류규칙 1.5 오분류비용을 고려한 분류규칙 1.6 이차판별분석 1.7 세 범주 이상의 분류 References "],
["tree-based-method.html", "Chapter 2 트리기반 기법 2.1 CART 개요 2.2 필요 R package 설치 2.3 CART 트리 생성 2.4 가지치기 및 최종 트리 선정 2.5 R패키지 내 분류 트리 방법", " Chapter 2 트리기반 기법 2.1 CART 개요 CART(Classification and Regression Trees)는 Breiman et al. (1984) 에 의하여 개발된 것인데, 각 (독립)변수를 이분화(binary split)하는 과정을 반복하여 트리 형태를 형성함으로써 분류(종속변수가 범주형일 때) 또는 회귀분석(종속변수가 연속형일 때)을 수행하는 것이다. 이 때 독립변수들은 범주형 또는 연속형 모두에 적용될 수 있다. 본 장에서는 분류를 위한 목적만을 설명하도록 한다. 2.2 필요 R package 설치 본 장에서 필요한 R 패키지들은 아래와 같다. package version tidyverse 1.2.1 rpart 4.1-13 rpart.plot 3.0.1 2.3 CART 트리 생성 2.3.1 기본 R 스크립트 train_df &lt;- tibble( x1 = c(1,2,2,2,2,3,4,4,4,5), x2 = c(4,6,5,4,3,6,6,5,4,3), class = as.factor(c(1,1,1,2,2,1,1,2,2,2)) ) Table 2.1: 학습표본 데이터 x1 x2 class 1 4 1 2 6 1 2 5 1 2 4 2 2 3 2 3 6 1 4 6 1 4 5 2 4 4 2 5 3 2 Table 2.1와 같이 두 독립변수 x1, x2와 이분형 종속변수 class의 관측값으로 이루어진 10개의 학습표본을 train_df라는 data frame에 저장한다. library(rpart) library(rpart.plot) cart.est &lt;- rpart( class ~ x1 + x2 , data = train_df , method = &quot;class&quot; , parms = list(split = &quot;gini&quot;) , control = rpart.control(minsplit = 2 , minbucket = 1 , cp = 0 , xval = 0 , maxcompete = 0) ) rpart.plot(cart.est) Figure 2.1: CART 트리 rpart 라는 package를 기반으로, 두 변수 x1과 x2를 이용하여 이분형 종속변수 class를 분류하는 CART 트리를 생성할 수 있으며, rpart.plot package를 이용하여 Figure 2.1과 같이 시각화할 수 있다. 2.3.2 기호 정의 본 장에서 사용될 수학적 기호는 아래와 같다. \\(T\\): 트리 \\(A(T)\\): 트리 \\(T\\)의 최종노드의 집합 \\(J\\): 범주수 \\(N\\): 학습표본의 총 객체수 \\(N_j\\): 범주 \\(j\\)에 속한 객체 수 \\(N(t)\\): 노드 \\(t\\)에서의 객체수 \\(N_j(t)\\): 노드 \\(t\\)에서 범주 \\(j\\)에 속한 객체수 \\(p(j,t)\\): 임의의 객체가 범주 \\(j\\)와 노드 \\(t\\)에 속할 확률 \\(p(t)\\): 임의의 객체가 노드 \\(t\\)에 속할 확률 \\[p(t) = \\sum_{j=1}^{J} p(j,t)\\] \\(p(j|t)\\): 임의의 객체가 노드 \\(t\\)에 속할 때 범주 \\(j\\)에 속할 조건부 확률 \\[p(j|t) = \\frac{p(j,t)}{p(t)}, \\quad \\sum_{j=1}^{J} p(j|t) = 1\\] 이 때, 각 확률은 학습표본에서 아래와 같이 추정할 수 있다. \\[\\begin{align} p(j,t) &amp;\\approx \\frac{N_j(t)}{N}\\\\ p(t) &amp;\\approx \\frac{N(t)}{N}\\\\ p(j|t) &amp;\\approx \\frac{N_j(t)}{N(t)} \\end{align}\\] 2.3.3 노드 및 트리의 불순도 2.3.3.1 노드의 불순도 CART는 지니 지수(Gini index)를 불순도 함수로 사용한다. 총 \\(J\\)개의 범주별 객체비율을 \\(p_1, \\cdots , p_J\\)라 할 때 (\\(\\sum_{j=1}^{J} p_j = 1\\)), 지니 지수는 식 (2.1)와 같다. \\[\\begin{equation} G(p_1, \\cdots, p_J) = \\sum_{j=1}^{J} p_j(1-p_j) = 1 - \\sum_{j=1}^{J}p_j^2 \\tag{2.1} \\end{equation}\\] 노드 \\(t\\)에서의 범주별 객체비율은 \\(p(1|t), \\cdots, p(J|t)\\)이므로, 노드 \\(t\\)의 불순도는 식 (2.2)와 같이 산출된다. \\[\\begin{equation} \\begin{split} i(t) &amp;= 1 - \\sum_{j=1}^{J} p(j|t)^2\\\\ &amp;\\approx 1 - \\sum_{j=1}^{J} \\left[\\frac{N_j(t)}{N(t)}\\right]^2 \\end{split} \\tag{2.2} \\end{equation}\\] 2.3.3.2 트리 불순도 트리 \\(T\\)의 불순도는 식 (2.3)와 같이 최종노드들의 불순도의 가중평균으로 정의된다. \\[\\begin{equation} I(T) = \\sum_{t \\in A(T)} i(t)p(t) \\tag{2.3} \\end{equation}\\] 여기서 \\[ I(t) = i(t)p(t) \\] 라 하면, 다음이 성립한다. \\[ I(T) = \\sum_{t \\in A(T)} I(t) \\] 2.3.4 분지기준 뿌리 노드에서의 분지만을 살펴보기 위해 control parameter maxdepth의 값을 1으로 설정한다. 이 경우, CART 알고리즘은 뿌리노드에서의 양 갈래 분지만을 선택한 뒤 종료된다. 아래 스크립트를 이용하여 뿌리노드에서 최적분지된 트리를 얻는다. cart.firstsplit &lt;- rpart(class ~ x1 + x2 , data = train_df , method = &quot;class&quot; , parms = list(split = &quot;gini&quot;) , control = rpart.control(minsplit = 2 , minbucket = 1 , maxdepth = 1 , cp = 0 , xval = 0 , maxcompete = 0 ) ) rpart.plot(cart.firstsplit) Figure 2.2: 뿌리노드 분지 또한 분지 결과 트리는 Table 2.2와 같이 frame이라는 이름의 data frame에 설명된다. 각 행 앞의 번호는 노드 인덱스 \\(t\\)를 나타내며, 각 열에 대한 설명은 아래와 같다. var: 노드 \\(t\\)를 분지하는 데 이용된 변수. 값이 &lt;leaf&gt;인 경우에는 노드 \\(t\\)가 최종 노드임을 나타낸다. n: 노드 내 객체 수 \\(N(t)\\) wt: 가중치 적용 후 객체 수 (추후 appendix에서 설명) dev: 오분류 객체 수 yval: 노드 \\(t\\)를 대표하는 범주 complexity: 노드 \\(t\\)에서 추가로 분지할 때 감소하는 relative error값; 본 분류트리 예제에서 error는 오분류율이며, 뿌리 노드의 relative error값을 1으로 한다. Table 2.2: 뿌리노드 분지 상세 (frame) var n wt dev yval complexity ncompete nsurrogate 1 x2 10 10 5 1 0.6 0 0 2 &lt;leaf&gt; 3 3 0 1 0.0 0 0 3 &lt;leaf&gt; 7 7 2 2 0.0 0 0 또한 frame에는 트리 내 각 노드에 속한 객체와 범주에 대한 정보를 나타내는 yval2라는 행렬이 Table 2.3와 같이 존재한다. 실제 yval2의 열의 개수는 전체 학습 대상 범주 수에 따라 달라지며, 본 예는 이분 분류 트리(범주개수 = 2)에 해당하는 열 구성을 보여준다. 각 행 앞의 번호는 노드 인덱스 \\(t\\)를 나타내며, 각 열에 대한 설명은 아래와 같다. 열1: 노드 \\(t\\)에서의 최적 추정 범주 \\(j^*\\) 열2: 노드 \\(t\\) 내 범주 class=1 객체 수 \\(N_1(t)\\) 열3: 노드 \\(t\\) 내 범주 class=2 객체 수 \\(N_2(t)\\) 열4: 노드 \\(t\\) 내 범주 class=1 관측 확률 \\(p(1|t) \\approx \\tfrac{N_1(t)}{N(t)}\\) 열5: 노드 \\(t\\) 내 범주 class=2 관측 확률 \\(p(2|t) \\approx \\tfrac{N_2(t)}{N(t)}\\) nodeprob: 노드 \\(t\\) 확률 \\(p(t) \\approx \\tfrac{N(t)}{N}\\) ## Warning: Setting row names on a tibble is deprecated. Table 2.3: 노드 내 객체 및 범주 정보 (yval2) 열1 열2 열3 열4 열5 nodeprob 1 1 5 5 0.50 0.50 1.0 2 1 3 0 1.00 0.00 0.3 3 2 2 5 0.29 0.71 0.7 위 CART 모델 데이터를 이용하여 트리의 불순도를 계산해보자. 우선 노드 상세 정보 행렬 yval2의 x번째 노드의 불순도(\\(i(t)\\))를 계산하는 함수 rpartNodeImpurity를 아래와 같이 구현한다. rpartNodeImpurity &lt;- function(x, yval2) { node_vec &lt;- yval2[x, ] n.columns &lt;- length(node_vec) class.prob &lt;- node_vec[((n.columns/2)+1):(n.columns-1)] return(1 - sum(class.prob^2)) } CART tree 객체의 각 leaf node에 함수 rpartNodeImpurity를 적용하여 노드 불순도 \\(i(t)\\)를 계산한 뒤, 노드 확률 \\(p(t)\\)을 이용한 가중합을 통해 트리 불순도 \\(I(T)\\)를 계산하는 함수 rpartImpurity를 아래와 같이 구현한다. rpartImpurity &lt;- function(rpart.obj) { leaf.nodes &lt;- which(rpart.obj$frame$var==&quot;&lt;leaf&gt;&quot;) node.impurity &lt;- sapply(leaf.nodes, rpartNodeImpurity, yval2 = rpart.obj$frame$yval2) node.prob &lt;- rpart.obj$frame$yval2[leaf.nodes, &#39;nodeprob&#39;] return(sum(node.prob * node.impurity)) } 위 함수를 이용하여 계산한 트리 Figure 2.1의 불순도는 0.29이다. rpartImpurity(cart.firstsplit) ## [1] 0.2857143 분지를 추가할수록 불순도는 감소한다. 분지를 추가하기 위해서는 maxdepth라는 control parameter 값을 증가시키면 된다. maxdepth: 뿌리노드부터 임의의 최종노드에 도달하는 최대 가능 분지 수 (default=30) maxdepth 파라미터의 값을 1부터 4까지 증가시키며 불순도의 변화를 살펴보자. library(ggplot2) tree.impurity &lt;- sapply(c(1:4), function(depth) { rpart(class ~ x1 + x2 , data = train_df , method = &quot;class&quot; , parms = list(split = &quot;gini&quot;) , control = rpart.control(minsplit = 2 , minbucket = 1 , maxdepth = depth , cp = 0 , xval = 0 , maxcompete = 0)) %&gt;% rpartImpurity() }) tibble(maxdepth=c(1:4), impurity=tree.impurity) %&gt;% ggplot(aes(x=maxdepth, y=impurity)) + geom_line() Figure 2.3: 파라미터 maxdepth값에 따른 트리불순도 변화 위 예에서, 트리의 분지가 증가함에 따라 불순도는 0.29, 0.17, 0.17, 0로 감소한다. maxdepth값이 3일 때 불순도가 감소하지 않는 이유는, 세 번째 분지 결과가 전체적인 오분류를 감소시키지 않아 rpart 함수가 해당 분지를 취소하기 때문이다. 여기에 작용하는 파라미터는 cp라는 control parameter이다. cp: 노드가 분지되기 위한 최소 relative error 감소치 (default = 0.01). 값이 0일 경우 최대트리를 생성한다. 위 예제에서는 cp값을 0으로 설정하여, 해당 분지가 트리 불순도를 감소시킨다 하더라도 전체 트리의 오분류를 감소시키는 데 기여하지 않는다면 시도하지 않도록 하였다. 2.4 가지치기 및 최종 트리 선정 2.4.1 가지치기 앞 장의 최대 트리 그림 2.1은 학습 데이터를 오분류 없이 완벽하게 분류하기 위해 복잡한 분류 구조를 형성하였다. 이러한 복잡한 분류 구조는 학습 데이터가 아닌 새로운 데이터에 대한 분류 정확도를 떨어뜨릴 수 있다. 이는 bias-variance tradeoff라 부르는 현상으로, 비단 분류트리 뿐 아니라 모든 데이터마이닝 방법에 일반적으로 적용된다. 분류 트리는 가지치기라는 방식을 통해, 분류 구조를 단순화함으로써 분류 트리가 새로운 데이터에도 정확한 분류를 제공하기를 추구한다. 가지치기란 트리 내 특정 내부노드를 기준으로 그 하위에 발생한 분지를 모두 제거하고, 해당 내부노드를 최종노드로 치환하는 방식이다. Table 2.4: 최대 트리 분지 상세 (frame) var n wt dev yval complexity ncompete nsurrogate 1 x2 10 10 5 1 0.6 0 0 2 &lt;leaf&gt; 3 3 0 1 0.0 0 0 3 x1 7 7 2 2 0.2 0 0 6 &lt;leaf&gt; 1 1 0 1 0.0 0 0 7 x2 6 6 1 2 0.1 0 0 14 x1 2 2 1 1 0.1 0 0 28 &lt;leaf&gt; 1 1 0 1 0.0 0 0 29 &lt;leaf&gt; 1 1 0 2 0.0 0 0 15 &lt;leaf&gt; 4 4 0 2 0.0 0 0 Table 2.4에서 생성 가능한 가지치기는 최종 노드(var값이 &lt;leaf&gt;)가 아닌 모든 노드(1, 3, 7, 14)에서 가능하며, 함수 snip.rpart를 이용하여 가지치기 된 트리를 생성할 수 있다. 각 내부 노드에서 가지치기된 트리들은 아래와 같이 얻어진다. internal.node.index &lt;- rownames(cart.est$frame)[which(cart.est$frame$var != &#39;&lt;leaf&gt;&#39;)] %&gt;% as.numeric() snipped &lt;- lapply(internal.node.index, function(x){snip.rpart(cart.est, x)}) n.trees &lt;- length(snipped) par(mfrow=c(2,2)) invisible(lapply(c(1:n.trees), function(x) { rpart.plot(snipped[[x]])} )) Figure 2.4: 각 내부노드 기준으로 가지치기된 트리 위 각 가지치기 후보 노드의 오분류 비용은 함수 nodeCost를 아래와 같이 구현하여 계산할 수 있다. nodeCost &lt;- function(node, tree) { node_vec &lt;- tree$frame$yval2[as.character(node) == row.names(tree$frame), ] n.columns &lt;- length(node_vec) class.prob.max &lt;- max(node_vec[((n.columns/2)+1):(n.columns-1)]) node.prob &lt;- node_vec[n.columns] node.misclassification.cost &lt;- (1-class.prob.max)*node.prob return(node.misclassification.cost) } tibble( pruning_node = internal.node.index, node_cost = sapply(internal.node.index, nodeCost, tree=cart.est) ) %&gt;% knitr::kable() pruning_node node_cost 1 0.5 3 0.2 7 0.1 14 0.1 각 가지치기 노드에 해당하는 하부 트리의 오분류비용 및 복잡도를 구하기 위해 subtreeEval라는 함수를 아래와 같이 구현한다. subtreeEval &lt;- function(node, tree) { snipped &lt;- snip.rpart(tree, node)$frame leaf.nodes &lt;- setdiff(rownames(tree$frame[tree$frame$var==&quot;&lt;leaf&gt;&quot;,]), rownames(snipped)) %&gt;% as.numeric() tibble( pruning_node = node, node.cost = nodeCost(node, tree), subtree.cost = sapply(leaf.nodes, nodeCost, tree=tree) %&gt;% sum(), subtree.size = length(leaf.nodes) ) %&gt;% mutate(alpha = (node.cost - subtree.cost) / (subtree.size - 1)) } 각 노드에 대하여 알파값을 다음과 같이 계산할 수 있다. df.cost &lt;- lapply(internal.node.index, subtreeEval, tree=cart.est) %&gt;% bind_rows() Table 2.5: 내부노드 가지치기 평가 (df.cost) pruning_node node.cost subtree.cost subtree.size alpha 1 0.5 0 5 0.12 3 0.2 0 4 0.07 7 0.1 0 3 0.05 14 0.1 0 2 0.10 위 Table 2.5 에서 최소 알파값에 해당하는 노드 7에서 가지치기를 한다. pruned.tree.1 &lt;- snip.rpart(cart.est, df.cost$pruning_node[which.min(df.cost$alpha)]) rpart.plot(pruned.tree.1) Figure 2.5: 1단계 가지치기 결과 가지치기로 형성된 트리에서 다시 각 가지치기 노드의 오분류비용, 복잡도 및 알파값을 구한다. df.cost &lt;- rownames(pruned.tree.1$frame)[pruned.tree.1$frame$var!=&quot;&lt;leaf&gt;&quot;] %&gt;% as.numeric() %&gt;% lapply(subtreeEval, tree=pruned.tree.1) %&gt;% bind_rows() knitr::kable(df.cost) pruning_node node.cost subtree.cost subtree.size alpha 1 0.5 0.1 3 0.2 3 0.2 0.1 2 0.1 위 결과에서 다시 최소 알파값에 해당하는 노드 3에서 가지치기를 하면 아래와 같은 트리가 형성된다. pruned.tree.2 &lt;- snip.rpart(pruned.tree.1, df.cost$pruning_node[which.min(df.cost$alpha)]) rpart.plot(pruned.tree.2) Figure 2.6: 2단계 가지치기 결과 2.4.2 최적 트리의 선정 위 가지치기 과정에서 얻는 가지친 트리들이 최종 트리의 후보가 되며, 이 중 테스트 표본에 대한 오분류율이 가장 작은 트리를 최적 트리로 선정하게 된다. 트리를 학습할 때 사용된 학습데이터 Table 2.1 외에, Table 2.6과 같은 6개의 테스트 데이터가 있다고 하자. test_df &lt;- tibble( x1 = c(1,0,3,4,2,1), x2 = c(5,5,4,3,7,4), class = factor(c(1,1,2,2,1,2), levels=c(1,2)) ) Table 2.6: 테스트 데이터 x1 x2 class 1 5 1 0 5 1 3 4 2 4 3 2 2 7 1 1 4 2 테스트 데이터에 위에서 학습된 세 개의 트리, 즉 최대 트리 cart.est와 두 개의 가지치기 트리 pruned.tree.1 &amp; pruned.tree.2를 적용하여 각 트리가 각각의 테스트 데이터를 어떻게 분류하는지 살펴보자. test_pred &lt;- test_df %&gt;% bind_cols( pred_maxtree = predict(cart.est, test_df, type=&quot;class&quot;), pred_prune1 = predict(pruned.tree.1, test_df, type=&quot;class&quot;), pred_prune2 = predict(pruned.tree.2, test_df, type=&quot;class&quot;) ) Table 2.7: 테스트 데이터에 대한 예측 결과 x1 x2 class pred_maxtree pred_prune1 pred_prune2 1 5 1 1 1 2 0 5 1 1 1 2 3 4 2 2 2 2 4 3 2 2 2 2 2 7 1 1 1 1 1 4 2 1 1 2 결과 Table 2.7에서 최대트리가 오분류한 테스트 표본은 1개, 첫번째 가지치기 트리가 오분류한 테스트 표본은 1개, 그리고 두 번째 가지치기 트리가 오분류한 테스트 표본은 2개이다. 위 결과를 토대로, 최적의 트리를 선정하는 과정은 아래와 같다. 각각의 트리에 의해 오분류된 테스트 표본의 개수를 전체 테스트 표본의 개수로 나누어 오분류율 \\(R^{ts}\\)를 구한다. 테스트 표본 수를 \\(n_{test}\\)라 할 때, 오분류의 표준편차를 아래와 같이 계산한다. \\[SE = \\sqrt{\\frac{R^{ts}(1 - R^{ts})}{n_{test}}}\\] 1에서 구한 오분류율에 2에서 구한 표준편차를 더하여 \\(R^{ts} + SE\\)를 각 트리의 평가척도로 계산한다. 후보 트리들 중 해당 평가척도가 가장 작은 트리를 최종 트리로 선정한다. test.summary &lt;- test_pred %&gt;% summarize(n.test = n(), cart.est = sum(pred_maxtree != class) / n.test, pruned.tree.1 = sum(pred_prune1 != class) / n.test, pruned.tree.2 = sum(pred_prune2 != class) / n.test) %&gt;% gather(&quot;tree&quot;,&quot;R.ts&quot;,-n.test) %&gt;% mutate(SE = sqrt((R.ts*(1 - R.ts))/n.test), score = R.ts + SE) %&gt;% select(-n.test) Table 2.8: 분류 성능 트리 오분류율(\\(R^{ts}\\)) 표준편차(\\(SE\\)) 척도(\\(R^{ts} + SE\\)) cart.est 0.17 0.15 0.32 pruned.tree.1 0.17 0.15 0.32 pruned.tree.2 0.33 0.19 0.53 위 결과, 최적 트리는 최대 트리 혹은 첫 번째 가지치기 트리가 된다. 위 절차를 임의의 데이터에 대해 수행하는 함수를 구현해보자. rpart_learn &lt;- function(formula, train_df, test_df) { # 최대 트리 생성 max_tree &lt;- rpart(formula , data = train_df , method = &quot;class&quot; , parms = list(split = &quot;gini&quot;) , control = rpart.control(minsplit = 2 , minbucket = 1 , cp = 0 , xval = 0 , maxcompete = 0 ) ) # 가지치기 curr_tree &lt;- list() k &lt;- 1 curr_tree[[k]] &lt;- max_tree while(dim(curr_tree[[k]]$frame)[1] &gt; 1) { internal.node.index &lt;- rownames(curr_tree[[k]]$frame)[which(curr_tree[[k]]$frame$var != &#39;&lt;leaf&gt;&#39;)] %&gt;% as.numeric() df.cost &lt;- lapply(internal.node.index, subtreeEval, tree=curr_tree[[k]]) %&gt;% bind_rows() curr_tree[[k + 1]] &lt;- snip.rpart(curr_tree[[k]], df.cost$pruning_node[which.min(df.cost$alpha)]) k &lt;- k + 1 } # 최적 가지치기 트리 선정 n.test &lt;- dim(test_df)[1] R.ts &lt;- lapply(curr_tree, function(x) { sum(predict(x, test_df, type=&quot;class&quot;) != test_df$class) / n.test }) %&gt;% unlist() score &lt;- R.ts + sqrt((R.ts*(1 - R.ts))/n.test) return(curr_tree[[max(which(score == min(score)))]]) } optimal_tree &lt;- rpart_learn(class ~ x1 + x2, train_df, test_df) rpart.plot(optimal_tree) 2.5 R패키지 내 분류 트리 방법 앞 장에서는 rpart의 결과를 이용하여 교재 8.2 - 8.3장의 예제를 재현해보았다. 실제로 rpart 내부의 기본 트리 방법은 교재의 예제와는 다소 다른 부분이 있다. 이 장에서는 실제 rpart 패키지의 분류 트리 방법에 대해 알아본다. 2.5.1 트리 확장 트리 내 임의의 노드 \\(t\\)에 대한 불순도는 아래와 같이 정의된다. \\[i(t) = \\sum_{j=1}^{J} f\\left(p(j|t)\\right)\\] 여기에서 \\(p(j|t)\\)는 노드 \\(t\\) 내 전체 샘플 \\(N(t)\\) 중 범주 \\(j\\)의 샘플 \\(N_j(t)\\)의 비율로 추정된다. \\[p(j|t) \\approx \\frac{N_j(t)}{N(t)}\\] 또한 함수 \\(f\\)는 concave 함수로, \\(f(0) = f(1) = 0\\)의 조건을 만족시켜야 한다. rpart 에서 설정할 수 있는 함수 \\(f\\)의 종류에 대해서는 아래에서 좀 더 자세히 살펴보기로 한다. 트리 내 임의의 노드 \\(t\\)가 분지규칙 \\(s\\)에 따라 두 개의 노드 \\(t_L\\)과 \\(t_R\\)로 분지된다고 할 때, 불순도의 감소량은 아래와 같이 계산된다. \\[\\begin{eqnarray} \\Delta I(s,t) &amp;=&amp; I(t) - I(t_L) - I(t_R)\\\\ &amp;=&amp; p(t)i(t) - p(t_L)i(t_L) - p(t_R)i(t_R) \\end{eqnarray}\\] rpart는 위 \\(\\Delta I(s,t)\\)값이 최대가 되는 분지 기준 \\(s^*\\)를 찾아 노드 \\(t\\)를 분지하여 트리를 확장하고, 확장된 트리의 최종 노드에서 다시 최적 분지를 찾는 과정을 반복한다. 2.5.1.1 분지 함수 함수 rpart 사용 시 parms 파라미터에 split 값으로 분지 방법을 설정할 수 있다. Gini index (parms=list(split=‘gini’)) 교재의 예제에 사용된 방법으로, 우선 아래와 같은 함수 \\(f\\)를 사용한다. \\[f(p) = p(1-p)\\] information index (parms=list(split=‘information’)) 교재에 엔트로피 지수(Entropy index)로 설명된 지수로, 아래와 같은 함수를 사용한다. \\[f(p) = -p\\log(p)\\] user-defined function 사용자가 임의로 함수를 정의하여 사용할 수 있다. 본 장에서는 자세한 설명은 생략한다. 2.5.2 가지치기 임의의 노드 \\(t\\)에 대한 위험도(오분류 비용의 기대치)는 아래와 같이 계산된다. \\[r(t) = \\sum_{j \\neq \\tau(t)} p(j|t)C\\left(\\tau(t)|j\\right)\\] 여기에서 함수 \\(C(i|j)\\)는 범주 \\(j\\)에 속하는 객체를 범주 \\(i\\)로 분류할 때의 오분류 비용이며, \\(\\tau(t)\\)는 노드 \\(t\\) 내의 오분류 비용을 최소화하도록 노드 \\(t\\)에 지정된 범주값이다. rpart의 오분류 비용 \\(C(i|j)\\)의 기본값은 \\[C(i|j) = \\begin{cases} 1, &amp; \\text{ if } i \\neq j\\\\ 0, &amp; \\text{ if } i = j \\end{cases} \\] 으로 설정되어 있으며, parms 파라미터에 loss 값으로 오분류 비용 \\(C(i|j)\\)를 재설정할 수 있다. 본 장에서는 기본값을 사용하도록 하자. \\(A(T)\\)를 트리 \\(T\\)의 최종 노드의 집합이라 정의하고, 트리의 최종 노드의 개수를 \\(|T|\\)라 할 때, 트리 \\(T\\)의 위험도 \\(R(T)\\)는 아래와 같이 정의된다. \\[R(T) = \\sum_{t \\in A(T)} p(t)r(t)\\] 복잡도 계수(complexity parameter) \\(\\alpha \\in [0, \\infty)\\)를 이용하여, 트리의 비용-복합도 척도를 다음과 같이 정의한다. \\[R_\\alpha(T) = R(T) + \\alpha|T|\\] 이 때, 임의의 계수 \\(\\alpha\\)에 대해 비용 \\(R_\\alpha(T)\\)가 최소가 되게하는 가지치기 트리를 \\(T_\\alpha\\)라 하면, 아래와 같은 관계들이 성립한다. \\(T_0\\): 최대 트리 \\(T_\\infty\\): 뿌리 노드 트리 (분지 없음) \\(\\alpha &gt; \\beta\\)일 때, \\(T_\\alpha\\)는 \\(T_\\beta\\)와 동일하거나 혹은 \\(T_\\beta\\)에서 가지치기된 트리이다. 2.5.3 파라미터값 결정 함수 rpart를 사용할 때 여러가지 사용자 정의 파라미터값을 설정할 수 있으며, 그 파라미터 값에 따라 생성되는 트리의 결과가 달라진다. 대표적인 파라미터 값으로는 아래와 같은 것들이 있다. minsplit: 분지를 시도하기 위해 필요한 노드 내 최소 관측객체 수 (default=20) cp: 노드가 분지되기 위한 최소 relative error 감소치 (default = 0.01). 값이 0일 경우 최대트리를 생성한다. maxdepth: 뿌리노드부터 임의의 최종노드에 도달하는 최대 가능 분지 수 (default=30) References "],
["svm.html", "Chapter 3 서포트 벡터 머신 3.1 개요 3.2 필요 R package 설치 3.3 선형 SVM - 분리 가능 경우 3.4 선형 SVM - 분리 불가능 경우 3.5 비선형 SVM 3.6 R패키지 내 SVM", " Chapter 3 서포트 벡터 머신 3.1 개요 서포트 벡터 머신(suuport vector machine; 이하 SVM)은 기본적으로 두 범주를 갖는 객체들을 분류하는 방법이다. 물론 세 범주 이상의 경우로 확장이 가능하다. 3.2 필요 R package 설치 본 장에서 필요한 R 패키지들은 아래와 같다. package version tidyverse 1.2.1 e1071 1.6-8 Matrix 1.2-14 quadprog 1.5-5 3.3 선형 SVM - 분리 가능 경우 3.3.1 기본 R 스크립트 train_df &lt;- tibble( x1 = c(5, 4, 7, 8, 3, 2, 6, 9, 5), x2 = c(7, 3, 8, 6, 6, 5, 6, 6, 4), class = c(1, -1, 1, 1, -1, -1, 1, 1, -1) ) knitr::kable(train_df, booktabs = TRUE, align = c(&#39;r&#39;, &#39;r&#39;, &#39;r&#39;), caption = &#39;선형분리가능 학습표본 데이터&#39;) Table 3.1: 선형분리가능 학습표본 데이터 x1 x2 class 5 7 1 4 3 -1 7 8 1 8 6 1 3 6 -1 2 5 -1 6 6 1 9 6 1 5 4 -1 Table 3.1와 같이 두 독립변수 x1, x2와 이분형 종속변수 class의 관측값으로 이루어진 9개의 학습표본을 train_df라는 data frame에 저장한다. library(e1071) svm_model &lt;- svm(as.factor(class) ~ x1 + x2, data = train_df, kernel = &quot;linear&quot;, scale = FALSE) plot(svm_model, data = train_df, formula = x2 ~ x1, grid = 200) Figure 3.1: 선형 SVM 분리 하이퍼플레인 그림 3.1에서 각 객체의 기호는 서포트 벡터 여부(“X”이면 서포트 벡터), 각 객체의 색상은 범주값(검정 = -1, 빨강 = 1)을 나타내며, 분리 하이퍼플레인은 아래와 같다. \\[ 0.6666667 x_{1} + 0.6666667 x_{2} = 7 \\] 3.3.2 기호 정의 본 장에서 사용될 수학적 기호는 아래와 같다. \\(\\mathbf{x} \\in \\mathbb{R}^p\\): p차원 변수벡터 \\(y \\in \\{-1, 1\\}\\): 범주 \\(N\\): 객체 수 \\((\\mathbf{x}_i, y_i)\\): \\(i\\)번째 객체의 변수벡터와 범주값 3.3.3 최적 하이퍼플레인 선형 SVM은 주어진 객체들의 두 범주를 완벽하게 분리하는 하이퍼플레인 중 각 범주의 서포트 벡터들로부터의 거리가 최대가 되는 하이퍼플레인을 찾는 문제로 귀착된다. 우선 아래와 같이 하이퍼플레인을 정의한다. \\[\\begin{equation} \\mathbf{w}^\\top \\mathbf{x} + b = 0 \\tag{3.1} \\end{equation}\\] 여기서 \\(\\mathbf{w} \\in \\mathbb{R}^p\\)와 \\(b \\in \\mathbb{R}\\)이 하이퍼플레인의 계수이다. 범주값이 1인 객체들 중 하이퍼플레인에서 가장 가까운 객체에 대해 다음과 같은 조건이 만족한다고 가정하자. \\[ H_1: \\mathbf{w}^\\top \\mathbf{x} + b = 1 \\] 또한 범주값이 -1인 객체들 중 하이퍼플레인에서 가장 가까운 객체에 대해 다음과 같은 조건이 만족한다고 가정하자. \\[ H_2: \\mathbf{w}^\\top \\mathbf{x} + b = -1 \\] 이 때 두 하이퍼플레인 \\(H_1\\)과 \\(H_2\\) 간의 거리(margin)는 \\(2 / \\lVert \\mathbf{w} \\rVert\\)이다. 선형 SVM은 아래와 같이 \\(H_1\\)과 \\(H_2\\) 간의 거리를 최대로 하는 최적화 문제가 된다. \\[\\begin{equation*} \\begin{split} \\max \\text{ } &amp; \\frac{2}{\\mathbf{w}^\\top \\mathbf{w}}\\\\ \\text{s.t.}&amp; \\\\ &amp; \\mathbf{w}^\\top \\mathbf{x}_i + b \\ge 1 \\text{ for } y_i = 1\\\\ &amp; \\mathbf{w}^\\top \\mathbf{x}_i + b \\le -1 \\text{ for } y_i = -1 \\end{split} \\end{equation*}\\] 이를 간략히 정리하면 \\[\\begin{equation*} \\begin{split} \\min \\text{ } &amp; \\frac{\\mathbf{w}^\\top \\mathbf{w}}{2}\\\\ \\text{s.t.}&amp; \\\\ &amp; y_i \\left( \\mathbf{w}^\\top \\mathbf{x}_i + b \\right) \\ge 1 \\end{split} \\end{equation*}\\] 과 같이 정리할 수 있으며, 각 객체 \\(i\\)에 대한 제약조건에 라그랑지 계수(Lagrange multiplier) \\(\\alpha_i \\ge 0\\)를 도입하여 라그랑지 함수를 유도하면 식 (3.2)과 같은 최적화 문제가 된다. 이를 원문제(primal problem)라 하자. \\[\\begin{equation} \\begin{split} \\min \\text{ } &amp; L_P = \\frac{1}{2} \\mathbf{w}^\\top \\mathbf{w} + \\sum_{i = 1}^{N} \\alpha_i \\left[ y_i \\left( \\mathbf{w}^\\top \\mathbf{x}_i + b \\right) - 1 \\right]\\\\ \\text{s.t. } &amp; \\alpha_i \\ge 0, \\text{ } i = 1, \\cdots, N \\end{split} \\tag{3.2} \\end{equation}\\] 원문제 식 (3.2)에 대한 울프쌍대문제(Wolfe dual problem)는 아래 식 (3.3)과 같이 도출된다. 보다 자세한 내용은 교재(전치혁 2012) 참고. \\[\\begin{equation} \\begin{split} \\max \\text{ } &amp; L_D = \\sum_{i = 1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^\\top \\mathbf{x}_j\\\\ \\text{s.t. } &amp;\\\\ &amp; \\sum_{i = 1}^{N} \\alpha_i y_i = 0\\\\ &amp; \\alpha_i \\ge 0, \\text{ } i = 1, \\cdots, N \\end{split} \\tag{3.3} \\end{equation}\\] 식 (3.3)은 이차계획(quadratic programming) 문제로, 각종 소프트웨어와 알고리즘을 이용하여 구할 수 있다. 본 장에서는 quadprog 패키지를 이용하여 해를 구하기로 한다. 이는 실제로 e1071의 svm 함수 호출 시 사용하는 방법은 아니며, 실제 svm 함수가 호출하는 알고리즘은 다음 장에서 다시 설명하기로 한다. quadprog의 solve.QP 함수는 아래와 같은 형태로 formulation된 문제(Goldfarb and Idnani 1983)에 대한 최적해를 구한다. \\[\\begin{equation} \\begin{split} \\min \\text{ } &amp; -\\mathbf{d}^{\\top}\\boldsymbol{\\alpha} + \\frac{1}{2} \\boldsymbol{\\alpha}^{\\top}\\mathbf{D}\\boldsymbol{\\alpha}\\\\ \\text{s.t. } &amp; \\mathbf{A}^{\\top}\\boldsymbol{\\alpha} \\ge \\mathbf{b}_0 \\end{split} \\tag{3.4} \\end{equation}\\] 식 (3.4)과 식 (3.3)이 동일한 문제를 나타내도록 아래와 같이 목적함수에 필요한 벡터 및 행렬을 정의한다. \\[\\begin{eqnarray*} \\mathbf{d} &amp;=&amp; \\mathbf{1}_{N \\times 1}\\\\ \\mathbf{D} &amp;=&amp; \\mathbf{y}\\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{X}^{\\top} \\end{eqnarray*}\\] where \\[\\begin{eqnarray*} \\mathbf{y} &amp;=&amp; \\left[ \\begin{array}{c c c c} y_1 &amp; y_2 &amp; \\cdots &amp; y_N \\end{array} \\right]^\\top\\\\ \\mathbf{X} &amp;=&amp; \\left[ \\begin{array}{c c c c} \\mathbf{x}_1 &amp; \\mathbf{x}_2 &amp; \\cdots &amp; \\mathbf{x}_N \\end{array} \\right]^{\\top} \\end{eqnarray*}\\] N &lt;- dim(train_df)[1] X &lt;- train_df[c(&#39;x1&#39;, &#39;x2&#39;)] %&gt;% as.matrix() y &lt;- train_df[[&#39;class&#39;]] %&gt;% as.numeric() d &lt;- rep(1, N) D &lt;- (y %*% t(y)) * (X %*% t(X)) 여기에서 행렬 \\(\\mathbf{D}\\)의 determinant 값은 0으로, Goldfarb and Idnani (1983) 가 가정하는 symmetric positive definite matrix 조건에 위배되어 solve.QP 함수 실행 시 오류가 발생한다. 이를 방지하기 위해 아래 예에서는 Matrix 패키지의 nearPD함수를 이용하여 행렬 \\(\\mathbf{D}\\)와 근사한 symmetric positive definite matrix를 아래와 같이 찾는다. D_pd &lt;- Matrix::nearPD(D, doSym = T)$mat %&gt;% as.matrix() 식 (3.4)의 제약식은 모두 inequality 형태로, 식 (3.3)의 equality constraint \\(\\sum_{i = 1}^{N} \\alpha_i y_i = 0\\)를 표현하기 위해서 두 개의 제약식 \\(\\sum_{i = 1}^{N} \\alpha_i y_i \\ge 0\\)와 \\(\\sum_{i = 1}^{N} - \\alpha_i y_i \\ge 0\\)를 생성한다. \\[\\begin{equation*} \\mathbf{A}^\\top = \\left[ \\begin{array}{c c c c} y_1 &amp; y_2 &amp; \\cdots &amp; y_N\\\\ -y_1 &amp; -y_2 &amp; \\cdots &amp; -y_N\\\\ 1 &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; 1 &amp; \\cdots &amp; 0\\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{array} \\right], \\mathbf{b}_0 = \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\cdots \\\\ 0 \\end{array} \\right] \\end{equation*}\\] A &lt;- cbind( y, -y, diag(N) ) b_zero &lt;- rep(0, 2 + N) 이제 위에서 구한 행렬과 벡터들을 solve.QP 함수에 입력하여 최적해를 구한다. res &lt;- quadprog::solve.QP(D_pd, d, A, b_zero) alpha_sol &lt;- res$solution obj_val &lt;- -res$value Table 3.2: 이차계획문제의 최적해 variable solution alpha_1 0.2234 alpha_2 0.0000 alpha_3 0.0000 alpha_4 0.0000 alpha_5 0.2228 alpha_6 0.0000 alpha_7 0.2210 alpha_8 0.0000 alpha_9 0.2216 표 3.2의 결과는 교재(전치혁 2012)에 나타난 최적해와는 다소 차이가 있으나, 결과적으로 목적함수값은 0.4444로 동일하다. 위의 과정으로 최적해 \\(\\alpha_{i}^{*}\\)를 구한 뒤, 아래와 같이 분리 하이퍼플레인의 계수를 결정할 수 있다. \\[\\begin{eqnarray*} \\mathbf{w} &amp;=&amp; \\sum_{i = 1}^{N} \\alpha_{i}^{*} y_{i} \\mathbf{x}_{i}\\\\ b &amp;=&amp; \\sum_{i: \\alpha_{i}^{*} &gt; 0} \\frac{1 - y_{i} \\mathbf{w}^{\\top} \\mathbf{x}_{i}}{y_{i}} \\left/ \\sum_{i: \\alpha_{i}^{*} &gt; 0} 1 \\right. \\end{eqnarray*}\\] w &lt;- colSums(alpha_sol * y * X) print(w) ## x1 x2 ## 0.6666658 0.6666657 sv_ind &lt;- which(round(alpha_sol, digits = 4) &gt; 0) b &lt;- mean((1 - y[sv_ind] * (X[sv_ind, ] %*% w)) / y[sv_ind]) print(b) ## [1] -6.99999 위 결과와 같이, 분리 하이퍼플레인은 교재와 동일하게 얻어진다. 3.4 선형 SVM - 분리 불가능 경우 본 장에서는 학습표본 내의 두 범주가 어떠한 선형 하이퍼플레인으로도 완전하게 분리되지 않아 식 (3.2)이 해를 갖지 못하는 경우에 대한 문제를 다룬다. 3.4.1 기본 R 스크립트 앞 장에서 사용한 학습표본에 아래와 같이 하나의 객체를 추가하여 전체 학습표본이 선형 하이퍼플레인으로 분리될 수 없도록 하자. inseparable_train_df &lt;- bind_rows(train_df, tibble(x1 = 7, x2 = 6, class = -1)) knitr::kable(inseparable_train_df, booktabs = TRUE, align = c(&#39;r&#39;, &#39;r&#39;, &#39;r&#39;), caption = &#39;선형분리불가능 학습표본 데이터&#39;) Table 3.3: 선형분리불가능 학습표본 데이터 x1 x2 class 5 7 1 4 3 -1 7 8 1 8 6 1 3 6 -1 2 5 -1 6 6 1 9 6 1 5 4 -1 7 6 -1 library(e1071) svm_model &lt;- svm(as.factor(class) ~ x1 + x2, data = inseparable_train_df, kernel = &quot;linear&quot;, cost = 1, scale = FALSE) plot(svm_model, data = inseparable_train_df, formula = x2 ~ x1, grid = 200) Figure 3.2: 선형 SVM 분리 불가능 경우의 하이퍼플레인 Figure 3.2에서 보이듯, 하나의 검정 객체(범주 = -1)가 범주 1로 분류되는 영역에 존재하여 오분류가 발생한다. 이처럼 선형 하이퍼플레인으로 두 범주 학습표본의 분리가 불가능한 경우, 오분류 학습표본에 대한 페널티를 적용하여 최적 분리 하이퍼플레인을 도출하게 된다. 위 예에서의 최적 하이퍼플레인은 아래와 같다. \\[ 0.6 x_{1} + 0.8 x_{2} = 7.6 \\] 3.4.2 최적 하이퍼플레인 여유변수(slack variable) \\(\\xi_i\\) 를 각 학습객체 \\(i = 1, \\cdots, N\\)에 대해 아래와 같이 정의한다. \\[\\begin{equation*} \\xi_i = \\max \\left\\{ 0, 1 - y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\right\\} \\end{equation*}\\] 이는 객체가 자신의 범주의 서포트 벡터를 지나는 하이퍼플레인(범주 1인 경우 \\(H_1\\), 범주 -1인 경우 \\(H_2\\))으로 부터 다른 범주 방향으로 떨어진 거리를 나타낸다. 이 여유변수 \\(\\xi_i\\)에 단위당 페널티 단가 \\(C\\)를 부여하여 아래와 같은 최적화 문제를 정의한다. \\[\\begin{equation*} \\begin{split} \\min \\text{ } &amp; \\frac{\\mathbf{w}^\\top \\mathbf{w}}{2} + C \\sum_{i = 1}^{N} \\xi_i \\\\ \\text{s.t.}&amp; \\\\ &amp; y_i \\left( \\mathbf{w}^\\top \\mathbf{x}_i + b \\right) \\ge 1 - \\xi_i, \\text{ } i = 1, \\cdots, N \\\\ &amp; \\xi \\ge 0, \\text{ } i = 1, \\cdots, N \\end{split} \\end{equation*}\\] 이에 대한 울프쌍대문제를 앞 3.3.3장과 같은 과정으로 도출하면 아래 식 (3.5)와 같다. \\[\\begin{equation} \\begin{split} \\max \\text{ } &amp; L_D = \\sum_{i = 1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^\\top \\mathbf{x}_j\\\\ \\text{s.t. } &amp;\\\\ &amp; \\sum_{i = 1}^{N} \\alpha_i y_i = 0\\\\ &amp; 0 \\le \\alpha_i \\le C, \\text{ } i = 1, \\cdots, N \\end{split} \\tag{3.5} \\end{equation}\\] 이는 분리 가능 경우의 식 (3.3)에 변수 \\(\\alpha_i\\)에 대한 상한값 \\(C\\)의 제약이 추가된 문제로, 이는 e1071 패키지의 svm 함수가 기본 방법으로 사용하는 LIBSVM 라이브러리(Chang and Lin 2011)의 \\(C\\)-support vector classification(\\(C\\)-SVC)이 사용하는 문제식이며, LIBSVM 라이브러리는 특정 알고리즘(Fan, Chen, and Lin 2005)을 이용하여 해를 제공한다. 아래 svm 함수의 입력 변수에서 type = &quot;C-classification&quot;은 식 (3.3)를 최적화하겠다는 것을 나타내며, cost = 1은 페널티 단가 \\(C\\)의 값을 1로 설정하겠다는 것을 나타낸다. svm_model &lt;- svm(as.factor(class) ~ x1 + x2, data = inseparable_train_df, kernel = &quot;linear&quot;, scale = FALSE, type = &quot;C-classification&quot;, cost = 1) 위 결과 모델 객체 svm_model의 원소 중 index는 학습표본 중 서포트 벡터에 해당하는 인덱스 \\(i\\)를 나타내며, coefs는 각 서포트 벡터의 \\(\\alpha_i y_i\\) 값을 나타낸다. 따라서, coefs를 각 서포트 벡터의 범주값 \\(y_i\\)로 나누면 식 (3.5)의 최적해를 아래와 같이 볼 수 있다. N &lt;- dim(inseparable_train_df)[1] X &lt;- inseparable_train_df[c(&#39;x1&#39;, &#39;x2&#39;)] %&gt;% as.matrix() y &lt;- inseparable_train_df[[&#39;class&#39;]] %&gt;% as.numeric() sv_ind &lt;- svm_model$index alpha_sol &lt;- vector(&quot;numeric&quot;, N) alpha_sol[sv_ind] &lt;- drop(svm_model$coefs[, 1]) / y[sv_ind] Table 3.4: 이차계획문제의 최적해: 선형 분리 불가능 경우 variable solution alpha_1 0.8 alpha_2 0.0 alpha_3 0.0 alpha_4 0.0 alpha_5 0.8 alpha_6 0.0 alpha_7 1.0 alpha_8 0.0 alpha_9 0.0 alpha_10 1.0 하이퍼플레인의 계수 \\(\\mathbf{w}\\)는 분리 가능의 경우와 동일하게 구할 수 있다. \\[\\begin{equation*} \\mathbf{w} = \\sum_{i = 1}^{N} \\alpha_{i}^{*} y_{i} \\mathbf{x}_{i} \\end{equation*}\\] w &lt;- colSums(alpha_sol * y * X) print(w) ## x1 x2 ## 0.6 0.8 상수 \\(b\\)는 아래와 같이 \\(0 &lt; \\alpha_{i}^{*} &lt; C\\)인 객체들을 이용해 산출할 수 있다. \\[\\begin{equation*} b = \\sum_{i: 0 &lt; \\alpha_{i}^{*} &lt; C} \\frac{1 - y_{i} \\mathbf{w}^{\\top} \\mathbf{x}_{i}}{y_{i}} \\left/ \\sum_{i: 0 &lt; \\alpha_{i}^{*} &lt; C} 1 \\right. \\end{equation*}\\] ind &lt;- sv_ind[alpha_sol[sv_ind] &lt; svm_model$cost] b &lt;- mean((1 - y[ind] * (X[ind, ] %*% w)) / y[ind]) print(b) ## [1] -7.6 위와 같은 하이퍼플레인의 계수 \\(\\mathbf{w}\\)와 상수 \\(b\\)값은 svm 객체에 원소들을 이용하여 보다 쉽게 확인할 수 있다. w &lt;- t(svm_model$coefs) %*% svm_model$SV print(w) ## x1 x2 ## [1,] 0.6 0.8 b &lt;- -svm_model$rho print(b) ## [1] -7.6 선형 하이퍼플레인으로 분리 불가능한 경우, 페널티 단가 \\(C\\)의 값에 따라 도출되는 분리 하이퍼플레인이 달라진다. \\(C\\)의 값이 1, 5, 100일 때의 하이퍼플레인을 비교해보자. svm_models &lt;- lapply(c(1, 5, 100), function(C) svm(as.factor(class) ~ x1 + x2, data = inseparable_train_df, kernel = &quot;linear&quot;, scale = FALSE, type = &quot;C-classification&quot;, cost = C)) getHyperplane &lt;- function(model) { list(C = model$cost, w = paste(round(t(model$coefs) %*% model$SV, digits = 2), collapse = &quot;, &quot;), b = -round(model$rho, digits = 2), sv = paste(model$index, collapse = &quot;, &quot;), misclassified = paste(which(model$fitted != as.factor(inseparable_train_df$class)), collapse = &quot;, &quot;)) } svm_summary &lt;- lapply(svm_models, getHyperplane) %&gt;% bind_rows() Table 3.5: 페널티 단가 C에 따른 하이퍼플레인 계수 및 결과 페널티 단가 \\(C\\) \\((w_1, w_2)\\) \\(b\\) 서포트 벡터 객체 오분류 객체 1 0.6, 0.8 -7.6 1, 7, 5, 10 10 5 0.4, 1.2 -9.4 1, 4, 7, 5, 10 10 100 0.4, 1.2 -9.4 1, 4, 7, 5, 10 10 Table 3.5에서 보이는 바와 같이, 페널티 단가 \\(C\\)의 값이 1과 5일 때 분리 하이퍼플레인이 변하는 것을 볼 수 있다. \\(C\\)값이 5와 100일 때의 분리 하이퍼플레인은 거의 동일하다. plot(svm_models[[2]], data = inseparable_train_df, formula = x2 ~ x1, grid = 200) Figure 3.3: 선형 SVM 분리 불가능 경우의 하이퍼플레인 (\\(C = 5\\)) Figure 3.3의 하이퍼플레인(\\(C = 5\\)인 경우)은 Figure 3.2의 하이퍼플레인(\\(C = 1\\)인 경우)보다 오분류 객체에 가깝게 위치함을 확인할 수 있다. 3.5 비선형 SVM 본 장에서는 선형으로 분리 성능이 좋지 않은 경우에 대해 원 입력변수에 대해 비선형인 하이퍼플레인을 찾는 문제를 다룬다. 이는 원 입력변수에 대해 비선형인 기저함수 공간으로 객체를 이동시킨 후 해당 공간에서 선형 분리 하이퍼플레인을 찾는 과정이다. 3.5.1 기본 R 스크립트 nonlinear_train_df &lt;- tibble( x1 = c(5, 4, 7, 8, 3, 2, 6, 9, 5), x2 = c(7, 3, 8, 6, 6, 5, 6, 6, 4), class = c(1, -1, -1, -1, 1, 1, 1, -1, -1) ) knitr::kable(nonlinear_train_df, booktabs = TRUE, align = c(&#39;r&#39;, &#39;r&#39;, &#39;r&#39;), caption = &#39;비선형 SVM 학습표본 데이터&#39;) Table 3.6: 비선형 SVM 학습표본 데이터 x1 x2 class 5 7 1 4 3 -1 7 8 -1 8 6 -1 3 6 1 2 5 1 6 6 1 9 6 -1 5 4 -1 library(e1071) svm_model &lt;- svm(as.factor(class) ~ x1 + x2, data = nonlinear_train_df, kernel = &quot;polynomial&quot;, coef0 = 1, gamma = 1, degree = 2, cost = 5, scale = FALSE) plot(svm_model, data = nonlinear_train_df, formula = x2 ~ x1, grid = 200) Figure 3.4: 비선형 SVM 하이퍼플레인 3.5.2 최적 하이퍼플레인 식 (3.1)을 일반화한 다음과 같은 하이퍼플레인을 고려하자. \\[\\begin{equation} f(\\mathbf{x}) = \\Phi(\\mathbf{x})^\\top \\mathbf{w} + b \\tag{3.6} \\end{equation}\\] 여기서 벡터함수 \\(\\Phi: \\mathbb{R}^p \\rightarrow \\mathbb{R}^m\\)는 \\(\\mathbf{x}\\)에 대한 새로운 특징(feature)을 추출하는 변환함수라 할 수 있는데, 통상 추출되는 특징의 차원 \\(m\\)이 원 변수 \\(\\mathbf{x}\\)의 차원 \\(p\\)보다 높다. 이를 \\(\\mathbf{x}\\)의 기저함수(basis function)라 부르며, 하이퍼플레인 계수 또한 \\(m\\)차원의 벡터가 된다 (\\(\\mathbf{w} \\in \\mathbb{R}^m\\)). 이 때, 비선형 SVM 문제는 선형 SVM 문제 식 (3.5)에서 변수를 기저변수로 치환한 형태로 아래 식 (3.7)과 같이 나타낼 수 있다. \\[\\begin{equation} \\begin{split} \\max \\text{ } &amp; L_D = \\sum_{i = 1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} \\alpha_i \\alpha_j y_i y_j \\Phi(\\mathbf{x}_i)^\\top \\Phi(\\mathbf{x}_j)\\\\ \\text{s.t. } &amp;\\\\ &amp; \\sum_{i = 1}^{N} \\alpha_i y_i = 0\\\\ &amp; 0 \\le \\alpha_i \\le C, \\text{ } i = 1, \\cdots, N \\end{split} \\tag{3.7} \\end{equation}\\] 식 (3.7)의 목적함수에서 기저함수의 내적 \\(\\Phi(\\mathbf{x}_i)^\\top \\Phi(\\mathbf{x}_j)\\)을 아래와 같이 커널함수(kernel function)로 나타낼 수 있으며, 이는 두 객체 \\(\\mathbf{x}_i, \\mathbf{x}_j\\)간의 일종의 유사성 척도(similarity measure)로 해석될 수 있다. \\[\\begin{equation*} K(\\mathbf{x}_i, \\mathbf{x}_j) = \\Phi(\\mathbf{x}_i)^\\top \\Phi(\\mathbf{x}_j) \\end{equation*}\\] 널리 사용되는 커널함수로는 아래와 같은 함수들이 있다. \\[\\begin{eqnarray*} \\text{Gaussian RBF:} &amp; &amp; K(\\mathbf{x}_i, \\mathbf{x}_j) &amp;=&amp; \\exp \\left( \\frac{- \\left\\lVert \\mathbf{x}_i - \\mathbf{x}_j \\right\\rVert^2}{2 \\sigma^2} \\right)\\\\ \\text{$r$-th order polynomial:} &amp; &amp; K(\\mathbf{x}_i, \\mathbf{x}_j) &amp;=&amp; \\left( \\mathbf{x}_i^\\top \\mathbf{x}_j + 1 \\right)^r \\\\ \\text{Sigmoid:} &amp; &amp; K(\\mathbf{x}_i, \\mathbf{x}_j) &amp;=&amp; \\tanh \\left(\\kappa \\mathbf{x}_i^\\top \\mathbf{x}_j - \\delta \\right) \\end{eqnarray*}\\] 커널함수를 이용하여 분리 하이퍼플레인을 찾기 위한 식을 아래와 같이 나타낸다. \\[\\begin{equation} \\begin{split} \\max \\text{ } &amp; L_D = \\sum_{i = 1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} \\alpha_i \\alpha_j y_i y_j k_{ij}\\\\ \\text{s.t. } &amp;\\\\ &amp; \\sum_{i = 1}^{N} \\alpha_i y_i = 0\\\\ &amp; 0 \\le \\alpha_i \\le C, \\text{ } i = 1, \\cdots, N \\end{split} \\tag{3.8} \\end{equation}\\] 이 때 \\(k_{ij}\\)는 \\(K(\\mathbf{x}_i, \\mathbf{x}_j)\\)를 나타낸다. 식 (3.8)의 최적해 \\(\\boldsymbol\\alpha^*\\)는 선형 SVM과 마찬가지로 이차계획(quadratic programming) 소프트웨어/알고리즘을 이용하여 구할 수 있다. Table 3.6의 학습데이터에 대해 e1071 패키지의 svm 함수를 이용하여 이차 다항 커널에 기반한 분리 하이퍼플레인을 구해보자. svm 함수에 파라미터값 kernel = &quot;polynomial&quot;를 설정함으로써 다항 커널을 사용할 수 있다. svm 함수의 다항 커널은 위에서 설명된 것보다 일반화된 형태로 아래와 같이 정의된다. \\[\\begin{equation*} K(\\mathbf{x}_i, \\mathbf{x}_j) = \\left( \\gamma \\mathbf{x}_i^\\top \\mathbf{x}_j + \\beta_0 \\right)^r \\end{equation*}\\] 위 커널함수의 파라미터 \\(\\gamma, \\beta_0, r\\)은 svm 함수에 파라미터 gamma, coef0, degree로 각각 정의된다. 따라서 이차 커널 \\[\\begin{equation*} K(\\mathbf{x}_i, \\mathbf{x}_j) = \\left( \\mathbf{x}_i^\\top \\mathbf{x}_j + 1 \\right)^2 \\end{equation*}\\] 에 기반한 SVM을 학습하기 위해서 아래와 같이 svm 함수를 호출한다. svm_model &lt;- svm(as.factor(class) ~ x1 + x2, data = nonlinear_train_df, kernel = &quot;polynomial&quot;, coef0 = 1, gamma = 1, degree = 2, scale = FALSE) 위 함수 호출 결과 서포트 벡터 객체는 1, 7, 2, 3, 9이다. 비선형 SVM의 분리 하이퍼플레인 또한 페널티 단가 \\(C\\)의 값에 따라 달라진다. 선형 SVM의 경우와 같이 \\(C = 1, 5, 100\\)에 대해 각각 비선형 SVM을 구해보자. svm_models &lt;- lapply( c(1, 5, 100), function(C) svm(as.factor(class) ~ x1 + x2, data = nonlinear_train_df, kernel = &quot;polynomial&quot;, coef0 = 1, gamma = 1, degree = 2, cost = C, scale = FALSE) ) getSummary &lt;- function(model) { list(C = model$cost, sv = paste(model$index, collapse = &quot;, &quot;), misclassified = paste(which(model$fitted != as.factor(nonlinear_train_df$class)), collapse = &quot;, &quot;)) } svm_summary &lt;- lapply(svm_models, getSummary) %&gt;% bind_rows() Table 3.7: 페널티 단가 C에 따른 비선형 SVM 결과 페널티 단가 \\(C\\) 서포트 벡터 객체 오분류 객체 1 1, 7, 2, 3, 9 7 5 6, 7, 2, 3, 9 100 6, 7, 2, 3, 9 3.6 R패키지 내 SVM 3.6.1 커널함수 앞 장에서는 선형 커널과 다항 커널함수의 예를 살펴보았다. 본 장에서는 가우시안 커널 및 시그모이드 커널을 사용하는 법을 살펴보자. 가우시안 커널의 경우 \\[\\begin{equation*} K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp \\left( -\\gamma \\left\\lVert \\mathbf{x}_i - \\mathbf{x}_j \\right\\rVert^2 \\right) \\end{equation*}\\] 과 같이 \\(\\gamma\\) 파라미터를 이용하여 함수를 정의하며, svm 함수에 gamma 파라미터값을 통해 설정할 수 있다. svm_model &lt;- svm(as.factor(class) ~ x1 + x2, data = nonlinear_train_df, kernel = &quot;radial&quot;, gamma = 1, cost = 5, scale = FALSE) plot(svm_model, data = nonlinear_train_df, formula = x2 ~ x1, grid = 200) Figure 3.5: 가우시안 커널을 이용한 비선형 SVM 하이퍼플레인 시그모이드 커널의 경우 \\[\\begin{equation*} K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh \\left(\\gamma \\mathbf{x}_i^\\top \\mathbf{x}_j + \\beta_0 \\right) \\end{equation*}\\] 와 같이 두 파라미터 \\(\\gamma, \\beta_0\\)의 값에 대응하는 svm 함수의 파라미터 gamma, coef0 값을 통해 설정할 수 있다. svm_model &lt;- svm(as.factor(class) ~ x1 + x2, data = nonlinear_train_df, kernel = &quot;sigmoid&quot;, gamma = 0.01, coef0 = -1, cost = 5, scale = FALSE) plot(svm_model, data = nonlinear_train_df, formula = x2 ~ x1, grid = 200) Figure 3.6: 시그모이드 커널을 이용한 비선형 SVM 하이퍼플레인 커널 함수의 종류 kernel, 커널 함수의 파라미터 gamma, coef0, degree, 페널티 단가 cost등의 svm 함수 파라미터는 학습 표본과는 별도의 테스트 데이터에 대해 오분류율을 최소화하는 값을 선택하는 것이 일반적이다. 3.6.2 \\(\\nu\\)-SVC \\(\\nu\\)-support vector classification(\\(\\nu\\)-SVC) (Schölkopf et al. 2000, Chang and Lin (2001))은 \\(C\\)-SVC의 이차계획식 (3.8)과 다른 형태로, 페널티 단가 \\(C\\) 대신 \\(\\nu\\)라는 파라미터를 이용한 아래 최적화 문제의 해를 구한다. \\[\\begin{equation} \\begin{split} \\min \\text{ } &amp; L_D = \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} \\alpha_i \\alpha_j y_i y_j k_{ij}\\\\ \\text{s.t. } &amp;\\\\ &amp; \\sum_{i = 1}^{N} \\alpha_i y_i = 0\\\\ &amp; 0 \\le \\alpha_i \\le \\frac{1}{N}, \\text{ } i = 1, \\cdots, N\\\\ &amp; \\sum_{i = 1}^{N} \\alpha_i = \\nu \\end{split} \\tag{3.9} \\end{equation}\\] 이 때, 각 \\(\\alpha_i\\)의 최대값은 \\(1/N\\)으로, \\(\\nu\\)를 포함한 제약식을 무시할 때 모든 객체에 대한 \\(\\alpha_i\\)값의 합의 이론적 최대치는 1이 되며, \\(\\nu \\in (0, 1]\\)은 전체 객체 중 서포트 벡터 객체의 개수를 제한하는 개념으로 생각할 수 있다. 식 (3.9)이 실제로 최적해를 가지기 위한 \\(\\nu\\)값의 범위는 \\[\\begin{equation*} 0 &lt; \\nu \\le \\frac{2}{N} \\min \\left( \\sum_i I(y_i = 1), \\sum_i I(y_i = -1) \\right) \\end{equation*}\\] 으로 (Chang and Lin 2001), 에를 들어 범주 1에 속하는 학습표본 객체 수가 전체의 10% 라면, \\(\\nu\\) 값은 최대 0.2 까지 설정할 수 있다. 또한 svm 함수가 호출하는 LIBSVM 라이브러리는 위 식 (3.9)을 \\(N\\)이 큰(학습 표본 수가 매우 많은) 경우에도 안정된 결과를 얻을 수 있도록 아래와 같이 변환한 문제를 다룬다. \\[\\begin{equation} \\begin{split} \\min \\text{ } &amp; L_D = \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} \\bar{\\alpha}_i \\bar{\\alpha}_j y_i y_j k_{ij}\\\\ \\text{s.t. } &amp;\\\\ &amp; \\sum_{i = 1}^{N} \\bar{\\alpha}_i y_i = 0\\\\ &amp; 0 \\le \\bar{\\alpha}_i \\le 1, \\text{ } i = 1, \\cdots, N\\\\ &amp; \\sum_{i = 1}^{N} \\bar{\\alpha}_i = \\nu N \\end{split} \\tag{3.10} \\end{equation}\\] 이 때 \\(\\bar{\\alpha}_i = \\alpha_i N\\)이다. \\(\\nu\\)-SVC은 아래와 같이 svm 함수를 호출할 때 type = &quot;nu-classification&quot;과 파라미터 nu 값을 설정함으로써 학습할 수 있다. svm_model &lt;- svm(as.factor(class) ~ x1 + x2, data = nonlinear_train_df, type = &quot;nu-classification&quot;, kernel = &quot;radial&quot;, gamma = 1, nu = 0.5, scale = FALSE) plot(svm_model, data = nonlinear_train_df, formula = x2 ~ x1, grid = 200) Figure 3.7: 가우시안 커널을 이용한 \\(\\nu\\)-SVC 하이퍼플레인 References "],
["references.html", "References", " References "]
]
