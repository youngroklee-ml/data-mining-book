<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 트리기반 기법 | 데이터마이닝 with R</title>
  <meta name="description" content="전치혁 교수님의 책 을 기반으로 한 R 예제" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 트리기반 기법 | 데이터마이닝 with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://youngroklee-ml.github.io/data-mining-book/" />
  
  <meta property="og:description" content="전치혁 교수님의 책 을 기반으로 한 R 예제" />
  <meta name="github-repo" content="youngroklee-ml/data-mining-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 트리기반 기법 | 데이터마이닝 with R" />
  
  <meta name="twitter:description" content="전치혁 교수님의 책 을 기반으로 한 R 예제" />
  

<meta name="author" content="전치혁, 이혜선, 이종석, 이영록" />


<meta name="date" content="2019-07-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="da.html">
<link rel="next" href="svm.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">데이터마이닝 with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>개요</a></li>
<li class="chapter" data-level="1" data-path="datamining-overview.html"><a href="datamining-overview.html"><i class="fa fa-check"></i><b>1</b> 데이터마이닝 개요</a></li>
<li class="part"><span><b>I 1부 - 예측</b></span></li>
<li class="chapter" data-level="2" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>2</b> 회귀분석</a><ul>
<li class="chapter" data-level="2.1" data-path="regression.html"><a href="regression.html#regression-packages-install"><i class="fa fa-check"></i><b>2.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="2.2" data-path="regression.html"><a href="regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>2.2</b> 다중회귀모형</a></li>
<li class="chapter" data-level="2.3" data-path="regression.html"><a href="regression.html#regression-response-confidence-prediction"><i class="fa fa-check"></i><b>2.3</b> 반응치에 대한 추정 및 예측</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regression.html"><a href="regression.html#regression-response-confidence"><i class="fa fa-check"></i><b>2.3.1</b> 평균반응치의 추정</a></li>
<li class="chapter" data-level="2.3.2" data-path="regression.html"><a href="regression.html#regression-response-prediction"><i class="fa fa-check"></i><b>2.3.2</b> 미래반응치의 예측</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regression.html"><a href="regression.html#regression-indicator-variable"><i class="fa fa-check"></i><b>2.4</b> 지시변수와 회귀모형</a><ul>
<li class="chapter" data-level="2.4.1" data-path="regression.html"><a href="regression.html#regression-indicator-variable-basic-script"><i class="fa fa-check"></i><b>2.4.1</b> 기본 R 스트립트</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>3</b> 주성분분석</a><ul>
<li class="chapter" data-level="3.1" data-path="pca.html"><a href="pca.html#pca-packages-install"><i class="fa fa-check"></i><b>3.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="3.2" data-path="pca.html"><a href="pca.html#pca-matrix-factorization"><i class="fa fa-check"></i><b>3.2</b> 행렬의 분해</a><ul>
<li class="chapter" data-level="3.2.1" data-path="pca.html"><a href="pca.html#pca-matrix-factorization-basic-script"><i class="fa fa-check"></i><b>3.2.1</b> 기본 R 스트립트</a></li>
<li class="chapter" data-level="3.2.2" data-path="pca.html"><a href="pca.html#pca-ss"><i class="fa fa-check"></i><b>3.2.2</b> 변수의 변동과 제곱합</a></li>
<li class="chapter" data-level="3.2.3" data-path="pca.html"><a href="pca.html#pca-intro"><i class="fa fa-check"></i><b>3.2.3</b> 주성분의 이해 및 행렬의 분해</a></li>
<li class="chapter" data-level="3.2.4" data-path="pca.html"><a href="pca.html#pca-svd"><i class="fa fa-check"></i><b>3.2.4</b> 특이치분해 (Singular Value Decomposition)</a></li>
<li class="chapter" data-level="3.2.5" data-path="pca.html"><a href="pca.html#pca-spectral"><i class="fa fa-check"></i><b>3.2.5</b> 분광분해 (Spectral Decomposition)</a></li>
<li class="chapter" data-level="3.2.6" data-path="pca.html"><a href="pca.html#pca-nipals"><i class="fa fa-check"></i><b>3.2.6</b> NIPALS 알고리즘</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="pca.html"><a href="pca.html#pca-regression"><i class="fa fa-check"></i><b>3.3</b> 주성분 회귀분석</a><ul>
<li class="chapter" data-level="3.3.1" data-path="pca.html"><a href="pca.html#pcr-basic-script"><i class="fa fa-check"></i><b>3.3.1</b> 기본 R 스트립트</a></li>
<li class="chapter" data-level="3.3.2" data-path="pca.html"><a href="pca.html#pcr-regression-coefficient"><i class="fa fa-check"></i><b>3.3.2</b> 주성분 회귀계수 추정</a></li>
<li class="chapter" data-level="3.3.3" data-path="pca.html"><a href="pca.html#pcr-regression-transform"><i class="fa fa-check"></i><b>3.3.3</b> 회귀계수 선형변환</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="plsr.html"><a href="plsr.html"><i class="fa fa-check"></i><b>4</b> 부분최소자승법</a><ul>
<li class="chapter" data-level="4.1" data-path="plsr.html"><a href="plsr.html#plsr-packages-install"><i class="fa fa-check"></i><b>4.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="4.2" data-path="plsr.html"><a href="plsr.html#plsr-single-target"><i class="fa fa-check"></i><b>4.2</b> 하나의 종속변수의 경우</a><ul>
<li class="chapter" data-level="4.2.1" data-path="plsr.html"><a href="plsr.html#plsr-basic-script"><i class="fa fa-check"></i><b>4.2.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="4.2.2" data-path="plsr.html"><a href="plsr.html#plsr-model"><i class="fa fa-check"></i><b>4.2.2</b> PLS 모형</a></li>
<li class="chapter" data-level="4.2.3" data-path="plsr.html"><a href="plsr.html#plsr-single-nipals"><i class="fa fa-check"></i><b>4.2.3</b> NIPALS 알고리즘</a></li>
<li class="chapter" data-level="4.2.4" data-path="plsr.html"><a href="plsr.html#plsr-single-transform"><i class="fa fa-check"></i><b>4.2.4</b> 회귀식 변환</a></li>
<li class="chapter" data-level="4.2.5" data-path="plsr.html"><a href="plsr.html#plsr-sst"><i class="fa fa-check"></i><b>4.2.5</b> 제곱합 분해</a></li>
<li class="chapter" data-level="4.2.6" data-path="plsr.html"><a href="plsr.html#plsr-variable-importance"><i class="fa fa-check"></i><b>4.2.6</b> 독립변수의 중요도</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="plsr.html"><a href="plsr.html#plsr-multivariate-target"><i class="fa fa-check"></i><b>4.3</b> 다수의 종속변수의 경우</a><ul>
<li class="chapter" data-level="4.3.1" data-path="plsr.html"><a href="plsr.html#plsr-multivariate-basic-script"><i class="fa fa-check"></i><b>4.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="4.3.2" data-path="plsr.html"><a href="plsr.html#plsr-multivariate-model"><i class="fa fa-check"></i><b>4.3.2</b> PLS 모형</a></li>
<li class="chapter" data-level="4.3.3" data-path="plsr.html"><a href="plsr.html#plsr-multivariate-nipals"><i class="fa fa-check"></i><b>4.3.3</b> NIPALS 알고리즘</a></li>
<li class="chapter" data-level="4.3.4" data-path="plsr.html"><a href="plsr.html#plsr-multivariate-transform"><i class="fa fa-check"></i><b>4.3.4</b> 회귀식 변환</a></li>
<li class="chapter" data-level="4.3.5" data-path="plsr.html"><a href="plsr.html#plsr-multivariate-sst"><i class="fa fa-check"></i><b>4.3.5</b> 제곱합 분해</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II 2부 - 분류분석</b></span></li>
<li class="chapter" data-level="5" data-path="classification-analysis.html"><a href="classification-analysis.html"><i class="fa fa-check"></i><b>5</b> 분류분석 개요</a><ul>
<li class="chapter" data-level="5.1" data-path="classification-analysis.html"><a href="classification-analysis.html#classification-packages-install"><i class="fa fa-check"></i><b>5.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="5.2" data-path="classification-analysis.html"><a href="classification-analysis.html#classification-problem-methods"><i class="fa fa-check"></i><b>5.2</b> 분류문제 및 분류기법</a></li>
<li class="chapter" data-level="5.3" data-path="classification-analysis.html"><a href="classification-analysis.html#simple-classification-methods"><i class="fa fa-check"></i><b>5.3</b> 기본적인 분류기법</a><ul>
<li class="chapter" data-level="5.3.1" data-path="classification-analysis.html"><a href="classification-analysis.html#nearest-neighbor-classification"><i class="fa fa-check"></i><b>5.3.1</b> 인접객체법</a></li>
<li class="chapter" data-level="5.3.2" data-path="classification-analysis.html"><a href="classification-analysis.html#naive-bayes"><i class="fa fa-check"></i><b>5.3.2</b> 나이브 베이지안 분류법</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>6</b> 로지스틱 회귀분석</a><ul>
<li class="chapter" data-level="6.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-packages-install"><i class="fa fa-check"></i><b>6.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="6.2" data-path="logistic-regression.html"><a href="logistic-regression.html#binary-logistic-regression"><i class="fa fa-check"></i><b>6.2</b> 이분 로지스틱 회귀모형</a><ul>
<li class="chapter" data-level="6.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#bianry-logistic-reg-basic-script"><i class="fa fa-check"></i><b>6.2.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="6.2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#binary-logistic-reg-model"><i class="fa fa-check"></i><b>6.2.2</b> 회귀모형</a></li>
<li class="chapter" data-level="6.2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#binary-logistic-reg-estimation"><i class="fa fa-check"></i><b>6.2.3</b> 회귀계수 추정</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="logistic-regression.html"><a href="logistic-regression.html#nominal-logistic-regression"><i class="fa fa-check"></i><b>6.3</b> 명목 로지스틱 회귀모형</a><ul>
<li class="chapter" data-level="6.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#nominal-logistic-reg-basic-script"><i class="fa fa-check"></i><b>6.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="6.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#baseline-category-logit-model"><i class="fa fa-check"></i><b>6.3.2</b> 기준범주 로짓모형</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="logistic-regression.html"><a href="logistic-regression.html#ordinal-logistic-regression"><i class="fa fa-check"></i><b>6.4</b> 서열 로지스틱 회귀모형</a><ul>
<li class="chapter" data-level="6.4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#ordinal-logistic-basic-script"><i class="fa fa-check"></i><b>6.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="6.4.2" data-path="logistic-regression.html"><a href="logistic-regression.html#cumulative-logit-model"><i class="fa fa-check"></i><b>6.4.2</b> 누적 로짓모형</a></li>
<li class="chapter" data-level="6.4.3" data-path="logistic-regression.html"><a href="logistic-regression.html#adjacent-categories-logit-model"><i class="fa fa-check"></i><b>6.4.3</b> 인근범주 로짓모형</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="da.html"><a href="da.html"><i class="fa fa-check"></i><b>7</b> 판별분석</a><ul>
<li class="chapter" data-level="7.1" data-path="da.html"><a href="da.html#da-overview"><i class="fa fa-check"></i><b>7.1</b> 개요</a></li>
<li class="chapter" data-level="7.2" data-path="da.html"><a href="da.html#da-packages-install"><i class="fa fa-check"></i><b>7.2</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="7.3" data-path="da.html"><a href="da.html#da-fisher"><i class="fa fa-check"></i><b>7.3</b> 피셔 방법</a><ul>
<li class="chapter" data-level="7.3.1" data-path="da.html"><a href="da.html#da-fisher-basic-script"><i class="fa fa-check"></i><b>7.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="7.3.2" data-path="da.html"><a href="da.html#-"><i class="fa fa-check"></i><b>7.3.2</b> 피셔 판별함수</a></li>
<li class="chapter" data-level="7.3.3" data-path="da.html"><a href="da.html#-"><i class="fa fa-check"></i><b>7.3.3</b> 분류 규칙</a></li>
<li class="chapter" data-level="7.3.4" data-path="da.html"><a href="da.html#r----"><i class="fa fa-check"></i><b>7.3.4</b> R 패키지를 이용한 분류규칙 도출</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="da.html"><a href="da.html#lda"><i class="fa fa-check"></i><b>7.4</b> 의사결정론에 의한 선형분류규칙</a><ul>
<li class="chapter" data-level="7.4.1" data-path="da.html"><a href="da.html#lda-basic-script"><i class="fa fa-check"></i><b>7.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="7.4.2" data-path="da.html"><a href="da.html#lda-function"><i class="fa fa-check"></i><b>7.4.2</b> 선형판별함수</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="da.html"><a href="da.html#lda-misclassification-cost"><i class="fa fa-check"></i><b>7.5</b> 오분류비용을 고려한 분류규칙</a></li>
<li class="chapter" data-level="7.6" data-path="da.html"><a href="da.html#qda"><i class="fa fa-check"></i><b>7.6</b> 이차판별분석</a><ul>
<li class="chapter" data-level="7.6.1" data-path="da.html"><a href="da.html#qda-basic-script"><i class="fa fa-check"></i><b>7.6.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="7.6.2" data-path="da.html"><a href="da.html#qda-function"><i class="fa fa-check"></i><b>7.6.2</b> 이차 판별함수</a></li>
<li class="chapter" data-level="7.6.3" data-path="da.html"><a href="da.html#qda-discriminant-rule"><i class="fa fa-check"></i><b>7.6.3</b> 이차판별함수에 의한 분류</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="da.html"><a href="da.html#da-multiclass"><i class="fa fa-check"></i><b>7.7</b> 세 범주 이상의 분류</a><ul>
<li class="chapter" data-level="7.7.1" data-path="da.html"><a href="da.html#mutliclass-da-basic-script"><i class="fa fa-check"></i><b>7.7.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="7.7.2" data-path="da.html"><a href="da.html#mutliclass-generalized-discriminant-function"><i class="fa fa-check"></i><b>7.7.2</b> 일반화된 판별함수</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="tree-based-method.html"><a href="tree-based-method.html"><i class="fa fa-check"></i><b>8</b> 트리기반 기법</a><ul>
<li class="chapter" data-level="8.1" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-overview"><i class="fa fa-check"></i><b>8.1</b> CART 개요</a></li>
<li class="chapter" data-level="8.2" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-packages-install"><i class="fa fa-check"></i><b>8.2</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="8.3" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-build"><i class="fa fa-check"></i><b>8.3</b> CART 트리 생성</a><ul>
<li class="chapter" data-level="8.3.1" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-basic-r-script"><i class="fa fa-check"></i><b>8.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="8.3.2" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-notation"><i class="fa fa-check"></i><b>8.3.2</b> 기호 정의</a></li>
<li class="chapter" data-level="8.3.3" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-impurity"><i class="fa fa-check"></i><b>8.3.3</b> 노드 및 트리의 불순도</a></li>
<li class="chapter" data-level="8.3.4" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-split"><i class="fa fa-check"></i><b>8.3.4</b> 분지기준</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-pruning-complete"><i class="fa fa-check"></i><b>8.4</b> 가지치기 및 최종 트리 선정</a><ul>
<li class="chapter" data-level="8.4.1" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-pruning"><i class="fa fa-check"></i><b>8.4.1</b> 가지치기</a></li>
<li class="chapter" data-level="8.4.2" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-best-tree"><i class="fa fa-check"></i><b>8.4.2</b> 최적 트리의 선정</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-r-pkg"><i class="fa fa-check"></i><b>8.5</b> R패키지 내 분류 트리 방법</a><ul>
<li class="chapter" data-level="8.5.1" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-r-pkg-split"><i class="fa fa-check"></i><b>8.5.1</b> 트리 확장</a></li>
<li class="chapter" data-level="8.5.2" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-r-pkg-pruning"><i class="fa fa-check"></i><b>8.5.2</b> 가지치기</a></li>
<li class="chapter" data-level="8.5.3" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-r-pkg-param"><i class="fa fa-check"></i><b>8.5.3</b> 파라미터값 결정</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>9</b> 서포트 벡터 머신</a><ul>
<li class="chapter" data-level="9.1" data-path="svm.html"><a href="svm.html#svm-overview"><i class="fa fa-check"></i><b>9.1</b> 개요</a></li>
<li class="chapter" data-level="9.2" data-path="svm.html"><a href="svm.html#svm-packages-install"><i class="fa fa-check"></i><b>9.2</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="9.3" data-path="svm.html"><a href="svm.html#linear-svm-separable"><i class="fa fa-check"></i><b>9.3</b> 선형 SVM - 분리 가능 경우</a><ul>
<li class="chapter" data-level="9.3.1" data-path="svm.html"><a href="svm.html#linear-svm-separable-basic-script"><i class="fa fa-check"></i><b>9.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="9.3.2" data-path="svm.html"><a href="svm.html#linear-svm-notation"><i class="fa fa-check"></i><b>9.3.2</b> 기호 정의</a></li>
<li class="chapter" data-level="9.3.3" data-path="svm.html"><a href="svm.html#linear-svm-separable-hyperplane"><i class="fa fa-check"></i><b>9.3.3</b> 최적 하이퍼플레인</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="svm.html"><a href="svm.html#linear-svm-inseparable"><i class="fa fa-check"></i><b>9.4</b> 선형 SVM - 분리 불가능 경우</a><ul>
<li class="chapter" data-level="9.4.1" data-path="svm.html"><a href="svm.html#linear-svm-inseparable-basic-script"><i class="fa fa-check"></i><b>9.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="9.4.2" data-path="svm.html"><a href="svm.html#linear-svm-inseparable-hyperplane"><i class="fa fa-check"></i><b>9.4.2</b> 최적 하이퍼플레인</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="svm.html"><a href="svm.html#nonlinear-svm"><i class="fa fa-check"></i><b>9.5</b> 비선형 SVM</a><ul>
<li class="chapter" data-level="9.5.1" data-path="svm.html"><a href="svm.html#nonlinear-svm-basic-script"><i class="fa fa-check"></i><b>9.5.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="9.5.2" data-path="svm.html"><a href="svm.html#nonlinear-svm-hyperplane"><i class="fa fa-check"></i><b>9.5.2</b> 최적 하이퍼플레인</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="svm.html"><a href="svm.html#svm-r-pkg"><i class="fa fa-check"></i><b>9.6</b> R패키지 내 SVM</a><ul>
<li class="chapter" data-level="9.6.1" data-path="svm.html"><a href="svm.html#svm-kernel-function"><i class="fa fa-check"></i><b>9.6.1</b> 커널함수</a></li>
<li class="chapter" data-level="9.6.2" data-path="svm.html"><a href="svm.html#svm-nu-classification"><i class="fa fa-check"></i><b>9.6.2</b> <span class="math inline">\(\nu\)</span>-SVC</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="classifier-evaluation.html"><a href="classifier-evaluation.html"><i class="fa fa-check"></i><b>10</b> 분류규칙의 성능 평가</a><ul>
<li class="chapter" data-level="10.1" data-path="classifier-evaluation.html"><a href="classifier-evaluation.html#classifier-evaluation-packages-install"><i class="fa fa-check"></i><b>10.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="10.2" data-path="classifier-evaluation.html"><a href="classifier-evaluation.html#classifier-evaluation-misclassification-rate"><i class="fa fa-check"></i><b>10.2</b> 분류오류율</a></li>
<li class="chapter" data-level="10.3" data-path="classifier-evaluation.html"><a href="classifier-evaluation.html#precision-sensitivity-specificity"><i class="fa fa-check"></i><b>10.3</b> 정확도, 민감도 및 특이도</a><ul>
<li class="chapter" data-level="10.3.1" data-path="classifier-evaluation.html"><a href="classifier-evaluation.html#confusion-matrix-r-package"><i class="fa fa-check"></i><b>10.3.1</b> R 패키지 내 정오분류표</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="classifier-evaluation.html"><a href="classifier-evaluation.html#roc-curve"><i class="fa fa-check"></i><b>10.4</b> ROC 곡선</a></li>
<li class="chapter" data-level="10.5" data-path="classifier-evaluation.html"><a href="classifier-evaluation.html#gain-chart"><i class="fa fa-check"></i><b>10.5</b> 이익도표</a></li>
</ul></li>
<li class="part"><span><b>III 3부 - 군집분석</b></span></li>
<li class="chapter" data-level="11" data-path="clustering-overview.html"><a href="clustering-overview.html"><i class="fa fa-check"></i><b>11</b> 군집분석 개요</a><ul>
<li class="chapter" data-level="11.1" data-path="clustering-overview.html"><a href="clustering-overview.html#clustering-overview-packages-install"><i class="fa fa-check"></i><b>11.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="11.2" data-path="clustering-overview.html"><a href="clustering-overview.html#clustering-method"><i class="fa fa-check"></i><b>11.2</b> 군집분석 기법</a></li>
<li class="chapter" data-level="11.3" data-path="clustering-overview.html"><a href="clustering-overview.html#object-similarity-metric"><i class="fa fa-check"></i><b>11.3</b> 객체 간의 유사성 척도</a><ul>
<li class="chapter" data-level="11.3.1" data-path="clustering-overview.html"><a href="clustering-overview.html#object-distance-metric"><i class="fa fa-check"></i><b>11.3.1</b> 거리 관련 척도</a></li>
<li class="chapter" data-level="11.3.2" data-path="clustering-overview.html"><a href="clustering-overview.html#object-correlation-metric"><i class="fa fa-check"></i><b>11.3.2</b> 상관계수 관련 척도</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="clustering-overview.html"><a href="clustering-overview.html#category-similarity-metric"><i class="fa fa-check"></i><b>11.4</b> 범주형 객체의 유사성 척도</a><ul>
<li class="chapter" data-level="11.4.1" data-path="clustering-overview.html"><a href="clustering-overview.html#binary-similarity-metric"><i class="fa fa-check"></i><b>11.4.1</b> 이분형 변수의 경우</a></li>
<li class="chapter" data-level="11.4.2" data-path="clustering-overview.html"><a href="clustering-overview.html#ordinal-similarity-metric"><i class="fa fa-check"></i><b>11.4.2</b> 서열형 변수의 경우</a></li>
<li class="chapter" data-level="11.4.3" data-path="clustering-overview.html"><a href="clustering-overview.html#nominal-similarity-metric"><i class="fa fa-check"></i><b>11.4.3</b> 명목형 변수의 경우</a></li>
<li class="chapter" data-level="11.4.4" data-path="clustering-overview.html"><a href="clustering-overview.html#mixed-similarity-metric"><i class="fa fa-check"></i><b>11.4.4</b> 혼합형의 경우</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>12</b> 계층적 군집방법</a><ul>
<li class="chapter" data-level="12.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#hierarchical-clustering-packages-install"><i class="fa fa-check"></i><b>12.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="12.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#distance-between-clusters"><i class="fa fa-check"></i><b>12.2</b> 군집 간 거리척도 및 연결법</a></li>
<li class="chapter" data-level="12.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#linkage-method"><i class="fa fa-check"></i><b>12.3</b> 연결법의 군집 알고리즘</a><ul>
<li class="chapter" data-level="12.3.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#linkage-method-basic-script"><i class="fa fa-check"></i><b>12.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="12.3.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#linkage-method-algorithm"><i class="fa fa-check"></i><b>12.3.2</b> 연결법 군집 알고리즘</a></li>
<li class="chapter" data-level="12.3.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#hclust"><i class="fa fa-check"></i><b>12.3.3</b> R 패키지 내 연결법</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#ward-method"><i class="fa fa-check"></i><b>12.4</b> 워드 방법</a><ul>
<li class="chapter" data-level="12.4.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#ward-method-basic-script"><i class="fa fa-check"></i><b>12.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="12.4.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#ward-method-algorithm"><i class="fa fa-check"></i><b>12.4.2</b> 워드 군집 알고리즘</a></li>
<li class="chapter" data-level="12.4.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#ward-rpackages"><i class="fa fa-check"></i><b>12.4.3</b> R 패키지 내 워드 방법</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#diana"><i class="fa fa-check"></i><b>12.5</b> 분리적 방법 - 다이아나</a><ul>
<li class="chapter" data-level="12.5.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#diana-basic-script"><i class="fa fa-check"></i><b>12.5.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="12.5.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#diana-algorithm"><i class="fa fa-check"></i><b>12.5.2</b> 다이아나 알고리즘</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#hierarchical-cluster-number"><i class="fa fa-check"></i><b>12.6</b> 군집수의 결정</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html"><i class="fa fa-check"></i><b>13</b> 비계층적 군집방법</a><ul>
<li class="chapter" data-level="13.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#nonhierarchical-clustering-packages-install"><i class="fa fa-check"></i><b>13.1</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="13.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans"><i class="fa fa-check"></i><b>13.2</b> K-means 알고리즘</a><ul>
<li class="chapter" data-level="13.2.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans-basic-script"><i class="fa fa-check"></i><b>13.2.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="13.2.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans-algorithm"><i class="fa fa-check"></i><b>13.2.2</b> 알고리즘</a></li>
<li class="chapter" data-level="13.2.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans-user-defined-functions"><i class="fa fa-check"></i><b>13.2.3</b> R 스크립트 구현</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmedoids"><i class="fa fa-check"></i><b>13.3</b> K-medoids 군집방법</a><ul>
<li class="chapter" data-level="13.3.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#pam"><i class="fa fa-check"></i><b>13.3.1</b> PAM 알고리즘</a></li>
<li class="chapter" data-level="13.3.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#clara"><i class="fa fa-check"></i><b>13.3.2</b> CLARA 알고리즘</a></li>
<li class="chapter" data-level="13.3.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#clarans"><i class="fa fa-check"></i><b>13.3.3</b> CLARANS 알고리즘</a></li>
<li class="chapter" data-level="13.3.4" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans-like"><i class="fa fa-check"></i><b>13.3.4</b> K-means-like 알고리즘</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#fuzzy-kmeans"><i class="fa fa-check"></i><b>13.4</b> 퍼지 K-means 알고리즘</a><ul>
<li class="chapter" data-level="13.4.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#fuzzy-kmeans-basic-script"><i class="fa fa-check"></i><b>13.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="13.4.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#fuzzy-kmeans-algorithm"><i class="fa fa-check"></i><b>13.4.2</b> 알고리즘</a></li>
<li class="chapter" data-level="13.4.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#fuzzy-kmeans-script-implement"><i class="fa fa-check"></i><b>13.4.3</b> R 스크립트 구현</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#model-based-clustering"><i class="fa fa-check"></i><b>13.5</b> 모형기반 군집방법</a><ul>
<li class="chapter" data-level="13.5.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#model-based-clustering-basic-script"><i class="fa fa-check"></i><b>13.5.1</b> 기본 R script</a></li>
<li class="chapter" data-level="13.5.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#model-based-clustering-em"><i class="fa fa-check"></i><b>13.5.2</b> EM 알고리즘</a></li>
<li class="chapter" data-level="13.5.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#model-based-clustering-script-implement"><i class="fa fa-check"></i><b>13.5.3</b> R 스크립트 구현</a></li>
<li class="chapter" data-level="13.5.4" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#r-packages-model-based-clustering"><i class="fa fa-check"></i><b>13.5.4</b> R 패키지 내 모형기반 군집분석</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html"><i class="fa fa-check"></i><b>14</b> 군집해의 평가 및 해석</a><ul>
<li class="chapter" data-level="14.1" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html#cluster-solution-evaluation-packages-install"><i class="fa fa-check"></i><b>14.1</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="14.2" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html#cluster-solution-evaluation-metric"><i class="fa fa-check"></i><b>14.2</b> 군집해의 평가</a><ul>
<li class="chapter" data-level="14.2.1" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html#cluster-evaluation-external-index"><i class="fa fa-check"></i><b>14.2.1</b> 외부평가지수</a></li>
<li class="chapter" data-level="14.2.2" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html#cluster-evaluation-internal-index"><i class="fa fa-check"></i><b>14.2.2</b> 내부평가지수</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html#cluster-solution-interpretation"><i class="fa fa-check"></i><b>14.3</b> 군집해의 해석</a></li>
</ul></li>
<li class="part"><span><b>IV 4부 - 연관규칙</b></span></li>
<li class="chapter" data-level="15" data-path="association-rule.html"><a href="association-rule.html"><i class="fa fa-check"></i><b>15</b> 연관규칙</a><ul>
<li class="chapter" data-level="15.1" data-path="association-rule.html"><a href="association-rule.html#association-packages-install"><i class="fa fa-check"></i><b>15.1</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="15.2" data-path="association-rule.html"><a href="association-rule.html#association-rule-definition-metric"><i class="fa fa-check"></i><b>15.2</b> 연관규칙의 정의 및 성능척도</a><ul>
<li class="chapter" data-level="15.2.1" data-path="association-rule.html"><a href="association-rule.html#association-rule-support"><i class="fa fa-check"></i><b>15.2.1</b> 지지도</a></li>
<li class="chapter" data-level="15.2.2" data-path="association-rule.html"><a href="association-rule.html#association-rule-confidence"><i class="fa fa-check"></i><b>15.2.2</b> 신뢰도</a></li>
<li class="chapter" data-level="15.2.3" data-path="association-rule.html"><a href="association-rule.html#association-rule-lift"><i class="fa fa-check"></i><b>15.2.3</b> 개선도</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="association-rule.html"><a href="association-rule.html#association-rule-exploration"><i class="fa fa-check"></i><b>15.3</b> 연관규칙의 탐사</a><ul>
<li class="chapter" data-level="15.3.1" data-path="association-rule.html"><a href="association-rule.html#apriori-large-itemsets"><i class="fa fa-check"></i><b>15.3.1</b> 빈발항목집합 생성</a></li>
<li class="chapter" data-level="15.3.2" data-path="association-rule.html"><a href="association-rule.html#apriori-rule-exploration"><i class="fa fa-check"></i><b>15.3.2</b> 규칙의 탐사</a></li>
<li class="chapter" data-level="15.3.3" data-path="association-rule.html"><a href="association-rule.html#apriori-r-package"><i class="fa fa-check"></i><b>15.3.3</b> R 패키지 내 Apriori</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="association-rule.html"><a href="association-rule.html#association-sequential-pattern"><i class="fa fa-check"></i><b>15.4</b> 순차적 패턴의 탐사</a><ul>
<li class="chapter" data-level="15.4.1" data-path="association-rule.html"><a href="association-rule.html#association-aprioriall"><i class="fa fa-check"></i><b>15.4.1</b> AprioriAll 알고리즘</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="recommender-system.html"><a href="recommender-system.html"><i class="fa fa-check"></i><b>16</b> 추천시스템</a><ul>
<li class="chapter" data-level="16.1" data-path="recommender-system.html"><a href="recommender-system.html#recommender-packages-install"><i class="fa fa-check"></i><b>16.1</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="16.2" data-path="recommender-system.html"><a href="recommender-system.html#content-based-recommender"><i class="fa fa-check"></i><b>16.2</b> 내용기반 추천시스템</a></li>
<li class="chapter" data-level="16.3" data-path="recommender-system.html"><a href="recommender-system.html#collaborative-filtering"><i class="fa fa-check"></i><b>16.3</b> 협업 필터링</a></li>
<li class="chapter" data-level="16.4" data-path="recommender-system.html"><a href="recommender-system.html#market-basket"><i class="fa fa-check"></i><b>16.4</b> 시장바구니 데이터를 이용한 협업 필터링</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">데이터마이닝 with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tree-based-method" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> 트리기반 기법</h1>
<div id="cart-overview" class="section level2">
<h2><span class="header-section-number">8.1</span> CART 개요</h2>
<p>CART(Classification and Regression Trees)는 <span class="citation">Breiman et al. (<a href="#ref-breiman1984classification">1984</a>)</span> 에 의하여 개발된 것인데, 각 (독립)변수를 이분화(binary split)하는 과정을 반복하여 트리 형태를 형성함으로써 분류(종속변수가 범주형일 때) 또는 회귀분석(종속변수가 연속형일 때)을 수행하는 것이다. 이 때 독립변수들은 범주형 또는 연속형 모두에 적용될 수 있다. 본 장에서는 분류를 위한 목적만을 설명하도록 한다.</p>
</div>
<div id="cart-packages-install" class="section level2">
<h2><span class="header-section-number">8.2</span> 필요 R package 설치</h2>
<p>본 장에서 필요한 R 패키지들은 아래와 같다.</p>
<table>
<thead>
<tr class="header">
<th align="left">package</th>
<th align="left">version</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">tidyverse</td>
<td align="left">1.2.1</td>
</tr>
<tr class="even">
<td align="left">rpart</td>
<td align="left">4.1-15</td>
</tr>
<tr class="odd">
<td align="left">rpart.plot</td>
<td align="left">3.0.7</td>
</tr>
</tbody>
</table>
</div>
<div id="cart-build" class="section level2">
<h2><span class="header-section-number">8.3</span> CART 트리 생성</h2>
<div id="cart-basic-r-script" class="section level3">
<h3><span class="header-section-number">8.3.1</span> 기본 R 스크립트</h3>
<pre class="sourceCode r"><code class="sourceCode r">train_df &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">x1 =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">5</span>),
  <span class="dt">x2 =</span> <span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">6</span>,<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">6</span>,<span class="dv">6</span>,<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">3</span>),
  <span class="dt">class =</span> <span class="kw">as.factor</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>))
)</code></pre>
<table>
<caption><span id="tab:tree-train-data-table">Table 8.1: </span>학습표본 데이터</caption>
<thead>
<tr class="header">
<th align="right">x1</th>
<th align="right">x2</th>
<th align="right">class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">4</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">6</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">5</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">4</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">3</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">6</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">6</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">5</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">4</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">3</td>
<td align="right">2</td>
</tr>
</tbody>
</table>
<p>Table <a href="tree-based-method.html#tab:tree-train-data-table">8.1</a>와 같이 두 독립변수 <em>x1</em>, <em>x2</em>와 이분형 종속변수 <em>class</em>의 관측값으로 이루어진 10개의 학습표본을 <em>train_df</em>라는 data frame에 저장한다.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rpart)
<span class="kw">library</span>(rpart.plot)
cart.est &lt;-<span class="st"> </span><span class="kw">rpart</span>(
  class <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2
  , <span class="dt">data =</span> train_df
  , <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>
  , <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">&quot;gini&quot;</span>)
  , <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">minsplit =</span> <span class="dv">2</span>
                            , <span class="dt">minbucket =</span> <span class="dv">1</span>
                            , <span class="dt">cp =</span> <span class="dv">0</span>
                            , <span class="dt">xval =</span> <span class="dv">0</span>
                            , <span class="dt">maxcompete =</span> <span class="dv">0</span>)
  )
<span class="kw">rpart.plot</span>(cart.est)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:cart-basic"></span>
<img src="data-mining-book_files/figure-html/cart-basic-1.png" alt="CART 트리" width="672" />
<p class="caption">
Figure 8.1: CART 트리
</p>
</div>
<p><a href="https://cran.r-project.org/web/packages/rpart/">rpart</a> 라는 package를 기반으로, 두 변수 x1과 x2를 이용하여 이분형 종속변수 class를 분류하는 CART 트리를 생성할 수 있으며, <a href="https://cran.r-project.org/web/packages/rpart.plot/">rpart.plot</a> package를 이용하여 Figure <a href="tree-based-method.html#fig:cart-basic">8.1</a>과 같이 시각화할 수 있다.</p>
</div>
<div id="cart-notation" class="section level3">
<h3><span class="header-section-number">8.3.2</span> 기호 정의</h3>
<p>본 장에서 사용될 수학적 기호는 아래와 같다.</p>
<ul>
<li><span class="math inline">\(T\)</span>: 트리</li>
<li><span class="math inline">\(A(T)\)</span>: 트리 <span class="math inline">\(T\)</span>의 최종노드의 집합</li>
<li><span class="math inline">\(J\)</span>: 범주수</li>
<li><span class="math inline">\(N\)</span>: 학습표본의 총 객체수</li>
<li><span class="math inline">\(N_j\)</span>: 범주 <span class="math inline">\(j\)</span>에 속한 객체 수</li>
<li><span class="math inline">\(N(t)\)</span>: 노드 <span class="math inline">\(t\)</span>에서의 객체수</li>
<li><span class="math inline">\(N_j(t)\)</span>: 노드 <span class="math inline">\(t\)</span>에서 범주 <span class="math inline">\(j\)</span>에 속한 객체수</li>
<li><span class="math inline">\(p(j,t)\)</span>: 임의의 객체가 범주 <span class="math inline">\(j\)</span>와 노드 <span class="math inline">\(t\)</span>에 속할 확률</li>
<li><span class="math inline">\(p(t)\)</span>: 임의의 객체가 노드 <span class="math inline">\(t\)</span>에 속할 확률
<span class="math display">\[p(t) = \sum_{j=1}^{J} p(j,t)\]</span></li>
<li><span class="math inline">\(p(j|t)\)</span>: 임의의 객체가 노드 <span class="math inline">\(t\)</span>에 속할 때 범주 <span class="math inline">\(j\)</span>에 속할 조건부 확률
<span class="math display">\[p(j|t) = \frac{p(j,t)}{p(t)}, \quad \sum_{j=1}^{J} p(j|t) = 1\]</span></li>
</ul>
<p>이 때, 각 확률은 학습표본에서 아래와 같이 추정할 수 있다.
<span class="math display">\[\begin{align}
p(j,t) &amp;\approx \frac{N_j(t)}{N}\\
p(t) &amp;\approx \frac{N(t)}{N}\\
p(j|t) &amp;\approx \frac{N_j(t)}{N(t)}
\end{align}\]</span></p>
</div>
<div id="cart-impurity" class="section level3">
<h3><span class="header-section-number">8.3.3</span> 노드 및 트리의 불순도</h3>
<div id="-" class="section level4">
<h4><span class="header-section-number">8.3.3.1</span> 노드의 불순도</h4>
<p>CART는 지니 지수(Gini index)를 불순도 함수로 사용한다. 총 <span class="math inline">\(J\)</span>개의 범주별 객체비율을 <span class="math inline">\(p_1, \cdots , p_J\)</span>라 할 때 (<span class="math inline">\(\sum_{j=1}^{J} p_j = 1\)</span>), 지니 지수는 식 <a href="tree-based-method.html#eq:gini-index">(8.1)</a>와 같다.</p>
<p><span class="math display" id="eq:gini-index">\[\begin{equation}
G(p_1, \cdots, p_J) = \sum_{j=1}^{J} p_j(1-p_j) = 1 - \sum_{j=1}^{J}p_j^2 \tag{8.1}
\end{equation}\]</span></p>
<p>노드 <span class="math inline">\(t\)</span>에서의 범주별 객체비율은 <span class="math inline">\(p(1|t), \cdots, p(J|t)\)</span>이므로, 노드 <span class="math inline">\(t\)</span>의 불순도는 식 <a href="tree-based-method.html#eq:node-impurity">(8.2)</a>와 같이 산출된다.</p>
<p><span class="math display" id="eq:node-impurity">\[\begin{equation}
\begin{split}
i(t) &amp;= 1 - \sum_{j=1}^{J} p(j|t)^2\\
&amp;\approx 1 - \sum_{j=1}^{J} \left[\frac{N_j(t)}{N(t)}\right]^2
\end{split}
\tag{8.2}
\end{equation}\]</span></p>
</div>
<div id="-" class="section level4">
<h4><span class="header-section-number">8.3.3.2</span> 트리 불순도</h4>
<p>트리 <span class="math inline">\(T\)</span>의 불순도는 식 <a href="tree-based-method.html#eq:tree-impurity">(8.3)</a>와 같이 최종노드들의 불순도의 가중평균으로 정의된다.</p>
<p><span class="math display" id="eq:tree-impurity">\[\begin{equation}
I(T) = \sum_{t \in A(T)} i(t)p(t) \tag{8.3}
\end{equation}\]</span></p>
<p>여기서
<span class="math display">\[ I(t) = i(t)p(t) \]</span>
라 하면, 다음이 성립한다.
<span class="math display">\[ I(T) = \sum_{t \in A(T)} I(t) \]</span></p>
</div>
</div>
<div id="cart-split" class="section level3">
<h3><span class="header-section-number">8.3.4</span> 분지기준</h3>
<p>뿌리 노드에서의 분지만을 살펴보기 위해 control parameter <em>maxdepth</em>의 값을 1으로 설정한다. 이 경우, CART 알고리즘은 뿌리노드에서의 양 갈래 분지만을 선택한 뒤 종료된다. 아래 스크립트를 이용하여 뿌리노드에서 최적분지된 트리를 얻는다.</p>
<pre class="sourceCode r"><code class="sourceCode r">cart.firstsplit &lt;-<span class="st"> </span><span class="kw">rpart</span>(class <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2
                  , <span class="dt">data =</span> train_df
                  , <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>
                  , <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">&quot;gini&quot;</span>)
                  , <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">minsplit =</span> <span class="dv">2</span>
                                          , <span class="dt">minbucket =</span> <span class="dv">1</span>
                                          , <span class="dt">maxdepth =</span> <span class="dv">1</span>
                                          , <span class="dt">cp =</span> <span class="dv">0</span>
                                          , <span class="dt">xval =</span> <span class="dv">0</span>
                                          , <span class="dt">maxcompete =</span> <span class="dv">0</span>
                                          )
                  )
<span class="kw">rpart.plot</span>(cart.firstsplit)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:firstsplit"></span>
<img src="data-mining-book_files/figure-html/firstsplit-1.png" alt="뿌리노드 분지" width="672" />
<p class="caption">
Figure 8.2: 뿌리노드 분지
</p>
</div>
<p>또한 분지 결과 트리는 Table <a href="tree-based-method.html#tab:firstsplit-frame">8.2</a>와 같이 <em>frame</em>이라는 이름의 data frame에 설명된다. 각 행 앞의 번호는 노드 인덱스 <span class="math inline">\(t\)</span>를 나타내며, 각 열에 대한 설명은 아래와 같다.</p>
<ul>
<li>var: 노드 <span class="math inline">\(t\)</span>를 분지하는 데 이용된 변수. 값이 &lt;leaf&gt;인 경우에는 노드 <span class="math inline">\(t\)</span>가 최종 노드임을 나타낸다.</li>
<li>n: 노드 내 객체 수 <span class="math inline">\(N(t)\)</span></li>
<li>wt: 가중치 적용 후 객체 수 (추후 appendix에서 설명)</li>
<li>dev: 오분류 객체 수</li>
<li>yval: 노드 <span class="math inline">\(t\)</span>를 대표하는 범주</li>
<li>complexity: 노드 <span class="math inline">\(t\)</span>에서 추가로 분지할 때 감소하는 relative error값; 본 분류트리 예제에서 error는 오분류율이며, 뿌리 노드의 relative error값을 1으로 한다.</li>
</ul>
<table>
<caption><span id="tab:firstsplit-frame">Table 8.2: </span>뿌리노드 분지 상세 (frame)</caption>
<thead>
<tr class="header">
<th></th>
<th align="left">var</th>
<th align="right">n</th>
<th align="right">wt</th>
<th align="right">dev</th>
<th align="right">yval</th>
<th align="right">complexity</th>
<th align="right">ncompete</th>
<th align="right">nsurrogate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td align="left">x2</td>
<td align="right">10</td>
<td align="right">10</td>
<td align="right">5</td>
<td align="right">1</td>
<td align="right">0.6</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>2</td>
<td align="left">&lt;leaf&gt;</td>
<td align="right">3</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>3</td>
<td align="left">&lt;leaf&gt;</td>
<td align="right">7</td>
<td align="right">7</td>
<td align="right">2</td>
<td align="right">2</td>
<td align="right">0.0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>또한 <em>frame</em>에는 트리 내 각 노드에 속한 객체와 범주에 대한 정보를 나타내는 <em>yval2</em>라는 행렬이 Table <a href="tree-based-method.html#tab:firstsplit-yval2">8.3</a>와 같이 존재한다. 실제 <em>yval2</em>의 열의 개수는 전체 학습 대상 범주 수에 따라 달라지며, 본 예는 이분 분류 트리(범주개수 = 2)에 해당하는 열 구성을 보여준다. 각 행 앞의 번호는 노드 인덱스 <span class="math inline">\(t\)</span>를 나타내며, 각 열에 대한 설명은 아래와 같다.</p>
<ul>
<li>열1: 노드 <span class="math inline">\(t\)</span>에서의 최적 추정 범주 <span class="math inline">\(j^*\)</span></li>
<li>열2: 노드 <span class="math inline">\(t\)</span> 내 범주 <em>class</em>=1 객체 수 <span class="math inline">\(N_1(t)\)</span></li>
<li>열3: 노드 <span class="math inline">\(t\)</span> 내 범주 <em>class</em>=2 객체 수 <span class="math inline">\(N_2(t)\)</span></li>
<li>열4: 노드 <span class="math inline">\(t\)</span> 내 범주 <em>class</em>=1 관측 확률 <span class="math inline">\(p(1|t) \approx \tfrac{N_1(t)}{N(t)}\)</span></li>
<li>열5: 노드 <span class="math inline">\(t\)</span> 내 범주 <em>class</em>=2 관측 확률 <span class="math inline">\(p(2|t) \approx \tfrac{N_2(t)}{N(t)}\)</span></li>
<li>nodeprob: 노드 <span class="math inline">\(t\)</span> 확률 <span class="math inline">\(p(t) \approx \tfrac{N(t)}{N}\)</span></li>
</ul>
<pre><code>## Warning: Setting row names on a tibble is deprecated.</code></pre>
<table>
<caption><span id="tab:firstsplit-yval2">Table 8.3: </span>노드 내 객체 및 범주 정보 (yval2)</caption>
<thead>
<tr class="header">
<th></th>
<th align="center">열1</th>
<th align="center">열2</th>
<th align="center">열3</th>
<th align="center">열4</th>
<th align="center">열5</th>
<th align="right">nodeprob</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td align="center">1</td>
<td align="center">5</td>
<td align="center">5</td>
<td align="center">0.50</td>
<td align="center">0.50</td>
<td align="right">1.0</td>
</tr>
<tr class="even">
<td>2</td>
<td align="center">1</td>
<td align="center">3</td>
<td align="center">0</td>
<td align="center">1.00</td>
<td align="center">0.00</td>
<td align="right">0.3</td>
</tr>
<tr class="odd">
<td>3</td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center">5</td>
<td align="center">0.29</td>
<td align="center">0.71</td>
<td align="right">0.7</td>
</tr>
</tbody>
</table>
<p>위 CART 모델 데이터를 이용하여 트리의 불순도를 계산해보자.</p>
<p>우선 노드 상세 정보 행렬 <em>yval2</em>의 <em>x</em>번째 노드의 불순도(<span class="math inline">\(i(t)\)</span>)를 계산하는 함수 <em>rpartNodeImpurity</em>를 아래와 같이 구현한다.</p>
<pre class="sourceCode r"><code class="sourceCode r">rpartNodeImpurity &lt;-<span class="st"> </span><span class="cf">function</span>(x, yval2) {
  node_vec &lt;-<span class="st"> </span>yval2[x, ]
  n.columns &lt;-<span class="st"> </span><span class="kw">length</span>(node_vec)
  class.prob &lt;-<span class="st"> </span>node_vec[((n.columns<span class="op">/</span><span class="dv">2</span>)<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(n.columns<span class="dv">-1</span>)]
  <span class="kw">return</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(class.prob<span class="op">^</span><span class="dv">2</span>))
}</code></pre>
<p>CART tree 객체의 각 leaf node에 함수 <em>rpartNodeImpurity</em>를 적용하여 노드 불순도 <span class="math inline">\(i(t)\)</span>를 계산한 뒤, 노드 확률 <span class="math inline">\(p(t)\)</span>을 이용한 가중합을 통해 트리 불순도 <span class="math inline">\(I(T)\)</span>를 계산하는 함수 <em>rpartImpurity</em>를 아래와 같이 구현한다.</p>
<pre class="sourceCode r"><code class="sourceCode r">rpartImpurity &lt;-<span class="st"> </span><span class="cf">function</span>(rpart.obj) {
  leaf.nodes &lt;-<span class="st"> </span><span class="kw">which</span>(rpart.obj<span class="op">$</span>frame<span class="op">$</span>var<span class="op">==</span><span class="st">&quot;&lt;leaf&gt;&quot;</span>)
  node.impurity &lt;-<span class="st"> </span><span class="kw">sapply</span>(leaf.nodes, 
                          rpartNodeImpurity, 
                          <span class="dt">yval2 =</span> rpart.obj<span class="op">$</span>frame<span class="op">$</span>yval2)
  node.prob &lt;-<span class="st"> </span>rpart.obj<span class="op">$</span>frame<span class="op">$</span>yval2[leaf.nodes, <span class="st">&#39;nodeprob&#39;</span>]
  <span class="kw">return</span>(<span class="kw">sum</span>(node.prob <span class="op">*</span><span class="st"> </span>node.impurity))
}</code></pre>
<p>위 함수를 이용하여 계산한 트리 Figure <a href="tree-based-method.html#fig:cart-basic">8.1</a>의 불순도는 0.29이다.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rpartImpurity</span>(cart.firstsplit)</code></pre>
<pre><code>## [1] 0.2857143</code></pre>
<p>분지를 추가할수록 불순도는 감소한다. 분지를 추가하기 위해서는 <em>maxdepth</em>라는 control parameter 값을 증가시키면 된다.</p>
<ul>
<li>maxdepth: 뿌리노드부터 임의의 최종노드에 도달하는 최대 가능 분지 수 (default=30)</li>
</ul>
<p><em>maxdepth</em> 파라미터의 값을 1부터 4까지 증가시키며 불순도의 변화를 살펴보자.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)

tree.impurity &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>), <span class="cf">function</span>(depth) {
  <span class="kw">rpart</span>(class <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2
        , <span class="dt">data =</span> train_df
        , <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>
        , <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">&quot;gini&quot;</span>)
        , <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">minsplit =</span> <span class="dv">2</span>
                                  , <span class="dt">minbucket =</span> <span class="dv">1</span>
                                  , <span class="dt">maxdepth =</span> depth
                                  , <span class="dt">cp =</span> <span class="dv">0</span>
                                  , <span class="dt">xval =</span> <span class="dv">0</span>
                                  , <span class="dt">maxcompete =</span> <span class="dv">0</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">rpartImpurity</span>()
})

<span class="kw">tibble</span>(<span class="dt">maxdepth=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>), <span class="dt">impurity=</span>tree.impurity) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>maxdepth, <span class="dt">y=</span>impurity)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>()</code></pre>
<div class="figure" style="text-align: center"><span id="fig:impurity-trend"></span>
<img src="data-mining-book_files/figure-html/impurity-trend-1.png" alt="파라미터 maxdepth값에 따른 트리불순도 변화" width="672" />
<p class="caption">
Figure 8.3: 파라미터 maxdepth값에 따른 트리불순도 변화
</p>
</div>
<p>위 예에서, 트리의 분지가 증가함에 따라 불순도는 0.29, 0.17, 0.17, 0로 감소한다. <em>maxdepth</em>값이 3일 때 불순도가 감소하지 않는 이유는, 세 번째 분지 결과가 전체적인 오분류를 감소시키지 않아 <em>rpart</em> 함수가 해당 분지를 취소하기 때문이다. 여기에 작용하는 파라미터는 <em>cp</em>라는 control parameter이다.</p>
<ul>
<li>cp: 노드가 분지되기 위한 최소 relative error 감소치 (default = 0.01). 값이 0일 경우 최대트리를 생성한다.</li>
</ul>
<p>위 예제에서는 <em>cp</em>값을 0으로 설정하여, 해당 분지가 트리 불순도를 감소시킨다 하더라도 전체 트리의 오분류를 감소시키는 데 기여하지 않는다면 시도하지 않도록 하였다.</p>
</div>
</div>
<div id="cart-pruning-complete" class="section level2">
<h2><span class="header-section-number">8.4</span> 가지치기 및 최종 트리 선정</h2>
<div id="cart-pruning" class="section level3">
<h3><span class="header-section-number">8.4.1</span> 가지치기</h3>
<p>앞 장의 최대 트리 그림 <a href="tree-based-method.html#fig:cart-basic">8.1</a>은 학습 데이터를 오분류 없이 완벽하게 분류하기 위해 복잡한 분류 구조를 형성하였다. 이러한 복잡한 분류 구조는 학습 데이터가 아닌 새로운 데이터에 대한 분류 정확도를 떨어뜨릴 수 있다. 이는 bias-variance tradeoff라 부르는 현상으로, 비단 분류트리 뿐 아니라 모든 데이터마이닝 방법에 일반적으로 적용된다.</p>
<p>분류 트리는 가지치기라는 방식을 통해, 분류 구조를 단순화함으로써 분류 트리가 새로운 데이터에도 정확한 분류를 제공하기를 추구한다. 가지치기란 트리 내 특정 내부노드를 기준으로 그 하위에 발생한 분지를 모두 제거하고, 해당 내부노드를 최종노드로 치환하는 방식이다.</p>
<table>
<caption><span id="tab:max-frame">Table 8.4: </span>최대 트리 분지 상세 (frame)</caption>
<thead>
<tr class="header">
<th></th>
<th align="left">var</th>
<th align="right">n</th>
<th align="right">wt</th>
<th align="right">dev</th>
<th align="right">yval</th>
<th align="right">complexity</th>
<th align="right">ncompete</th>
<th align="right">nsurrogate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td align="left">x2</td>
<td align="right">10</td>
<td align="right">10</td>
<td align="right">5</td>
<td align="right">1</td>
<td align="right">0.6</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>2</td>
<td align="left">&lt;leaf&gt;</td>
<td align="right">3</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>3</td>
<td align="left">x1</td>
<td align="right">7</td>
<td align="right">7</td>
<td align="right">2</td>
<td align="right">2</td>
<td align="right">0.2</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>6</td>
<td align="left">&lt;leaf&gt;</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>7</td>
<td align="left">x2</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">1</td>
<td align="right">2</td>
<td align="right">0.1</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>14</td>
<td align="left">x1</td>
<td align="right">2</td>
<td align="right">2</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0.1</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>28</td>
<td align="left">&lt;leaf&gt;</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>29</td>
<td align="left">&lt;leaf&gt;</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0.0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>15</td>
<td align="left">&lt;leaf&gt;</td>
<td align="right">4</td>
<td align="right">4</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0.0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>Table <a href="tree-based-method.html#tab:max-frame">8.4</a>에서 생성 가능한 가지치기는 최종 노드(<em>var값이 &lt;leaf&gt;</em>)가 아닌 모든 노드(1, 3, 7, 14)에서 가능하며, 함수 <em>snip.rpart</em>를 이용하여 가지치기 된 트리를 생성할 수 있다. 각 내부 노드에서 가지치기된 트리들은 아래와 같이 얻어진다.</p>
<pre class="sourceCode r"><code class="sourceCode r">internal.node.index &lt;-<span class="st"> </span><span class="kw">rownames</span>(cart.est<span class="op">$</span>frame)[<span class="kw">which</span>(cart.est<span class="op">$</span>frame<span class="op">$</span>var <span class="op">!=</span><span class="st"> &#39;&lt;leaf&gt;&#39;</span>)] <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as.numeric</span>()
snipped &lt;-<span class="st"> </span><span class="kw">lapply</span>(internal.node.index, <span class="cf">function</span>(x){<span class="kw">snip.rpart</span>(cart.est, x)})
n.trees &lt;-<span class="st"> </span><span class="kw">length</span>(snipped)
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">invisible</span>(<span class="kw">lapply</span>(<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span>n.trees), <span class="cf">function</span>(x) {
  <span class="kw">rpart.plot</span>(snipped[[x]])}
  ))</code></pre>
<div class="figure" style="text-align: center"><span id="fig:snipped"></span>
<img src="data-mining-book_files/figure-html/snipped-1.png" alt="각 내부노드 기준으로 가지치기된 트리" width="672" />
<p class="caption">
Figure 8.4: 각 내부노드 기준으로 가지치기된 트리
</p>
</div>
<p>위 각 가지치기 후보 노드의 오분류 비용은 함수 <em>nodeCost</em>를 아래와 같이 구현하여 계산할 수 있다.</p>
<pre class="sourceCode r"><code class="sourceCode r">nodeCost &lt;-<span class="st"> </span><span class="cf">function</span>(node, tree) {
  node_vec &lt;-<span class="st"> </span>tree<span class="op">$</span>frame<span class="op">$</span>yval2[<span class="kw">as.character</span>(node) <span class="op">==</span><span class="st"> </span><span class="kw">row.names</span>(tree<span class="op">$</span>frame), ]
  n.columns &lt;-<span class="st"> </span><span class="kw">length</span>(node_vec)
  class.prob.max &lt;-<span class="st"> </span><span class="kw">max</span>(node_vec[((n.columns<span class="op">/</span><span class="dv">2</span>)<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(n.columns<span class="dv">-1</span>)])
  node.prob &lt;-<span class="st"> </span>node_vec[n.columns]
  node.misclassification.cost &lt;-<span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>class.prob.max)<span class="op">*</span>node.prob
  <span class="kw">return</span>(node.misclassification.cost)
}

<span class="kw">tibble</span>(
  <span class="dt">pruning_node =</span> internal.node.index,
  <span class="dt">node_cost =</span> <span class="kw">sapply</span>(internal.node.index, nodeCost, <span class="dt">tree=</span>cart.est)
) <span class="op">%&gt;%</span>
<span class="st">  </span>knitr<span class="op">::</span><span class="kw">kable</span>()</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">pruning_node</th>
<th align="right">node_cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0.5</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.2</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">0.1</td>
</tr>
<tr class="even">
<td align="right">14</td>
<td align="right">0.1</td>
</tr>
</tbody>
</table>
<p>각 가지치기 노드에 해당하는 하부 트리의 오분류비용 및 복잡도를 구하기 위해 <em>subtreeEval</em>라는 함수를 아래와 같이 구현한다.</p>
<pre class="sourceCode r"><code class="sourceCode r">subtreeEval &lt;-<span class="st"> </span><span class="cf">function</span>(node, tree) {
  snipped &lt;-<span class="st"> </span><span class="kw">snip.rpart</span>(tree, node)<span class="op">$</span>frame
  leaf.nodes &lt;-<span class="st"> </span><span class="kw">setdiff</span>(<span class="kw">rownames</span>(tree<span class="op">$</span>frame[tree<span class="op">$</span>frame<span class="op">$</span>var<span class="op">==</span><span class="st">&quot;&lt;leaf&gt;&quot;</span>,]),
          <span class="kw">rownames</span>(snipped)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">as.numeric</span>()

  <span class="kw">tibble</span>(
    <span class="dt">pruning_node =</span> node,
    <span class="dt">node.cost =</span> <span class="kw">nodeCost</span>(node, tree),
    <span class="dt">subtree.cost =</span> <span class="kw">sapply</span>(leaf.nodes, nodeCost, <span class="dt">tree=</span>tree) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sum</span>(),
    <span class="dt">subtree.size =</span> <span class="kw">length</span>(leaf.nodes)
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">alpha =</span> (node.cost <span class="op">-</span><span class="st"> </span>subtree.cost) <span class="op">/</span><span class="st"> </span>(subtree.size <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))
}</code></pre>
<p>각 노드에 대하여 알파값을 다음과 같이 계산할 수 있다.</p>
<pre class="sourceCode r"><code class="sourceCode r">df.cost &lt;-<span class="st"> </span><span class="kw">lapply</span>(internal.node.index, subtreeEval, <span class="dt">tree=</span>cart.est) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_rows</span>()</code></pre>
<table>
<caption><span id="tab:first-prune-candidate-tab">Table 8.5: </span>내부노드 가지치기 평가 (df.cost)</caption>
<thead>
<tr class="header">
<th align="right">pruning_node</th>
<th align="right">node.cost</th>
<th align="right">subtree.cost</th>
<th align="right">subtree.size</th>
<th align="right">alpha</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0.5</td>
<td align="right">0</td>
<td align="right">5</td>
<td align="right">0.12</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.2</td>
<td align="right">0</td>
<td align="right">4</td>
<td align="right">0.07</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">0.1</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">0.05</td>
</tr>
<tr class="even">
<td align="right">14</td>
<td align="right">0.1</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0.10</td>
</tr>
</tbody>
</table>
<p>위 Table <a href="tree-based-method.html#tab:first-prune-candidate-tab">8.5</a> 에서 최소 알파값에 해당하는 노드 7에서 가지치기를 한다.</p>
<pre class="sourceCode r"><code class="sourceCode r">pruned.tree<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">snip.rpart</span>(cart.est,
                            df.cost<span class="op">$</span>pruning_node[<span class="kw">which.min</span>(df.cost<span class="op">$</span>alpha)])
<span class="kw">rpart.plot</span>(pruned.tree<span class="fl">.1</span>)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:first-prune-result"></span>
<img src="data-mining-book_files/figure-html/first-prune-result-1.png" alt="1단계 가지치기 결과" width="672" />
<p class="caption">
Figure 8.5: 1단계 가지치기 결과
</p>
</div>
<p>가지치기로 형성된 트리에서 다시 각 가지치기 노드의 오분류비용, 복잡도 및 알파값을 구한다.</p>
<pre class="sourceCode r"><code class="sourceCode r">df.cost &lt;-<span class="st"> </span><span class="kw">rownames</span>(pruned.tree<span class="fl">.1</span><span class="op">$</span>frame)[pruned.tree<span class="fl">.1</span><span class="op">$</span>frame<span class="op">$</span>var<span class="op">!=</span><span class="st">&quot;&lt;leaf&gt;&quot;</span>] <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">as.numeric</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">lapply</span>(subtreeEval, <span class="dt">tree=</span>pruned.tree<span class="fl">.1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_rows</span>()

knitr<span class="op">::</span><span class="kw">kable</span>(df.cost)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">pruning_node</th>
<th align="right">node.cost</th>
<th align="right">subtree.cost</th>
<th align="right">subtree.size</th>
<th align="right">alpha</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0.5</td>
<td align="right">0.1</td>
<td align="right">3</td>
<td align="right">0.2</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.2</td>
<td align="right">0.1</td>
<td align="right">2</td>
<td align="right">0.1</td>
</tr>
</tbody>
</table>
<p>위 결과에서 다시 최소 알파값에 해당하는 노드 3에서 가지치기를 하면 아래와 같은 트리가 형성된다.</p>
<pre class="sourceCode r"><code class="sourceCode r">pruned.tree<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">snip.rpart</span>(pruned.tree<span class="fl">.1</span>,
                            df.cost<span class="op">$</span>pruning_node[<span class="kw">which.min</span>(df.cost<span class="op">$</span>alpha)])
<span class="kw">rpart.plot</span>(pruned.tree<span class="fl">.2</span>)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:second-prune-result"></span>
<img src="data-mining-book_files/figure-html/second-prune-result-1.png" alt="2단계 가지치기 결과" width="672" />
<p class="caption">
Figure 8.6: 2단계 가지치기 결과
</p>
</div>
</div>
<div id="cart-best-tree" class="section level3">
<h3><span class="header-section-number">8.4.2</span> 최적 트리의 선정</h3>
<p>위 가지치기 과정에서 얻는 가지친 트리들이 최종 트리의 후보가 되며, 이 중 테스트 표본에 대한 오분류율이 가장 작은 트리를 최적 트리로 선정하게 된다.</p>
<p>트리를 학습할 때 사용된 학습데이터 Table <a href="tree-based-method.html#tab:tree-train-data-table">8.1</a> 외에, Table <a href="tree-based-method.html#tab:tree-test-data-table">8.6</a>과 같은 6개의 테스트 데이터가 있다고 하자.</p>
<pre class="sourceCode r"><code class="sourceCode r">test_df &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">x1 =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">1</span>),
  <span class="dt">x2 =</span> <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">7</span>,<span class="dv">4</span>),
  <span class="dt">class =</span> <span class="kw">factor</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">2</span>), <span class="dt">levels=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
)</code></pre>
<table>
<caption><span id="tab:tree-test-data-table">Table 8.6: </span>테스트 데이터</caption>
<thead>
<tr class="header">
<th align="right">x1</th>
<th align="right">x2</th>
<th align="right">class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">5</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">0</td>
<td align="right">5</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">4</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">3</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">7</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">4</td>
<td align="right">2</td>
</tr>
</tbody>
</table>
<p>테스트 데이터에 위에서 학습된 세 개의 트리, 즉 최대 트리 <em>cart.est</em>와 두 개의 가지치기 트리 <em>pruned.tree.1</em> &amp; <em>pruned.tree.2</em>를 적용하여 각 트리가 각각의 테스트 데이터를 어떻게 분류하는지 살펴보자.</p>
<pre class="sourceCode r"><code class="sourceCode r">test_pred &lt;-<span class="st"> </span>test_df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(
    <span class="dt">pred_maxtree =</span> <span class="kw">predict</span>(cart.est, test_df, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>),
    <span class="dt">pred_prune1 =</span> <span class="kw">predict</span>(pruned.tree<span class="fl">.1</span>, test_df, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>),
    <span class="dt">pred_prune2 =</span> <span class="kw">predict</span>(pruned.tree<span class="fl">.2</span>, test_df, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)
  )</code></pre>
<table>
<caption><span id="tab:tree-class-prediction-table">Table 8.7: </span>테스트 데이터에 대한 예측 결과</caption>
<thead>
<tr class="header">
<th align="right">x1</th>
<th align="right">x2</th>
<th align="left">class</th>
<th align="left">pred_maxtree</th>
<th align="left">pred_prune1</th>
<th align="left">pred_prune2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">5</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">2</td>
</tr>
<tr class="even">
<td align="right">0</td>
<td align="right">5</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">4</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">3</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">7</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">4</td>
<td align="left">2</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">2</td>
</tr>
</tbody>
</table>
<p>결과 Table <a href="tree-based-method.html#tab:tree-class-prediction-table">8.7</a>에서 최대트리가 오분류한 테스트 표본은 1개, 첫번째 가지치기 트리가 오분류한 테스트 표본은 1개, 그리고 두 번째 가지치기 트리가 오분류한 테스트 표본은 2개이다.</p>
<p>위 결과를 토대로, 최적의 트리를 선정하는 과정은 아래와 같다.</p>
<ol style="list-style-type: decimal">
<li>각각의 트리에 의해 오분류된 테스트 표본의 개수를 전체 테스트 표본의 개수로 나누어 오분류율 <span class="math inline">\(R^{ts}\)</span>를 구한다.</li>
<li>테스트 표본 수를 <span class="math inline">\(n_{test}\)</span>라 할 때, 오분류의 표준편차를 아래와 같이 계산한다.
<span class="math display">\[SE = \sqrt{\frac{R^{ts}(1 - R^{ts})}{n_{test}}}\]</span></li>
<li>1에서 구한 오분류율에 2에서 구한 표준편차를 더하여 <span class="math inline">\(R^{ts} + SE\)</span>를 각 트리의 평가척도로 계산한다. 후보 트리들 중 해당 평가척도가 가장 작은 트리를 최종 트리로 선정한다.</li>
</ol>
<pre class="sourceCode r"><code class="sourceCode r">test.summary &lt;-<span class="st"> </span>test_pred <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">n.test =</span> <span class="kw">n</span>(),
            <span class="dt">cart.est =</span> <span class="kw">sum</span>(pred_maxtree <span class="op">!=</span><span class="st"> </span>class) <span class="op">/</span><span class="st"> </span>n.test,
            <span class="dt">pruned.tree.1 =</span> <span class="kw">sum</span>(pred_prune1 <span class="op">!=</span><span class="st"> </span>class) <span class="op">/</span><span class="st"> </span>n.test,
            <span class="dt">pruned.tree.2 =</span> <span class="kw">sum</span>(pred_prune2 <span class="op">!=</span><span class="st"> </span>class) <span class="op">/</span><span class="st"> </span>n.test) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(<span class="st">&quot;tree&quot;</span>,<span class="st">&quot;R.ts&quot;</span>,<span class="op">-</span>n.test) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">SE =</span> <span class="kw">sqrt</span>((R.ts<span class="op">*</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>R.ts))<span class="op">/</span>n.test),
         <span class="dt">score =</span> R.ts <span class="op">+</span><span class="st"> </span>SE) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>n.test)</code></pre>
<table>
<caption><span id="tab:misclassification-rate-table">Table 8.8: </span>분류 성능</caption>
<thead>
<tr class="header">
<th align="left">트리</th>
<th align="center">오분류율(<span class="math inline">\(R^{ts}\)</span>)</th>
<th align="center">표준편차(<span class="math inline">\(SE\)</span>)</th>
<th align="center">척도(<span class="math inline">\(R^{ts} + SE\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">cart.est</td>
<td align="center">0.17</td>
<td align="center">0.15</td>
<td align="center">0.32</td>
</tr>
<tr class="even">
<td align="left">pruned.tree.1</td>
<td align="center">0.17</td>
<td align="center">0.15</td>
<td align="center">0.32</td>
</tr>
<tr class="odd">
<td align="left">pruned.tree.2</td>
<td align="center">0.33</td>
<td align="center">0.19</td>
<td align="center">0.53</td>
</tr>
</tbody>
</table>
<p>위 결과, 최적 트리는 최대 트리 혹은 첫 번째 가지치기 트리가 된다.</p>
<p>위 절차를 임의의 데이터에 대해 수행하는 함수를 구현해보자.</p>
<pre class="sourceCode r"><code class="sourceCode r">rpart_learn &lt;-<span class="st"> </span><span class="cf">function</span>(formula, train_df, test_df) {
  <span class="co"># 최대 트리 생성</span>
  max_tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(formula
                    , <span class="dt">data =</span> train_df
                    , <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>
                    , <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">&quot;gini&quot;</span>)
                    , <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">minsplit =</span> <span class="dv">2</span>
                                              , <span class="dt">minbucket =</span> <span class="dv">1</span>
                                              , <span class="dt">cp =</span> <span class="dv">0</span>
                                              , <span class="dt">xval =</span> <span class="dv">0</span>
                                              , <span class="dt">maxcompete =</span> <span class="dv">0</span>
                    )
  )
  
  <span class="co"># 가지치기</span>
  curr_tree &lt;-<span class="st"> </span><span class="kw">list</span>()
  k &lt;-<span class="st"> </span><span class="dv">1</span>
  curr_tree[[k]] &lt;-<span class="st"> </span>max_tree
  <span class="cf">while</span>(<span class="kw">dim</span>(curr_tree[[k]]<span class="op">$</span>frame)[<span class="dv">1</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) {
    internal.node.index &lt;-<span class="st"> </span><span class="kw">rownames</span>(curr_tree[[k]]<span class="op">$</span>frame)[<span class="kw">which</span>(curr_tree[[k]]<span class="op">$</span>frame<span class="op">$</span>var <span class="op">!=</span><span class="st"> &#39;&lt;leaf&gt;&#39;</span>)] <span class="op">%&gt;%</span>
<span class="st">      </span><span class="kw">as.numeric</span>()
    df.cost &lt;-<span class="st"> </span><span class="kw">lapply</span>(internal.node.index, subtreeEval, <span class="dt">tree=</span>curr_tree[[k]]) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_rows</span>()
    curr_tree[[k <span class="op">+</span><span class="st"> </span><span class="dv">1</span>]] &lt;-<span class="st"> </span><span class="kw">snip.rpart</span>(curr_tree[[k]],
               df.cost<span class="op">$</span>pruning_node[<span class="kw">which.min</span>(df.cost<span class="op">$</span>alpha)])
    k &lt;-<span class="st"> </span>k <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
  }

  <span class="co"># 최적 가지치기 트리 선정</span>
  n.test &lt;-<span class="st"> </span><span class="kw">dim</span>(test_df)[<span class="dv">1</span>]
  R.ts &lt;-<span class="st"> </span><span class="kw">lapply</span>(curr_tree, <span class="cf">function</span>(x) {
    <span class="kw">sum</span>(<span class="kw">predict</span>(x, test_df, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>) <span class="op">!=</span><span class="st"> </span>test_df<span class="op">$</span>class) <span class="op">/</span><span class="st"> </span>n.test
    }) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unlist</span>()
  score &lt;-<span class="st"> </span>R.ts <span class="op">+</span><span class="st"> </span><span class="kw">sqrt</span>((R.ts<span class="op">*</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>R.ts))<span class="op">/</span>n.test)
  <span class="kw">return</span>(curr_tree[[<span class="kw">max</span>(<span class="kw">which</span>(score <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(score)))]])
}

optimal_tree &lt;-<span class="st"> </span><span class="kw">rpart_learn</span>(class <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, train_df, test_df)
<span class="kw">rpart.plot</span>(optimal_tree)</code></pre>
<p><img src="data-mining-book_files/figure-html/rpart-learn-1.png" width="672" /></p>
</div>
</div>
<div id="cart-r-pkg" class="section level2">
<h2><span class="header-section-number">8.5</span> R패키지 내 분류 트리 방법</h2>
<p>앞 장에서는 <em>rpart</em>의 결과를 이용하여 교재 8.2 - 8.3장의 예제를 재현해보았다. 실제로 <em>rpart</em> 내부의 기본 트리 방법은 교재의 예제와는 다소 다른 부분이 있다. 이 장에서는 실제 <em>rpart</em> 패키지의 분류 트리 방법에 대해 알아본다.</p>
<div id="cart-r-pkg-split" class="section level3">
<h3><span class="header-section-number">8.5.1</span> 트리 확장</h3>
<p>트리 내 임의의 노드 <span class="math inline">\(t\)</span>에 대한 불순도는 아래와 같이 정의된다.
<span class="math display">\[i(t) = \sum_{j=1}^{J} f\left(p(j|t)\right)\]</span>
여기에서 <span class="math inline">\(p(j|t)\)</span>는 노드 <span class="math inline">\(t\)</span> 내 전체 샘플 <span class="math inline">\(N(t)\)</span> 중 범주 <span class="math inline">\(j\)</span>의 샘플 <span class="math inline">\(N_j(t)\)</span>의 비율로 추정된다.
<span class="math display">\[p(j|t) \approx \frac{N_j(t)}{N(t)}\]</span>
또한 함수 <span class="math inline">\(f\)</span>는 concave 함수로, <span class="math inline">\(f(0) = f(1) = 0\)</span>의 조건을 만족시켜야 한다. <em>rpart</em> 에서 설정할 수 있는 함수 <span class="math inline">\(f\)</span>의 종류에 대해서는 아래에서 좀 더 자세히 살펴보기로 한다.</p>
<p>트리 내 임의의 노드 <span class="math inline">\(t\)</span>가 분지규칙 <span class="math inline">\(s\)</span>에 따라 두 개의 노드 <span class="math inline">\(t_L\)</span>과 <span class="math inline">\(t_R\)</span>로 분지된다고 할 때, 불순도의 감소량은 아래와 같이 계산된다.</p>
<p><span class="math display">\[\begin{eqnarray}
\Delta I(s,t) &amp;=&amp; I(t) - I(t_L) - I(t_R)\\ &amp;=&amp; p(t)i(t) - p(t_L)i(t_L) - p(t_R)i(t_R) 
\end{eqnarray}\]</span></p>
<p><em>rpart</em>는 위 <span class="math inline">\(\Delta I(s,t)\)</span>값이 최대가 되는 분지 기준 <span class="math inline">\(s^*\)</span>를 찾아 노드 <span class="math inline">\(t\)</span>를 분지하여 트리를 확장하고, 확장된 트리의 최종 노드에서 다시 최적 분지를 찾는 과정을 반복한다.</p>
<div id="-" class="section level4">
<h4><span class="header-section-number">8.5.1.1</span> 분지 함수</h4>
<p>함수 <em>rpart</em> 사용 시 <em>parms</em> 파라미터에 <em>split</em> 값으로 분지 방법을 설정할 수 있다.</p>
<ol style="list-style-type: decimal">
<li>Gini index (parms=list(split=‘gini’))
교재의 예제에 사용된 방법으로, 우선 아래와 같은 함수 <span class="math inline">\(f\)</span>를 사용한다.
<span class="math display">\[f(p) = p(1-p)\]</span></li>
<li>information index (parms=list(split=‘information’))
교재에 엔트로피 지수(Entropy index)로 설명된 지수로, 아래와 같은 함수를 사용한다.
<span class="math display">\[f(p) = -p\log(p)\]</span></li>
<li>user-defined function
사용자가 임의로 함수를 정의하여 사용할 수 있다. 본 장에서는 자세한 설명은 생략한다.</li>
</ol>
</div>
</div>
<div id="cart-r-pkg-pruning" class="section level3">
<h3><span class="header-section-number">8.5.2</span> 가지치기</h3>
<p>임의의 노드 <span class="math inline">\(t\)</span>에 대한 위험도(오분류 비용의 기대치)는 아래와 같이 계산된다.
<span class="math display">\[r(t) = \sum_{j \neq \tau(t)} p(j|t)C\left(\tau(t)|j\right)\]</span>
여기에서 함수 <span class="math inline">\(C(i|j)\)</span>는 범주 <span class="math inline">\(j\)</span>에 속하는 객체를 범주 <span class="math inline">\(i\)</span>로 분류할 때의 오분류 비용이며, <span class="math inline">\(\tau(t)\)</span>는 노드 <span class="math inline">\(t\)</span> 내의 오분류 비용을 최소화하도록 노드 <span class="math inline">\(t\)</span>에 지정된 범주값이다.</p>
<p><em>rpart</em>의 오분류 비용 <span class="math inline">\(C(i|j)\)</span>의 기본값은
<span class="math display">\[C(i|j) = 
\begin{cases} 1,  &amp; \text{  if } i \neq j\\
              0,  &amp; \text{  if } i = j
\end{cases} \]</span>
으로 설정되어 있으며, <em>parms</em> 파라미터에 <em>loss</em> 값으로 오분류 비용 <span class="math inline">\(C(i|j)\)</span>를 재설정할 수 있다. 본 장에서는 기본값을 사용하도록 하자.</p>
<p><span class="math inline">\(A(T)\)</span>를 트리 <span class="math inline">\(T\)</span>의 최종 노드의 집합이라 정의하고, 트리의 최종 노드의 개수를 <span class="math inline">\(|T|\)</span>라 할 때, 트리 <span class="math inline">\(T\)</span>의 위험도 <span class="math inline">\(R(T)\)</span>는 아래와 같이 정의된다.
<span class="math display">\[R(T) = \sum_{t \in A(T)} p(t)r(t)\]</span></p>
<p>복잡도 계수(complexity parameter) <span class="math inline">\(\alpha \in [0, \infty)\)</span>를 이용하여, 트리의 비용-복합도 척도를 다음과 같이 정의한다.
<span class="math display">\[R_\alpha(T) = R(T) + \alpha|T|\]</span>
이 때, 임의의 계수 <span class="math inline">\(\alpha\)</span>에 대해 비용 <span class="math inline">\(R_\alpha(T)\)</span>가 최소가 되게하는 가지치기 트리를 <span class="math inline">\(T_\alpha\)</span>라 하면, 아래와 같은 관계들이 성립한다.</p>
<ul>
<li><span class="math inline">\(T_0\)</span>: 최대 트리</li>
<li><span class="math inline">\(T_\infty\)</span>: 뿌리 노드 트리 (분지 없음)</li>
<li><span class="math inline">\(\alpha &gt; \beta\)</span>일 때, <span class="math inline">\(T_\alpha\)</span>는 <span class="math inline">\(T_\beta\)</span>와 동일하거나 혹은 <span class="math inline">\(T_\beta\)</span>에서 가지치기된 트리이다.</li>
</ul>
</div>
<div id="cart-r-pkg-param" class="section level3">
<h3><span class="header-section-number">8.5.3</span> 파라미터값 결정</h3>
<p>함수 <em>rpart</em>를 사용할 때 여러가지 사용자 정의 파라미터값을 설정할 수 있으며, 그 파라미터 값에 따라 생성되는 트리의 결과가 달라진다. 대표적인 파라미터 값으로는 아래와 같은 것들이 있다.</p>
<ul>
<li>minsplit: 분지를 시도하기 위해 필요한 노드 내 최소 관측객체 수 (default=20)</li>
<li>cp: 노드가 분지되기 위한 최소 relative error 감소치 (default = 0.01). 값이 0일 경우 최대트리를 생성한다.</li>
<li>maxdepth: 뿌리노드부터 임의의 최종노드에 도달하는 최대 가능 분지 수 (default=30)</li>
</ul>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-breiman1984classification">
<p>Breiman, Leo, Jerome Friedman, Charles J Stone, and Richard A Olshen. 1984. <em>Classification and Regression Trees</em>. CRC press.</p>
</div>
</div>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://r-data-mining-book.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="da.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="svm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["data-mining-book.pdf"],
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
