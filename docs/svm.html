<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 4 서포트 벡터 머신 | 데이터마이닝 with R</title>
  <meta name="description" content="전치혁 교수님의 책 을 기반으로 한 R 예제">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 4 서포트 벡터 머신 | 데이터마이닝 with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://youngroklee-ml.github.io/data-mining-book/" />
  
  <meta property="og:description" content="전치혁 교수님의 책 을 기반으로 한 R 예제" />
  <meta name="github-repo" content="youngroklee-ml/data-mining-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 서포트 벡터 머신 | 데이터마이닝 with R" />
  
  <meta name="twitter:description" content="전치혁 교수님의 책 을 기반으로 한 R 예제" />
  

<meta name="author" content="전치혁, 이혜선, 이종석, 이영록">


<meta name="date" content="2019-02-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="tree-based-method.html">
<link rel="next" href="clustering-overview.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">데이터마이닝 with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>개요</a></li>
<li class="chapter" data-level="1" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>1</b> 로지스틱 회귀분석</a><ul>
<li class="chapter" data-level="1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-packages-install"><i class="fa fa-check"></i><b>1.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#binary-logistic-regression"><i class="fa fa-check"></i><b>1.2</b> 이분 로지스틱 회귀모형</a><ul>
<li class="chapter" data-level="1.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#bianry-logistic-reg-basic-script"><i class="fa fa-check"></i><b>1.2.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="1.2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#binary-logistic-reg-model"><i class="fa fa-check"></i><b>1.2.2</b> 회귀모형</a></li>
<li class="chapter" data-level="1.2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#binary-logistic-reg-estimation"><i class="fa fa-check"></i><b>1.2.3</b> 회귀계수 추정</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="logistic-regression.html"><a href="logistic-regression.html#nominal-logistic-regression"><i class="fa fa-check"></i><b>1.3</b> 명목 로지스틱 회귀모형</a><ul>
<li class="chapter" data-level="1.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#nominal-logistic-reg-basic-script"><i class="fa fa-check"></i><b>1.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="1.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#baseline-category-logit-model"><i class="fa fa-check"></i><b>1.3.2</b> 기준범주 로짓모형</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="logistic-regression.html"><a href="logistic-regression.html#ordinal-logistic-regression"><i class="fa fa-check"></i><b>1.4</b> 서열 로지스틱 회귀모형</a><ul>
<li class="chapter" data-level="1.4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#ordinal-logistic-basic-script"><i class="fa fa-check"></i><b>1.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="1.4.2" data-path="logistic-regression.html"><a href="logistic-regression.html#cumulative-logit-model"><i class="fa fa-check"></i><b>1.4.2</b> 누적 로짓모형</a></li>
<li class="chapter" data-level="1.4.3" data-path="logistic-regression.html"><a href="logistic-regression.html#adjacent-categories-logit-model"><i class="fa fa-check"></i><b>1.4.3</b> 인근범주 로짓모형</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="da.html"><a href="da.html"><i class="fa fa-check"></i><b>2</b> 판별분석</a><ul>
<li class="chapter" data-level="2.1" data-path="da.html"><a href="da.html#da-overview"><i class="fa fa-check"></i><b>2.1</b> 개요</a></li>
<li class="chapter" data-level="2.2" data-path="da.html"><a href="da.html#da-packages-install"><i class="fa fa-check"></i><b>2.2</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="2.3" data-path="da.html"><a href="da.html#da-fisher"><i class="fa fa-check"></i><b>2.3</b> 피셔 방법</a><ul>
<li class="chapter" data-level="2.3.1" data-path="da.html"><a href="da.html#da-fisher-basic-script"><i class="fa fa-check"></i><b>2.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="2.3.2" data-path="da.html"><a href="da.html#-"><i class="fa fa-check"></i><b>2.3.2</b> 피셔 판별함수</a></li>
<li class="chapter" data-level="2.3.3" data-path="da.html"><a href="da.html#-"><i class="fa fa-check"></i><b>2.3.3</b> 분류 규칙</a></li>
<li class="chapter" data-level="2.3.4" data-path="da.html"><a href="da.html#r----"><i class="fa fa-check"></i><b>2.3.4</b> R 패키지를 이용한 분류규칙 도출</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="da.html"><a href="da.html#lda"><i class="fa fa-check"></i><b>2.4</b> 의사결정론에 의한 선형분류규칙</a><ul>
<li class="chapter" data-level="2.4.1" data-path="da.html"><a href="da.html#lda-basic-script"><i class="fa fa-check"></i><b>2.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="2.4.2" data-path="da.html"><a href="da.html#lda-function"><i class="fa fa-check"></i><b>2.4.2</b> 선형판별함수</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="da.html"><a href="da.html#lda-misclassification-cost"><i class="fa fa-check"></i><b>2.5</b> 오분류비용을 고려한 분류규칙</a></li>
<li class="chapter" data-level="2.6" data-path="da.html"><a href="da.html#qda"><i class="fa fa-check"></i><b>2.6</b> 이차판별분석</a><ul>
<li class="chapter" data-level="2.6.1" data-path="da.html"><a href="da.html#qda-basic-script"><i class="fa fa-check"></i><b>2.6.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="2.6.2" data-path="da.html"><a href="da.html#qda-function"><i class="fa fa-check"></i><b>2.6.2</b> 이차 판별함수</a></li>
<li class="chapter" data-level="2.6.3" data-path="da.html"><a href="da.html#qda-discriminant-rule"><i class="fa fa-check"></i><b>2.6.3</b> 이차판별함수에 의한 분류</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="da.html"><a href="da.html#da-multiclass"><i class="fa fa-check"></i><b>2.7</b> 세 범주 이상의 분류</a><ul>
<li class="chapter" data-level="2.7.1" data-path="da.html"><a href="da.html#mutliclass-da-basic-script"><i class="fa fa-check"></i><b>2.7.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="2.7.2" data-path="da.html"><a href="da.html#mutliclass-generalized-discriminant-function"><i class="fa fa-check"></i><b>2.7.2</b> 일반화된 판별함수</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tree-based-method.html"><a href="tree-based-method.html"><i class="fa fa-check"></i><b>3</b> 트리기반 기법</a><ul>
<li class="chapter" data-level="3.1" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-overview"><i class="fa fa-check"></i><b>3.1</b> CART 개요</a></li>
<li class="chapter" data-level="3.2" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-packages-install"><i class="fa fa-check"></i><b>3.2</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="3.3" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-build"><i class="fa fa-check"></i><b>3.3</b> CART 트리 생성</a><ul>
<li class="chapter" data-level="3.3.1" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-basic-r-script"><i class="fa fa-check"></i><b>3.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="3.3.2" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-notation"><i class="fa fa-check"></i><b>3.3.2</b> 기호 정의</a></li>
<li class="chapter" data-level="3.3.3" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-impurity"><i class="fa fa-check"></i><b>3.3.3</b> 노드 및 트리의 불순도</a></li>
<li class="chapter" data-level="3.3.4" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-split"><i class="fa fa-check"></i><b>3.3.4</b> 분지기준</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-pruning-complete"><i class="fa fa-check"></i><b>3.4</b> 가지치기 및 최종 트리 선정</a><ul>
<li class="chapter" data-level="3.4.1" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-pruning"><i class="fa fa-check"></i><b>3.4.1</b> 가지치기</a></li>
<li class="chapter" data-level="3.4.2" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-best-tree"><i class="fa fa-check"></i><b>3.4.2</b> 최적 트리의 선정</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-r-pkg"><i class="fa fa-check"></i><b>3.5</b> R패키지 내 분류 트리 방법</a><ul>
<li class="chapter" data-level="3.5.1" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-r-pkg-split"><i class="fa fa-check"></i><b>3.5.1</b> 트리 확장</a></li>
<li class="chapter" data-level="3.5.2" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-r-pkg-pruning"><i class="fa fa-check"></i><b>3.5.2</b> 가지치기</a></li>
<li class="chapter" data-level="3.5.3" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-r-pkg-param"><i class="fa fa-check"></i><b>3.5.3</b> 파라미터값 결정</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>4</b> 서포트 벡터 머신</a><ul>
<li class="chapter" data-level="4.1" data-path="svm.html"><a href="svm.html#svm-overview"><i class="fa fa-check"></i><b>4.1</b> 개요</a></li>
<li class="chapter" data-level="4.2" data-path="svm.html"><a href="svm.html#svm-packages-install"><i class="fa fa-check"></i><b>4.2</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="4.3" data-path="svm.html"><a href="svm.html#linear-svm-separable"><i class="fa fa-check"></i><b>4.3</b> 선형 SVM - 분리 가능 경우</a><ul>
<li class="chapter" data-level="4.3.1" data-path="svm.html"><a href="svm.html#linear-svm-separable-basic-script"><i class="fa fa-check"></i><b>4.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="4.3.2" data-path="svm.html"><a href="svm.html#linear-svm-notation"><i class="fa fa-check"></i><b>4.3.2</b> 기호 정의</a></li>
<li class="chapter" data-level="4.3.3" data-path="svm.html"><a href="svm.html#linear-svm-separable-hyperplane"><i class="fa fa-check"></i><b>4.3.3</b> 최적 하이퍼플레인</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="svm.html"><a href="svm.html#linear-svm-inseparable"><i class="fa fa-check"></i><b>4.4</b> 선형 SVM - 분리 불가능 경우</a><ul>
<li class="chapter" data-level="4.4.1" data-path="svm.html"><a href="svm.html#linear-svm-inseparable-basic-script"><i class="fa fa-check"></i><b>4.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="4.4.2" data-path="svm.html"><a href="svm.html#linear-svm-inseparable-hyperplane"><i class="fa fa-check"></i><b>4.4.2</b> 최적 하이퍼플레인</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="svm.html"><a href="svm.html#nonlinear-svm"><i class="fa fa-check"></i><b>4.5</b> 비선형 SVM</a><ul>
<li class="chapter" data-level="4.5.1" data-path="svm.html"><a href="svm.html#nonlinear-svm-basic-script"><i class="fa fa-check"></i><b>4.5.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="4.5.2" data-path="svm.html"><a href="svm.html#nonlinear-svm-hyperplane"><i class="fa fa-check"></i><b>4.5.2</b> 최적 하이퍼플레인</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="svm.html"><a href="svm.html#svm-r-pkg"><i class="fa fa-check"></i><b>4.6</b> R패키지 내 SVM</a><ul>
<li class="chapter" data-level="4.6.1" data-path="svm.html"><a href="svm.html#svm-kernel-function"><i class="fa fa-check"></i><b>4.6.1</b> 커널함수</a></li>
<li class="chapter" data-level="4.6.2" data-path="svm.html"><a href="svm.html#svm-nu-classification"><i class="fa fa-check"></i><b>4.6.2</b> <span class="math inline">\(\nu\)</span>-SVC</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="clustering-overview.html"><a href="clustering-overview.html"><i class="fa fa-check"></i><b>5</b> 군집분석 개요</a><ul>
<li class="chapter" data-level="5.1" data-path="clustering-overview.html"><a href="clustering-overview.html#clustering-overview-packages-install"><i class="fa fa-check"></i><b>5.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="5.2" data-path="clustering-overview.html"><a href="clustering-overview.html#clustering-method"><i class="fa fa-check"></i><b>5.2</b> 군집분석 기법</a></li>
<li class="chapter" data-level="5.3" data-path="clustering-overview.html"><a href="clustering-overview.html#object-similarity-metric"><i class="fa fa-check"></i><b>5.3</b> 객체 간의 유사성 척도</a><ul>
<li class="chapter" data-level="5.3.1" data-path="clustering-overview.html"><a href="clustering-overview.html#object-distance-metric"><i class="fa fa-check"></i><b>5.3.1</b> 거리 관련 척도</a></li>
<li class="chapter" data-level="5.3.2" data-path="clustering-overview.html"><a href="clustering-overview.html#object-correlation-metric"><i class="fa fa-check"></i><b>5.3.2</b> 상관계수 관련 척도</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="clustering-overview.html"><a href="clustering-overview.html#category-similarity-metric"><i class="fa fa-check"></i><b>5.4</b> 범주형 객체의 유사성 척도</a><ul>
<li class="chapter" data-level="5.4.1" data-path="clustering-overview.html"><a href="clustering-overview.html#binary-similarity-metric"><i class="fa fa-check"></i><b>5.4.1</b> 이분형 변수의 경우</a></li>
<li class="chapter" data-level="5.4.2" data-path="clustering-overview.html"><a href="clustering-overview.html#ordinal-similarity-metric"><i class="fa fa-check"></i><b>5.4.2</b> 서열형 변수의 경우</a></li>
<li class="chapter" data-level="5.4.3" data-path="clustering-overview.html"><a href="clustering-overview.html#nominal-similarity-metric"><i class="fa fa-check"></i><b>5.4.3</b> 명목형 변수의 경우</a></li>
<li class="chapter" data-level="5.4.4" data-path="clustering-overview.html"><a href="clustering-overview.html#mixed-similarity-metric"><i class="fa fa-check"></i><b>5.4.4</b> 혼합형의 경우</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>6</b> 계층적 군집방법</a><ul>
<li class="chapter" data-level="6.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#hierarchical-clustering-packages-install"><i class="fa fa-check"></i><b>6.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="6.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#distance-between-clusters"><i class="fa fa-check"></i><b>6.2</b> 군집 간 거리척도 및 연결법</a></li>
<li class="chapter" data-level="6.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#linkage-method"><i class="fa fa-check"></i><b>6.3</b> 연결법의 군집 알고리즘</a><ul>
<li class="chapter" data-level="6.3.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#linkage-method-basic-script"><i class="fa fa-check"></i><b>6.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="6.3.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#linkage-method-algorithm"><i class="fa fa-check"></i><b>6.3.2</b> 연결법 군집 알고리즘</a></li>
<li class="chapter" data-level="6.3.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#hclust"><i class="fa fa-check"></i><b>6.3.3</b> R 패키지 내 연결법</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#ward-method"><i class="fa fa-check"></i><b>6.4</b> 워드 방법</a><ul>
<li class="chapter" data-level="6.4.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#ward-method-basic-script"><i class="fa fa-check"></i><b>6.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="6.4.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#ward-method-algorithm"><i class="fa fa-check"></i><b>6.4.2</b> 워드 군집 알고리즘</a></li>
<li class="chapter" data-level="6.4.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#ward-rpackages"><i class="fa fa-check"></i><b>6.4.3</b> R 패키지 내 워드 방법</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#diana"><i class="fa fa-check"></i><b>6.5</b> 분리적 방법 - 다이아나</a><ul>
<li class="chapter" data-level="6.5.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#diana-basic-script"><i class="fa fa-check"></i><b>6.5.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="6.5.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#diana-algorithm"><i class="fa fa-check"></i><b>6.5.2</b> 다이아나 알고리즘</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#hierarchical-cluster-number"><i class="fa fa-check"></i><b>6.6</b> 군집수의 결정</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html"><i class="fa fa-check"></i><b>7</b> 비계층적 군집방법</a><ul>
<li class="chapter" data-level="7.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#nonhierarchical-clustering-packages-install"><i class="fa fa-check"></i><b>7.1</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="7.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans"><i class="fa fa-check"></i><b>7.2</b> K-means 알고리즘</a><ul>
<li class="chapter" data-level="7.2.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans-basic-script"><i class="fa fa-check"></i><b>7.2.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="7.2.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans-algorithm"><i class="fa fa-check"></i><b>7.2.2</b> 알고리즘</a></li>
<li class="chapter" data-level="7.2.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans-user-defined-functions"><i class="fa fa-check"></i><b>7.2.3</b> R 스크립트 구현</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmedoids"><i class="fa fa-check"></i><b>7.3</b> K-medoids 군집방법</a><ul>
<li class="chapter" data-level="7.3.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#pam"><i class="fa fa-check"></i><b>7.3.1</b> PAM 알고리즘</a></li>
<li class="chapter" data-level="7.3.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#clara"><i class="fa fa-check"></i><b>7.3.2</b> CLARA 알고리즘</a></li>
<li class="chapter" data-level="7.3.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#clarans"><i class="fa fa-check"></i><b>7.3.3</b> CLARANS 알고리즘</a></li>
<li class="chapter" data-level="7.3.4" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans-like"><i class="fa fa-check"></i><b>7.3.4</b> K-means-like 알고리즘</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#fuzzy-kmeans"><i class="fa fa-check"></i><b>7.4</b> 퍼지 K-means 알고리즘</a><ul>
<li class="chapter" data-level="7.4.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#fuzzy-kmeans-basic-script"><i class="fa fa-check"></i><b>7.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="7.4.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#fuzzy-kmeans-algorithm"><i class="fa fa-check"></i><b>7.4.2</b> 알고리즘</a></li>
<li class="chapter" data-level="7.4.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#fuzzy-kmeans-script-implement"><i class="fa fa-check"></i><b>7.4.3</b> R 스크립트 구현</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#model-based-clustering"><i class="fa fa-check"></i><b>7.5</b> 모형기반 군집방법</a><ul>
<li class="chapter" data-level="7.5.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#model-based-clustering-basic-script"><i class="fa fa-check"></i><b>7.5.1</b> 기본 R script</a></li>
<li class="chapter" data-level="7.5.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#model-based-clustering-em"><i class="fa fa-check"></i><b>7.5.2</b> EM 알고리즘</a></li>
<li class="chapter" data-level="7.5.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#model-based-clustering-script-implement"><i class="fa fa-check"></i><b>7.5.3</b> R 스크립트 구현</a></li>
<li class="chapter" data-level="7.5.4" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#r-packages-model-based-clustering"><i class="fa fa-check"></i><b>7.5.4</b> R 패키지 내 모형기반 군집분석</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">데이터마이닝 with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="svm" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> 서포트 벡터 머신</h1>
<div id="svm-overview" class="section level2">
<h2><span class="header-section-number">4.1</span> 개요</h2>
<p>서포트 벡터 머신(suuport vector machine; 이하 SVM)은 기본적으로 두 범주를 갖는 객체들을 분류하는 방법이다. 물론 세 범주 이상의 경우로 확장이 가능하다.</p>
</div>
<div id="svm-packages-install" class="section level2">
<h2><span class="header-section-number">4.2</span> 필요 R package 설치</h2>
<p>본 장에서 필요한 R 패키지들은 아래와 같다.</p>
<table>
<thead>
<tr class="header">
<th align="left">package</th>
<th align="left">version</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">tidyverse</td>
<td align="left">1.2.1</td>
</tr>
<tr class="even">
<td align="left">e1071</td>
<td align="left">1.7-0</td>
</tr>
<tr class="odd">
<td align="left">Matrix</td>
<td align="left">1.2-15</td>
</tr>
<tr class="even">
<td align="left">quadprog</td>
<td align="left">1.5-5</td>
</tr>
</tbody>
</table>
</div>
<div id="linear-svm-separable" class="section level2">
<h2><span class="header-section-number">4.3</span> 선형 SVM - 분리 가능 경우</h2>
<div id="linear-svm-separable-basic-script" class="section level3">
<h3><span class="header-section-number">4.3.1</span> 기본 R 스크립트</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_df &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">x1 =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">9</span>, <span class="dv">5</span>),
  <span class="dt">x2 =</span> <span class="kw">c</span>(<span class="dv">7</span>, <span class="dv">3</span>, <span class="dv">8</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">4</span>),
  <span class="dt">class =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)
)

knitr<span class="op">::</span><span class="kw">kable</span>(train_df, <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
             <span class="dt">align =</span> <span class="kw">c</span>(<span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>),
             <span class="dt">caption =</span> <span class="st">&#39;선형분리가능 학습표본 데이터&#39;</span>)</code></pre></div>
<table>
<caption><span id="tab:svm-train-data-table">Table 4.1: </span>선형분리가능 학습표본 데이터</caption>
<thead>
<tr class="header">
<th align="right">x1</th>
<th align="right">x2</th>
<th align="right">class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">5</td>
<td align="right">7</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">3</td>
<td align="right">-1</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">8</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="right">6</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">6</td>
<td align="right">-1</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">5</td>
<td align="right">-1</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">6</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">9</td>
<td align="right">6</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">4</td>
<td align="right">-1</td>
</tr>
</tbody>
</table>
<p>Table <a href="svm.html#tab:svm-train-data-table">4.1</a>와 같이 두 독립변수 <em>x1</em>, <em>x2</em>와 이분형 종속변수 <em>class</em>의 관측값으로 이루어진 9개의 학습표본을 <em>train_df</em>라는 data frame에 저장한다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(e1071)
svm_model &lt;-<span class="st"> </span><span class="kw">svm</span>(<span class="kw">as.factor</span>(class) <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> train_df, <span class="dt">kernel =</span> <span class="st">&quot;linear&quot;</span>, <span class="dt">scale =</span> <span class="ot">FALSE</span>)
<span class="kw">plot</span>(svm_model, <span class="dt">data =</span> train_df, <span class="dt">formula =</span> x2 <span class="op">~</span><span class="st"> </span>x1, <span class="dt">grid =</span> <span class="dv">200</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:linear-svm-basic"></span>
<img src="data-mining-book_files/figure-html/linear-svm-basic-1.png" alt="선형 SVM 분리 하이퍼플레인" width="672" />
<p class="caption">
Figure 4.1: 선형 SVM 분리 하이퍼플레인
</p>
</div>
<p>그림 <a href="svm.html#fig:linear-svm-basic">4.1</a>에서 각 객체의 기호는 서포트 벡터 여부(“X”이면 서포트 벡터), 각 객체의 색상은 범주값(검정 = -1, 빨강 = 1)을 나타내며, 분리 하이퍼플레인은 아래와 같다.</p>
<p><span class="math display">\[
0.6666667 x_{1} + 0.6666667 x_{2} = 7
\]</span></p>
</div>
<div id="linear-svm-notation" class="section level3">
<h3><span class="header-section-number">4.3.2</span> 기호 정의</h3>
<p>본 장에서 사용될 수학적 기호는 아래와 같다.</p>
<ul>
<li><span class="math inline">\(\mathbf{x} \in \mathbb{R}^p\)</span>: p차원 변수벡터</li>
<li><span class="math inline">\(y \in \{-1, 1\}\)</span>: 범주</li>
<li><span class="math inline">\(N\)</span>: 객체 수</li>
<li><span class="math inline">\((\mathbf{x}_i, y_i)\)</span>: <span class="math inline">\(i\)</span>번째 객체의 변수벡터와 범주값</li>
</ul>
</div>
<div id="linear-svm-separable-hyperplane" class="section level3">
<h3><span class="header-section-number">4.3.3</span> 최적 하이퍼플레인</h3>
<p>선형 SVM은 주어진 객체들의 두 범주를 완벽하게 분리하는 하이퍼플레인 중 각 범주의 서포트 벡터들로부터의 거리가 최대가 되는 하이퍼플레인을 찾는 문제로 귀착된다.</p>
<p>우선 아래와 같이 하이퍼플레인을 정의한다.</p>
<span class="math display" id="eq:linear-svm-hyperplane">\[\begin{equation}
\mathbf{w}^\top \mathbf{x} + b = 0 \tag{4.1}
\end{equation}\]</span>
<p>여기서 <span class="math inline">\(\mathbf{w} \in \mathbb{R}^p\)</span>와 <span class="math inline">\(b \in \mathbb{R}\)</span>이 하이퍼플레인의 계수이다.</p>
<p>범주값이 1인 객체들 중 하이퍼플레인에서 가장 가까운 객체에 대해 다음과 같은 조건이 만족한다고 가정하자.</p>
<p><span class="math display">\[
H_1: \mathbf{w}^\top \mathbf{x} + b = 1 
\]</span></p>
<p>또한 범주값이 -1인 객체들 중 하이퍼플레인에서 가장 가까운 객체에 대해 다음과 같은 조건이 만족한다고 가정하자.</p>
<p><span class="math display">\[
H_2: \mathbf{w}^\top \mathbf{x} + b = -1
\]</span></p>
<p>이 때 두 하이퍼플레인 <span class="math inline">\(H_1\)</span>과 <span class="math inline">\(H_2\)</span> 간의 거리(margin)는 <span class="math inline">\(2 / \lVert \mathbf{w} \rVert\)</span>이다. 선형 SVM은 아래와 같이 <span class="math inline">\(H_1\)</span>과 <span class="math inline">\(H_2\)</span> 간의 거리를 최대로 하는 최적화 문제가 된다.</p>
<span class="math display">\[\begin{equation*}
\begin{split}
\max \text{  } &amp; \frac{2}{\mathbf{w}^\top \mathbf{w}}\\
\text{s.t.}&amp; \\
&amp; \mathbf{w}^\top \mathbf{x}_i + b \ge 1 \text{ for } y_i = 1\\
&amp; \mathbf{w}^\top \mathbf{x}_i + b \le -1 \text{ for } y_i = -1
\end{split}
\end{equation*}\]</span>
<p>이를 간략히 정리하면</p>
<span class="math display">\[\begin{equation*}
\begin{split}
\min \text{  } &amp; \frac{\mathbf{w}^\top \mathbf{w}}{2}\\
\text{s.t.}&amp; \\
&amp; y_i \left( \mathbf{w}^\top \mathbf{x}_i + b \right) \ge 1
\end{split}
\end{equation*}\]</span>
<p>과 같이 정리할 수 있으며, 각 객체 <span class="math inline">\(i\)</span>에 대한 제약조건에 라그랑지 계수(Lagrange multiplier) <span class="math inline">\(\alpha_i \ge 0\)</span>를 도입하여 라그랑지 함수를 유도하면 식 <a href="svm.html#eq:linear-svm-primal">(4.2)</a>과 같은 최적화 문제가 된다. 이를 원문제(primal problem)라 하자.</p>
<span class="math display" id="eq:linear-svm-primal">\[\begin{equation}
\begin{split}
\min \text{  } &amp; L_P = \frac{1}{2} \mathbf{w}^\top \mathbf{w} + \sum_{i = 1}^{N} \alpha_i \left[ y_i \left( \mathbf{w}^\top \mathbf{x}_i + b \right) - 1 \right]\\
\text{s.t.  } &amp; \alpha_i \ge 0, \text{  } i = 1, \cdots, N
\end{split}
\tag{4.2}
\end{equation}\]</span>
<p>원문제 식 <a href="svm.html#eq:linear-svm-primal">(4.2)</a>에 대한 울프쌍대문제(Wolfe dual problem)는 아래 식 <a href="svm.html#eq:linear-svm-dual">(4.3)</a>과 같이 도출된다. 보다 자세한 내용은 교재<span class="citation">(전치혁 <a href="#ref-jun2012datamining">2012</a>)</span> 참고.</p>
<span class="math display" id="eq:linear-svm-dual">\[\begin{equation}
\begin{split}
\max \text{  } &amp; L_D = \sum_{i = 1}^{N} \alpha_i - \frac{1}{2} \sum_{i = 1}^{N} \sum_{j = 1}^{N} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j\\
\text{s.t. } &amp;\\
&amp; \sum_{i = 1}^{N} \alpha_i y_i = 0\\
&amp; \alpha_i \ge 0, \text{  } i = 1, \cdots, N
\end{split}
\tag{4.3}
\end{equation}\]</span>
<p>식 <a href="svm.html#eq:linear-svm-dual">(4.3)</a>은 이차계획(quadratic programming) 문제로, 각종 소프트웨어와 알고리즘을 이용하여 구할 수 있다. 본 장에서는 <code>quadprog</code> 패키지를 이용하여 해를 구하기로 한다. 이는 실제로 <code>e1071</code>의 <code>svm</code> 함수 호출 시 사용하는 방법은 아니며, 실제 <code>svm</code> 함수가 호출하는 알고리즘은 다음 장에서 다시 설명하기로 한다.</p>
<p><code>quadprog</code>의 <code>solve.QP</code> 함수는 아래와 같은 형태로 formulation된 문제<span class="citation">(Goldfarb and Idnani <a href="#ref-goldfarb1983numerically">1983</a>)</span>에 대한 최적해를 구한다.</p>
<span class="math display" id="eq:quadprog">\[\begin{equation}
\begin{split}
\min \text{  } &amp; -\mathbf{d}^{\top}\boldsymbol{\alpha} + \frac{1}{2} \boldsymbol{\alpha}^{\top}\mathbf{D}\boldsymbol{\alpha}\\
\text{s.t. } &amp; \mathbf{A}^{\top}\boldsymbol{\alpha} \ge \mathbf{b}_0
\end{split}
\tag{4.4}
\end{equation}\]</span>
<p>식 <a href="svm.html#eq:quadprog">(4.4)</a>과 식 <a href="svm.html#eq:linear-svm-dual">(4.3)</a>이 동일한 문제를 나타내도록 아래와 같이 목적함수에 필요한 벡터 및 행렬을 정의한다.</p>
<span class="math display">\[\begin{eqnarray*}
\mathbf{d} &amp;=&amp; \mathbf{1}_{N \times 1}\\
\mathbf{D} &amp;=&amp; \mathbf{y}\mathbf{y}^{\top}\mathbf{X}\mathbf{X}^{\top}
\end{eqnarray*}\]</span>
where
<span class="math display">\[\begin{eqnarray*}
\mathbf{y} &amp;=&amp; \left[ \begin{array}{c c c c} y_1 &amp; y_2 &amp; \cdots &amp; y_N \end{array} \right]^\top\\
\mathbf{X} &amp;=&amp; \left[ \begin{array}{c c c c} \mathbf{x}_1 &amp; \mathbf{x}_2 &amp; \cdots &amp; \mathbf{x}_N \end{array} \right]^{\top}
\end{eqnarray*}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="kw">dim</span>(train_df)[<span class="dv">1</span>]
X &lt;-<span class="st"> </span>train_df[<span class="kw">c</span>(<span class="st">&#39;x1&#39;</span>, <span class="st">&#39;x2&#39;</span>)] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>()
y &lt;-<span class="st"> </span>train_df[[<span class="st">&#39;class&#39;</span>]] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.numeric</span>()

d &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, N)
D &lt;-<span class="st"> </span>(y <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(y)) <span class="op">*</span><span class="st"> </span>(X <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X))</code></pre></div>
<p>여기에서 행렬 <span class="math inline">\(\mathbf{D}\)</span>의 determinant 값은 0으로, <span class="citation">Goldfarb and Idnani (<a href="#ref-goldfarb1983numerically">1983</a>)</span> 가 가정하는 symmetric positive definite matrix 조건에 위배되어 <code>solve.QP</code> 함수 실행 시 오류가 발생한다. 이를 방지하기 위해 아래 예에서는 <code>Matrix</code> 패키지의 <code>nearPD</code>함수를 이용하여 행렬 <span class="math inline">\(\mathbf{D}\)</span>와 근사한 symmetric positive definite matrix를 아래와 같이 찾는다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_pd &lt;-<span class="st"> </span>Matrix<span class="op">::</span><span class="kw">nearPD</span>(D, <span class="dt">doSym =</span> T)<span class="op">$</span>mat <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>()</code></pre></div>
<p>식 <a href="svm.html#eq:quadprog">(4.4)</a>의 제약식은 모두 inequality 형태로, 식 <a href="svm.html#eq:linear-svm-dual">(4.3)</a>의 equality constraint <span class="math inline">\(\sum_{i = 1}^{N} \alpha_i y_i = 0\)</span>를 표현하기 위해서 두 개의 제약식 <span class="math inline">\(\sum_{i = 1}^{N} \alpha_i y_i \ge 0\)</span>와 <span class="math inline">\(\sum_{i = 1}^{N} - \alpha_i y_i \ge 0\)</span>를 생성한다.</p>
<span class="math display">\[\begin{equation*}
\mathbf{A}^\top = \left[ 
\begin{array}{c c c c}
y_1 &amp; y_2 &amp; \cdots &amp; y_N\\
-y_1 &amp; -y_2 &amp; \cdots &amp; -y_N\\
1 &amp; 0 &amp; \cdots &amp; 0\\
0 &amp; 1 &amp; \cdots &amp; 0\\
\cdots &amp; \cdots &amp; \cdots &amp; \cdots \\
0 &amp; 0 &amp; \cdots &amp; 1
\end{array}
\right],
\mathbf{b}_0 = \left[ \begin{array}{c}
0 \\ 0 \\ 0 \\ 0 \\ \cdots \\ 0
\end{array}
\right]
\end{equation*}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">A &lt;-<span class="st"> </span><span class="kw">cbind</span>(
  y,
  <span class="op">-</span>y,
  <span class="kw">diag</span>(N)
)
b_zero &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">2</span> <span class="op">+</span><span class="st"> </span>N)</code></pre></div>
<p>이제 위에서 구한 행렬과 벡터들을 <code>solve.QP</code> 함수에 입력하여 최적해를 구한다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res &lt;-<span class="st"> </span>quadprog<span class="op">::</span><span class="kw">solve.QP</span>(D_pd, d, A, b_zero)
alpha_sol &lt;-<span class="st"> </span>res<span class="op">$</span>solution
obj_val &lt;-<span class="st"> </span><span class="op">-</span>res<span class="op">$</span>value</code></pre></div>
<table>
<caption><span id="tab:svm-separable-alpha">Table 4.2: </span>이차계획문제의 최적해</caption>
<thead>
<tr class="header">
<th align="center">variable</th>
<th align="center">solution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">alpha_1</td>
<td align="center">0.2234</td>
</tr>
<tr class="even">
<td align="center">alpha_2</td>
<td align="center">0.0000</td>
</tr>
<tr class="odd">
<td align="center">alpha_3</td>
<td align="center">0.0000</td>
</tr>
<tr class="even">
<td align="center">alpha_4</td>
<td align="center">0.0000</td>
</tr>
<tr class="odd">
<td align="center">alpha_5</td>
<td align="center">0.2228</td>
</tr>
<tr class="even">
<td align="center">alpha_6</td>
<td align="center">0.0000</td>
</tr>
<tr class="odd">
<td align="center">alpha_7</td>
<td align="center">0.2210</td>
</tr>
<tr class="even">
<td align="center">alpha_8</td>
<td align="center">0.0000</td>
</tr>
<tr class="odd">
<td align="center">alpha_9</td>
<td align="center">0.2216</td>
</tr>
</tbody>
</table>
<p>표 <a href="svm.html#tab:svm-separable-alpha">4.2</a>의 결과는 교재<span class="citation">(전치혁 <a href="#ref-jun2012datamining">2012</a>)</span>에 나타난 최적해와는 다소 차이가 있으나, 결과적으로 목적함수값은 0.4444로 동일하다.</p>
<p>위의 과정으로 최적해 <span class="math inline">\(\alpha_{i}^{*}\)</span>를 구한 뒤, 아래와 같이 분리 하이퍼플레인의 계수를 결정할 수 있다.</p>
<span class="math display">\[\begin{eqnarray*}
\mathbf{w} &amp;=&amp; \sum_{i = 1}^{N} \alpha_{i}^{*} y_{i} \mathbf{x}_{i}\\
b &amp;=&amp; \sum_{i: \alpha_{i}^{*} &gt; 0} \frac{1 - y_{i} \mathbf{w}^{\top} \mathbf{x}_{i}}{y_{i}} \left/ \sum_{i: \alpha_{i}^{*} &gt; 0} 1 \right. 
\end{eqnarray*}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">w &lt;-<span class="st"> </span><span class="kw">colSums</span>(alpha_sol <span class="op">*</span><span class="st"> </span>y <span class="op">*</span><span class="st"> </span>X)
<span class="kw">print</span>(w)</code></pre></div>
<pre><code>##        x1        x2 
## 0.6666658 0.6666657</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sv_ind &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">round</span>(alpha_sol, <span class="dt">digits =</span> <span class="dv">4</span>) <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)
b &lt;-<span class="st"> </span><span class="kw">mean</span>((<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>y[sv_ind] <span class="op">*</span><span class="st"> </span>(X[sv_ind, ] <span class="op">%*%</span><span class="st"> </span>w)) <span class="op">/</span><span class="st"> </span>y[sv_ind])
<span class="kw">print</span>(b)</code></pre></div>
<pre><code>## [1] -6.99999</code></pre>
<p>위 결과와 같이, 분리 하이퍼플레인은 교재와 동일하게 얻어진다.</p>
</div>
</div>
<div id="linear-svm-inseparable" class="section level2">
<h2><span class="header-section-number">4.4</span> 선형 SVM - 분리 불가능 경우</h2>
<p>본 장에서는 학습표본 내의 두 범주가 어떠한 선형 하이퍼플레인으로도 완전하게 분리되지 않아 식 <a href="svm.html#eq:linear-svm-primal">(4.2)</a>이 해를 갖지 못하는 경우에 대한 문제를 다룬다.</p>
<div id="linear-svm-inseparable-basic-script" class="section level3">
<h3><span class="header-section-number">4.4.1</span> 기본 R 스크립트</h3>
<p>앞 장에서 사용한 학습표본에 아래와 같이 하나의 객체를 추가하여 전체 학습표본이 선형 하이퍼플레인으로 분리될 수 없도록 하자.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">inseparable_train_df &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(train_df, 
                                  <span class="kw">tibble</span>(<span class="dt">x1 =</span> <span class="dv">7</span>, <span class="dt">x2 =</span> <span class="dv">6</span>, <span class="dt">class =</span> <span class="op">-</span><span class="dv">1</span>))

knitr<span class="op">::</span><span class="kw">kable</span>(inseparable_train_df, <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
             <span class="dt">align =</span> <span class="kw">c</span>(<span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>),
             <span class="dt">caption =</span> <span class="st">&#39;선형분리불가능 학습표본 데이터&#39;</span>)</code></pre></div>
<table>
<caption><span id="tab:svm-inseparable-train-data-table">Table 4.3: </span>선형분리불가능 학습표본 데이터</caption>
<thead>
<tr class="header">
<th align="right">x1</th>
<th align="right">x2</th>
<th align="right">class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">5</td>
<td align="right">7</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">3</td>
<td align="right">-1</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">8</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="right">6</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">6</td>
<td align="right">-1</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">5</td>
<td align="right">-1</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">6</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">9</td>
<td align="right">6</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">4</td>
<td align="right">-1</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">6</td>
<td align="right">-1</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(e1071)
svm_model &lt;-<span class="st"> </span><span class="kw">svm</span>(<span class="kw">as.factor</span>(class) <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> inseparable_train_df, 
                 <span class="dt">kernel =</span> <span class="st">&quot;linear&quot;</span>, <span class="dt">cost =</span> <span class="dv">1</span>, <span class="dt">scale =</span> <span class="ot">FALSE</span>)
<span class="kw">plot</span>(svm_model, <span class="dt">data =</span> inseparable_train_df, <span class="dt">formula =</span> x2 <span class="op">~</span><span class="st"> </span>x1, <span class="dt">grid =</span> <span class="dv">200</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:linear-svm-basic-inseparable"></span>
<img src="data-mining-book_files/figure-html/linear-svm-basic-inseparable-1.png" alt="선형 SVM 분리 불가능 경우의 하이퍼플레인" width="672" />
<p class="caption">
Figure 4.2: 선형 SVM 분리 불가능 경우의 하이퍼플레인
</p>
</div>
<p>Figure <a href="svm.html#fig:linear-svm-basic-inseparable">4.2</a>에서 보이듯, 하나의 검정 객체(범주 = -1)가 범주 1로 분류되는 영역에 존재하여 오분류가 발생한다. 이처럼 선형 하이퍼플레인으로 두 범주 학습표본의 분리가 불가능한 경우, 오분류 학습표본에 대한 페널티를 적용하여 최적 분리 하이퍼플레인을 도출하게 된다. 위 예에서의 최적 하이퍼플레인은 아래와 같다.</p>
<p><span class="math display">\[
0.6 x_{1} + 0.8 x_{2} = 7.6
\]</span></p>
</div>
<div id="linear-svm-inseparable-hyperplane" class="section level3">
<h3><span class="header-section-number">4.4.2</span> 최적 하이퍼플레인</h3>
<p>여유변수(slack variable) <span class="math inline">\(\xi_i\)</span> 를 각 학습객체 <span class="math inline">\(i = 1, \cdots, N\)</span>에 대해 아래와 같이 정의한다.</p>
<span class="math display">\[\begin{equation*}
\xi_i = \max \left\{ 0, 1 - y_i (\mathbf{w}^\top \mathbf{x}_i + b) \right\}
\end{equation*}\]</span>
<p>이는 객체가 자신의 범주의 서포트 벡터를 지나는 하이퍼플레인(범주 1인 경우 <span class="math inline">\(H_1\)</span>, 범주 -1인 경우 <span class="math inline">\(H_2\)</span>)으로 부터 다른 범주 방향으로 떨어진 거리를 나타낸다. 이 여유변수 <span class="math inline">\(\xi_i\)</span>에 단위당 페널티 단가 <span class="math inline">\(C\)</span>를 부여하여 아래와 같은 최적화 문제를 정의한다.</p>
<span class="math display">\[\begin{equation*}
\begin{split}
\min \text{  } &amp; \frac{\mathbf{w}^\top \mathbf{w}}{2} + C \sum_{i = 1}^{N} \xi_i \\
\text{s.t.}&amp; \\
&amp; y_i \left( \mathbf{w}^\top \mathbf{x}_i + b \right) \ge 1 - \xi_i, \text{  } i = 1, \cdots, N \\
&amp; \xi \ge 0, \text{  } i = 1, \cdots, N
\end{split}
\end{equation*}\]</span>
<p>이에 대한 울프쌍대문제를 앞 <a href="svm.html#linear-svm-separable-hyperplane">4.3.3</a>장과 같은 과정으로 도출하면 아래 식 <a href="svm.html#eq:linear-svm-inseparable-dual">(4.5)</a>와 같다.</p>
<span class="math display" id="eq:linear-svm-inseparable-dual">\[\begin{equation}
\begin{split}
\max \text{  } &amp; L_D = \sum_{i = 1}^{N} \alpha_i - \frac{1}{2} \sum_{i = 1}^{N} \sum_{j = 1}^{N} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j\\
\text{s.t. } &amp;\\
&amp; \sum_{i = 1}^{N} \alpha_i y_i = 0\\
&amp; 0 \le \alpha_i \le C, \text{  } i = 1, \cdots, N
\end{split}
\tag{4.5}
\end{equation}\]</span>
<p>이는 분리 가능 경우의 식 <a href="svm.html#eq:linear-svm-dual">(4.3)</a>에 변수 <span class="math inline">\(\alpha_i\)</span>에 대한 상한값 <span class="math inline">\(C\)</span>의 제약이 추가된 문제로, 이는 <code>e1071</code> 패키지의 <code>svm</code> 함수가 기본 방법으로 사용하는 <code>LIBSVM</code> 라이브러리<span class="citation">(Chang and Lin <a href="#ref-chang2011libsvm">2011</a>)</span>의 <span class="math inline">\(C\)</span>-support vector classification(<span class="math inline">\(C\)</span>-SVC)이 사용하는 문제식이며, <code>LIBSVM</code> 라이브러리는 특정 알고리즘<span class="citation">(Fan, Chen, and Lin <a href="#ref-fan2005working">2005</a>)</span>을 이용하여 해를 제공한다.</p>
<p>아래 <code>svm</code> 함수의 입력 변수에서 <code>type = &quot;C-classification&quot;</code>은 식 <a href="svm.html#eq:linear-svm-dual">(4.3)</a>를 최적화하겠다는 것을 나타내며, <code>cost = 1</code>은 페널티 단가 <span class="math inline">\(C\)</span>의 값을 1로 설정하겠다는 것을 나타낸다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">svm_model &lt;-<span class="st"> </span><span class="kw">svm</span>(<span class="kw">as.factor</span>(class) <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> inseparable_train_df,
                 <span class="dt">kernel =</span> <span class="st">&quot;linear&quot;</span>, <span class="dt">scale =</span> <span class="ot">FALSE</span>,
                 <span class="dt">type =</span> <span class="st">&quot;C-classification&quot;</span>, <span class="dt">cost =</span> <span class="dv">1</span>)</code></pre></div>
<p>위 결과 모델 객체 <code>svm_model</code>의 원소 중 <code>index</code>는 학습표본 중 서포트 벡터에 해당하는 인덱스 <span class="math inline">\(i\)</span>를 나타내며, <code>coefs</code>는 각 서포트 벡터의 <span class="math inline">\(\alpha_i y_i\)</span> 값을 나타낸다. 따라서, <code>coefs</code>를 각 서포트 벡터의 범주값 <span class="math inline">\(y_i\)</span>로 나누면 식 <a href="svm.html#eq:linear-svm-inseparable-dual">(4.5)</a>의 최적해를 아래와 같이 볼 수 있다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="kw">dim</span>(inseparable_train_df)[<span class="dv">1</span>]
X &lt;-<span class="st"> </span>inseparable_train_df[<span class="kw">c</span>(<span class="st">&#39;x1&#39;</span>, <span class="st">&#39;x2&#39;</span>)] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>()
y &lt;-<span class="st"> </span>inseparable_train_df[[<span class="st">&#39;class&#39;</span>]] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.numeric</span>()

sv_ind &lt;-<span class="st"> </span>svm_model<span class="op">$</span>index
alpha_sol &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>, N)
alpha_sol[sv_ind] &lt;-<span class="st"> </span><span class="kw">drop</span>(svm_model<span class="op">$</span>coefs[, <span class="dv">1</span>]) <span class="op">/</span><span class="st"> </span>y[sv_ind]</code></pre></div>
<table>
<caption><span id="tab:svm-inseparable-alpha">Table 4.4: </span>이차계획문제의 최적해: 선형 분리 불가능 경우</caption>
<thead>
<tr class="header">
<th align="center">variable</th>
<th align="center">solution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">alpha_1</td>
<td align="center">0.8</td>
</tr>
<tr class="even">
<td align="center">alpha_2</td>
<td align="center">0.0</td>
</tr>
<tr class="odd">
<td align="center">alpha_3</td>
<td align="center">0.0</td>
</tr>
<tr class="even">
<td align="center">alpha_4</td>
<td align="center">0.0</td>
</tr>
<tr class="odd">
<td align="center">alpha_5</td>
<td align="center">0.8</td>
</tr>
<tr class="even">
<td align="center">alpha_6</td>
<td align="center">0.0</td>
</tr>
<tr class="odd">
<td align="center">alpha_7</td>
<td align="center">1.0</td>
</tr>
<tr class="even">
<td align="center">alpha_8</td>
<td align="center">0.0</td>
</tr>
<tr class="odd">
<td align="center">alpha_9</td>
<td align="center">0.0</td>
</tr>
<tr class="even">
<td align="center">alpha_10</td>
<td align="center">1.0</td>
</tr>
</tbody>
</table>
<p>하이퍼플레인의 계수 <span class="math inline">\(\mathbf{w}\)</span>는 분리 가능의 경우와 동일하게 구할 수 있다.</p>
<span class="math display">\[\begin{equation*}
\mathbf{w} = \sum_{i = 1}^{N} \alpha_{i}^{*} y_{i} \mathbf{x}_{i}
\end{equation*}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">w &lt;-<span class="st"> </span><span class="kw">colSums</span>(alpha_sol <span class="op">*</span><span class="st"> </span>y <span class="op">*</span><span class="st"> </span>X)
<span class="kw">print</span>(w)</code></pre></div>
<pre><code>##  x1  x2 
## 0.6 0.8</code></pre>
<p>상수 <span class="math inline">\(b\)</span>는 아래와 같이 <span class="math inline">\(0 &lt; \alpha_{i}^{*} &lt; C\)</span>인 객체들을 이용해 산출할 수 있다.</p>
<span class="math display">\[\begin{equation*}
b = \sum_{i: 0 &lt; \alpha_{i}^{*} &lt; C} \frac{1 - y_{i} \mathbf{w}^{\top} \mathbf{x}_{i}}{y_{i}} \left/ \sum_{i: 0 &lt; \alpha_{i}^{*} &lt; C} 1 \right. 
\end{equation*}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ind &lt;-<span class="st"> </span>sv_ind[alpha_sol[sv_ind] <span class="op">&lt;</span><span class="st"> </span>svm_model<span class="op">$</span>cost]
b &lt;-<span class="st"> </span><span class="kw">mean</span>((<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>y[ind] <span class="op">*</span><span class="st"> </span>(X[ind, ] <span class="op">%*%</span><span class="st"> </span>w)) <span class="op">/</span><span class="st"> </span>y[ind])
<span class="kw">print</span>(b)</code></pre></div>
<pre><code>## [1] -7.6</code></pre>
<p>위와 같은 하이퍼플레인의 계수 <span class="math inline">\(\mathbf{w}\)</span>와 상수 <span class="math inline">\(b\)</span>값은 <code>svm</code> 객체에 원소들을 이용하여 보다 쉽게 확인할 수 있다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">w &lt;-<span class="st"> </span><span class="kw">t</span>(svm_model<span class="op">$</span>coefs) <span class="op">%*%</span><span class="st"> </span>svm_model<span class="op">$</span>SV
<span class="kw">print</span>(w)</code></pre></div>
<pre><code>##       x1  x2
## [1,] 0.6 0.8</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">b &lt;-<span class="st"> </span><span class="op">-</span>svm_model<span class="op">$</span>rho
<span class="kw">print</span>(b)</code></pre></div>
<pre><code>## [1] -7.6</code></pre>
<p>선형 하이퍼플레인으로 분리 불가능한 경우, 페널티 단가 <span class="math inline">\(C\)</span>의 값에 따라 도출되는 분리 하이퍼플레인이 달라진다. <span class="math inline">\(C\)</span>의 값이 1, 5, 100일 때의 하이퍼플레인을 비교해보자.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">svm_models &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">100</span>), <span class="cf">function</span>(C)
  <span class="kw">svm</span>(<span class="kw">as.factor</span>(class) <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> inseparable_train_df,
      <span class="dt">kernel =</span> <span class="st">&quot;linear&quot;</span>, <span class="dt">scale =</span> <span class="ot">FALSE</span>,
      <span class="dt">type =</span> <span class="st">&quot;C-classification&quot;</span>, <span class="dt">cost =</span> C))

getHyperplane &lt;-<span class="st"> </span><span class="cf">function</span>(model) {
  <span class="kw">list</span>(<span class="dt">C =</span> model<span class="op">$</span>cost,
       <span class="dt">w =</span> <span class="kw">paste</span>(<span class="kw">round</span>(<span class="kw">t</span>(model<span class="op">$</span>coefs) <span class="op">%*%</span><span class="st"> </span>model<span class="op">$</span>SV, <span class="dt">digits =</span> <span class="dv">2</span>), <span class="dt">collapse =</span> <span class="st">&quot;, &quot;</span>),
       <span class="dt">b =</span> <span class="op">-</span><span class="kw">round</span>(model<span class="op">$</span>rho, <span class="dt">digits =</span> <span class="dv">2</span>),
       <span class="dt">sv =</span> <span class="kw">paste</span>(model<span class="op">$</span>index, <span class="dt">collapse =</span> <span class="st">&quot;, &quot;</span>),
       <span class="dt">misclassified =</span> <span class="kw">paste</span>(<span class="kw">which</span>(model<span class="op">$</span>fitted <span class="op">!=</span><span class="st"> </span><span class="kw">as.factor</span>(inseparable_train_df<span class="op">$</span>class)), <span class="dt">collapse =</span> <span class="st">&quot;, &quot;</span>))
}

svm_summary &lt;-<span class="st"> </span><span class="kw">lapply</span>(svm_models, getHyperplane) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_rows</span>()</code></pre></div>
<table>
<caption><span id="tab:svm-inseparable-summary">Table 4.5: </span>페널티 단가 C에 따른 하이퍼플레인 계수 및 결과</caption>
<thead>
<tr class="header">
<th align="center">페널티 단가 <span class="math inline">\(C\)</span></th>
<th align="center"><span class="math inline">\((w_1, w_2)\)</span></th>
<th align="center"><span class="math inline">\(b\)</span></th>
<th align="center">서포트 벡터 객체</th>
<th align="center">오분류 객체</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">0.6, 0.8</td>
<td align="center">-7.6</td>
<td align="center">1, 7, 5, 10</td>
<td align="center">10</td>
</tr>
<tr class="even">
<td align="center">5</td>
<td align="center">0.4, 1.2</td>
<td align="center">-9.4</td>
<td align="center">1, 4, 7, 5, 10</td>
<td align="center">10</td>
</tr>
<tr class="odd">
<td align="center">100</td>
<td align="center">0.4, 1.2</td>
<td align="center">-9.4</td>
<td align="center">1, 4, 7, 5, 10</td>
<td align="center">10</td>
</tr>
</tbody>
</table>
<p>Table <a href="svm.html#tab:svm-inseparable-summary">4.5</a>에서 보이는 바와 같이, 페널티 단가 <span class="math inline">\(C\)</span>의 값이 1과 5일 때 분리 하이퍼플레인이 변하는 것을 볼 수 있다. <span class="math inline">\(C\)</span>값이 5와 100일 때의 분리 하이퍼플레인은 거의 동일하다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(svm_models[[<span class="dv">2</span>]], <span class="dt">data =</span> inseparable_train_df, <span class="dt">formula =</span> x2 <span class="op">~</span><span class="st"> </span>x1, <span class="dt">grid =</span> <span class="dv">200</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:linear-svm-inseparable-highcost"></span>
<img src="data-mining-book_files/figure-html/linear-svm-inseparable-highcost-1.png" alt="선형 SVM 분리 불가능 경우의 하이퍼플레인 ($C = 5$)" width="672" />
<p class="caption">
Figure 4.3: 선형 SVM 분리 불가능 경우의 하이퍼플레인 (<span class="math inline">\(C = 5\)</span>)
</p>
</div>
<p>Figure <a href="svm.html#fig:linear-svm-inseparable-highcost">4.3</a>의 하이퍼플레인(<span class="math inline">\(C = 5\)</span>인 경우)은 Figure <a href="svm.html#fig:linear-svm-basic-inseparable">4.2</a>의 하이퍼플레인(<span class="math inline">\(C = 1\)</span>인 경우)보다 오분류 객체에 가깝게 위치함을 확인할 수 있다.</p>
</div>
</div>
<div id="nonlinear-svm" class="section level2">
<h2><span class="header-section-number">4.5</span> 비선형 SVM</h2>
<p>본 장에서는 선형으로 분리 성능이 좋지 않은 경우에 대해 원 입력변수에 대해 비선형인 하이퍼플레인을 찾는 문제를 다룬다. 이는 원 입력변수에 대해 비선형인 기저함수 공간으로 객체를 이동시킨 후 해당 공간에서 선형 분리 하이퍼플레인을 찾는 과정이다.</p>
<div id="nonlinear-svm-basic-script" class="section level3">
<h3><span class="header-section-number">4.5.1</span> 기본 R 스크립트</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nonlinear_train_df &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">x1 =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">9</span>, <span class="dv">5</span>), 
  <span class="dt">x2 =</span> <span class="kw">c</span>(<span class="dv">7</span>, <span class="dv">3</span>, <span class="dv">8</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">4</span>), 
  <span class="dt">class =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)
)

knitr<span class="op">::</span><span class="kw">kable</span>(nonlinear_train_df, <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
             <span class="dt">align =</span> <span class="kw">c</span>(<span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>),
             <span class="dt">caption =</span> <span class="st">&#39;비선형 SVM 학습표본 데이터&#39;</span>)</code></pre></div>
<table>
<caption><span id="tab:nonlinear-svm-train-data">Table 4.6: </span>비선형 SVM 학습표본 데이터</caption>
<thead>
<tr class="header">
<th align="right">x1</th>
<th align="right">x2</th>
<th align="right">class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">5</td>
<td align="right">7</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">3</td>
<td align="right">-1</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">8</td>
<td align="right">-1</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="right">6</td>
<td align="right">-1</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">6</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">5</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">6</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">9</td>
<td align="right">6</td>
<td align="right">-1</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">4</td>
<td align="right">-1</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(e1071)
svm_model &lt;-<span class="st"> </span><span class="kw">svm</span>(<span class="kw">as.factor</span>(class) <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> nonlinear_train_df, 
                 <span class="dt">kernel =</span> <span class="st">&quot;polynomial&quot;</span>, <span class="dt">coef0 =</span> <span class="dv">1</span>, <span class="dt">gamma =</span> <span class="dv">1</span>, <span class="dt">degree =</span> <span class="dv">2</span>,
                 <span class="dt">cost =</span> <span class="dv">5</span>, <span class="dt">scale =</span> <span class="ot">FALSE</span>)
<span class="kw">plot</span>(svm_model, <span class="dt">data =</span> nonlinear_train_df, <span class="dt">formula =</span> x2 <span class="op">~</span><span class="st"> </span>x1, <span class="dt">grid =</span> <span class="dv">200</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:nonlinear-svm-basic"></span>
<img src="data-mining-book_files/figure-html/nonlinear-svm-basic-1.png" alt="비선형 SVM 하이퍼플레인" width="672" />
<p class="caption">
Figure 4.4: 비선형 SVM 하이퍼플레인
</p>
</div>
</div>
<div id="nonlinear-svm-hyperplane" class="section level3">
<h3><span class="header-section-number">4.5.2</span> 최적 하이퍼플레인</h3>
<p>식 <a href="svm.html#eq:linear-svm-hyperplane">(4.1)</a>을 일반화한 다음과 같은 하이퍼플레인을 고려하자.</p>
<span class="math display" id="eq:nonlinear-svm-hyperplane">\[\begin{equation}
f(\mathbf{x}) = \Phi(\mathbf{x})^\top \mathbf{w} + b \tag{4.6}
\end{equation}\]</span>
<p>여기서 벡터함수 <span class="math inline">\(\Phi: \mathbb{R}^p \rightarrow \mathbb{R}^m\)</span>는 <span class="math inline">\(\mathbf{x}\)</span>에 대한 새로운 특징(feature)을 추출하는 변환함수라 할 수 있는데, 통상 추출되는 특징의 차원 <span class="math inline">\(m\)</span>이 원 변수 <span class="math inline">\(\mathbf{x}\)</span>의 차원 <span class="math inline">\(p\)</span>보다 높다. 이를 <span class="math inline">\(\mathbf{x}\)</span>의 기저함수(basis function)라 부르며, 하이퍼플레인 계수 또한 <span class="math inline">\(m\)</span>차원의 벡터가 된다 (<span class="math inline">\(\mathbf{w} \in \mathbb{R}^m\)</span>). 이 때, 비선형 SVM 문제는 선형 SVM 문제 식 <a href="svm.html#eq:linear-svm-inseparable-dual">(4.5)</a>에서 변수를 기저변수로 치환한 형태로 아래 식 <a href="svm.html#eq:nonlinear-svm-dual">(4.7)</a>과 같이 나타낼 수 있다.</p>
<span class="math display" id="eq:nonlinear-svm-dual">\[\begin{equation}
\begin{split}
\max \text{  } &amp; L_D = \sum_{i = 1}^{N} \alpha_i - \frac{1}{2} \sum_{i = 1}^{N} \sum_{j = 1}^{N} \alpha_i \alpha_j y_i y_j \Phi(\mathbf{x}_i)^\top \Phi(\mathbf{x}_j)\\
\text{s.t. } &amp;\\
&amp; \sum_{i = 1}^{N} \alpha_i y_i = 0\\
&amp; 0 \le \alpha_i \le C, \text{  } i = 1, \cdots, N
\end{split}
\tag{4.7}
\end{equation}\]</span>
<p>식 <a href="svm.html#eq:nonlinear-svm-dual">(4.7)</a>의 목적함수에서 기저함수의 내적 <span class="math inline">\(\Phi(\mathbf{x}_i)^\top \Phi(\mathbf{x}_j)\)</span>을 아래와 같이 커널함수(kernel function)로 나타낼 수 있으며, 이는 두 객체 <span class="math inline">\(\mathbf{x}_i, \mathbf{x}_j\)</span>간의 일종의 유사성 척도(similarity measure)로 해석될 수 있다.</p>
<span class="math display">\[\begin{equation*}
K(\mathbf{x}_i, \mathbf{x}_j) = \Phi(\mathbf{x}_i)^\top \Phi(\mathbf{x}_j)
\end{equation*}\]</span>
<p>널리 사용되는 커널함수로는 아래와 같은 함수들이 있다.</p>
<span class="math display">\[\begin{eqnarray*}
\text{Gaussian RBF:} &amp; &amp; K(\mathbf{x}_i, \mathbf{x}_j) &amp;=&amp; \exp \left( \frac{- \left\lVert \mathbf{x}_i - \mathbf{x}_j \right\rVert^2}{2 \sigma^2} \right)\\
\text{$r$-th order polynomial:} &amp; &amp; K(\mathbf{x}_i, \mathbf{x}_j) &amp;=&amp; \left( \mathbf{x}_i^\top \mathbf{x}_j + 1 \right)^r \\
\text{Sigmoid:} &amp; &amp; K(\mathbf{x}_i, \mathbf{x}_j) &amp;=&amp; \tanh \left(\kappa \mathbf{x}_i^\top \mathbf{x}_j - \delta \right)
\end{eqnarray*}\]</span>
<p>커널함수를 이용하여 분리 하이퍼플레인을 찾기 위한 식을 아래와 같이 나타낸다.</p>
<span class="math display" id="eq:nonlinear-svm-dual-kernel">\[\begin{equation}
\begin{split}
\max \text{  } &amp; L_D = \sum_{i = 1}^{N} \alpha_i - \frac{1}{2} \sum_{i = 1}^{N} \sum_{j = 1}^{N} \alpha_i \alpha_j y_i y_j k_{ij}\\
\text{s.t. } &amp;\\
&amp; \sum_{i = 1}^{N} \alpha_i y_i = 0\\
&amp; 0 \le \alpha_i \le C, \text{  } i = 1, \cdots, N
\end{split}
\tag{4.8}
\end{equation}\]</span>
<p>이 때 <span class="math inline">\(k_{ij}\)</span>는 <span class="math inline">\(K(\mathbf{x}_i, \mathbf{x}_j)\)</span>를 나타낸다. 식 <a href="svm.html#eq:nonlinear-svm-dual-kernel">(4.8)</a>의 최적해 <span class="math inline">\(\boldsymbol\alpha^*\)</span>는 선형 SVM과 마찬가지로 이차계획(quadratic programming) 소프트웨어/알고리즘을 이용하여 구할 수 있다.</p>
<p>Table <a href="svm.html#tab:nonlinear-svm-train-data">4.6</a>의 학습데이터에 대해 <code>e1071</code> 패키지의 <code>svm</code> 함수를 이용하여 이차 다항 커널에 기반한 분리 하이퍼플레인을 구해보자. <code>svm</code> 함수에 파라미터값 <code>kernel = &quot;polynomial&quot;</code>를 설정함으로써 다항 커널을 사용할 수 있다. <code>svm</code> 함수의 다항 커널은 위에서 설명된 것보다 일반화된 형태로 아래와 같이 정의된다.</p>
<span class="math display">\[\begin{equation*}
K(\mathbf{x}_i, \mathbf{x}_j) = \left( \gamma \mathbf{x}_i^\top \mathbf{x}_j + \beta_0 \right)^r
\end{equation*}\]</span>
<p>위 커널함수의 파라미터 <span class="math inline">\(\gamma, \beta_0, r\)</span>은 <code>svm</code> 함수에 파라미터 <code>gamma, coef0, degree</code>로 각각 정의된다. 따라서 이차 커널</p>
<span class="math display">\[\begin{equation*}
K(\mathbf{x}_i, \mathbf{x}_j) = \left( \mathbf{x}_i^\top \mathbf{x}_j + 1 \right)^2
\end{equation*}\]</span>
<p>에 기반한 SVM을 학습하기 위해서 아래와 같이 <code>svm</code> 함수를 호출한다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">svm_model &lt;-<span class="st"> </span><span class="kw">svm</span>(<span class="kw">as.factor</span>(class) <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> nonlinear_train_df, 
                 <span class="dt">kernel =</span> <span class="st">&quot;polynomial&quot;</span>, <span class="dt">coef0 =</span> <span class="dv">1</span>, <span class="dt">gamma =</span> <span class="dv">1</span>, <span class="dt">degree =</span> <span class="dv">2</span>,
                 <span class="dt">scale =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p>위 함수 호출 결과 서포트 벡터 객체는 1, 7, 2, 3, 9이다.</p>
<p>비선형 SVM의 분리 하이퍼플레인 또한 페널티 단가 <span class="math inline">\(C\)</span>의 값에 따라 달라진다. 선형 SVM의 경우와 같이 <span class="math inline">\(C = 1, 5, 100\)</span>에 대해 각각 비선형 SVM을 구해보자.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">svm_models &lt;-<span class="st"> </span><span class="kw">lapply</span>(
  <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">100</span>),
  <span class="cf">function</span>(C)
    <span class="kw">svm</span>(<span class="kw">as.factor</span>(class) <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> nonlinear_train_df,
        <span class="dt">kernel =</span> <span class="st">&quot;polynomial&quot;</span>, <span class="dt">coef0 =</span> <span class="dv">1</span>, <span class="dt">gamma =</span> <span class="dv">1</span>, <span class="dt">degree =</span> <span class="dv">2</span>,
        <span class="dt">cost =</span> C, <span class="dt">scale =</span> <span class="ot">FALSE</span>)
)

getSummary &lt;-<span class="st"> </span><span class="cf">function</span>(model) {
  <span class="kw">list</span>(<span class="dt">C =</span> model<span class="op">$</span>cost,
       <span class="dt">sv =</span> <span class="kw">paste</span>(model<span class="op">$</span>index, <span class="dt">collapse =</span> <span class="st">&quot;, &quot;</span>),
       <span class="dt">misclassified =</span> <span class="kw">paste</span>(<span class="kw">which</span>(model<span class="op">$</span>fitted <span class="op">!=</span><span class="st"> </span><span class="kw">as.factor</span>(nonlinear_train_df<span class="op">$</span>class)), <span class="dt">collapse =</span> <span class="st">&quot;, &quot;</span>))
}

svm_summary &lt;-<span class="st"> </span><span class="kw">lapply</span>(svm_models, getSummary) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_rows</span>()</code></pre></div>
<table>
<caption><span id="tab:nonlinear-svm-summary">Table 4.7: </span>페널티 단가 C에 따른 비선형 SVM 결과</caption>
<thead>
<tr class="header">
<th align="center">페널티 단가 <span class="math inline">\(C\)</span></th>
<th align="center">서포트 벡터 객체</th>
<th align="center">오분류 객체</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">1, 7, 2, 3, 9</td>
<td align="center">7</td>
</tr>
<tr class="even">
<td align="center">5</td>
<td align="center">6, 7, 2, 3, 9</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">100</td>
<td align="center">6, 7, 2, 3, 9</td>
<td align="center"></td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="svm-r-pkg" class="section level2">
<h2><span class="header-section-number">4.6</span> R패키지 내 SVM</h2>
<div id="svm-kernel-function" class="section level3">
<h3><span class="header-section-number">4.6.1</span> 커널함수</h3>
<p>앞 장에서는 선형 커널과 다항 커널함수의 예를 살펴보았다. 본 장에서는 가우시안 커널 및 시그모이드 커널을 사용하는 법을 살펴보자.</p>
<p>가우시안 커널의 경우</p>
<span class="math display">\[\begin{equation*}
K(\mathbf{x}_i, \mathbf{x}_j) = \exp \left( -\gamma \left\lVert \mathbf{x}_i - \mathbf{x}_j \right\rVert^2 \right)
\end{equation*}\]</span>
<p>과 같이 <span class="math inline">\(\gamma\)</span> 파라미터를 이용하여 함수를 정의하며, <code>svm</code> 함수에 <code>gamma</code> 파라미터값을 통해 설정할 수 있다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">svm_model &lt;-<span class="st"> </span><span class="kw">svm</span>(<span class="kw">as.factor</span>(class) <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> nonlinear_train_df, 
                 <span class="dt">kernel =</span> <span class="st">&quot;radial&quot;</span>, <span class="dt">gamma =</span> <span class="dv">1</span>,
                 <span class="dt">cost =</span> <span class="dv">5</span>, <span class="dt">scale =</span> <span class="ot">FALSE</span>)
<span class="kw">plot</span>(svm_model, <span class="dt">data =</span> nonlinear_train_df, <span class="dt">formula =</span> x2 <span class="op">~</span><span class="st"> </span>x1, <span class="dt">grid =</span> <span class="dv">200</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:nonlinear-svm-radial"></span>
<img src="data-mining-book_files/figure-html/nonlinear-svm-radial-1.png" alt="가우시안 커널을 이용한 비선형 SVM 하이퍼플레인" width="672" />
<p class="caption">
Figure 4.5: 가우시안 커널을 이용한 비선형 SVM 하이퍼플레인
</p>
</div>
<p>시그모이드 커널의 경우</p>
<span class="math display">\[\begin{equation*}
K(\mathbf{x}_i, \mathbf{x}_j) = \tanh \left(\gamma \mathbf{x}_i^\top \mathbf{x}_j + \beta_0 \right)
\end{equation*}\]</span>
<p>와 같이 두 파라미터 <span class="math inline">\(\gamma, \beta_0\)</span>의 값에 대응하는 <code>svm</code> 함수의 파라미터 <code>gamma, coef0</code> 값을 통해 설정할 수 있다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">svm_model &lt;-<span class="st"> </span><span class="kw">svm</span>(<span class="kw">as.factor</span>(class) <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> nonlinear_train_df, 
                 <span class="dt">kernel =</span> <span class="st">&quot;sigmoid&quot;</span>, <span class="dt">gamma =</span> <span class="fl">0.01</span>, <span class="dt">coef0 =</span> <span class="op">-</span><span class="dv">1</span>,
                 <span class="dt">cost =</span> <span class="dv">5</span>, <span class="dt">scale =</span> <span class="ot">FALSE</span>)
<span class="kw">plot</span>(svm_model, <span class="dt">data =</span> nonlinear_train_df, <span class="dt">formula =</span> x2 <span class="op">~</span><span class="st"> </span>x1, <span class="dt">grid =</span> <span class="dv">200</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:nonlinear-svm-sigmoid"></span>
<img src="data-mining-book_files/figure-html/nonlinear-svm-sigmoid-1.png" alt="시그모이드 커널을 이용한 비선형 SVM 하이퍼플레인" width="672" />
<p class="caption">
Figure 4.6: 시그모이드 커널을 이용한 비선형 SVM 하이퍼플레인
</p>
</div>
<p>커널 함수의 종류 <code>kernel</code>, 커널 함수의 파라미터 <code>gamma, coef0, degree</code>, 페널티 단가 <code>cost</code>등의 <code>svm</code> 함수 파라미터는 학습 표본과는 별도의 테스트 데이터에 대해 오분류율을 최소화하는 값을 선택하는 것이 일반적이다.</p>
</div>
<div id="svm-nu-classification" class="section level3">
<h3><span class="header-section-number">4.6.2</span> <span class="math inline">\(\nu\)</span>-SVC</h3>
<p><span class="math inline">\(\nu\)</span>-support vector classification(<span class="math inline">\(\nu\)</span>-SVC) <span class="citation">(Schölkopf et al. <a href="#ref-scholkopf2000new">2000</a>, <span class="citation">Chang and Lin (<a href="#ref-chang2001training">2001</a>)</span>)</span>은 <span class="math inline">\(C\)</span>-SVC의 이차계획식 <a href="svm.html#eq:nonlinear-svm-dual-kernel">(4.8)</a>과 다른 형태로, 페널티 단가 <span class="math inline">\(C\)</span> 대신 <span class="math inline">\(\nu\)</span>라는 파라미터를 이용한 아래 최적화 문제의 해를 구한다.</p>
<span class="math display" id="eq:nonlinear-nu-svc-dual">\[\begin{equation}
\begin{split}
\min \text{  } &amp; L_D = \sum_{i = 1}^{N} \sum_{j = 1}^{N} \alpha_i \alpha_j y_i y_j k_{ij}\\
\text{s.t. } &amp;\\
&amp; \sum_{i = 1}^{N} \alpha_i y_i = 0\\
&amp; 0 \le \alpha_i \le \frac{1}{N}, \text{  } i = 1, \cdots, N\\
&amp; \sum_{i = 1}^{N} \alpha_i = \nu
\end{split}
\tag{4.9}
\end{equation}\]</span>
<p>이 때, 각 <span class="math inline">\(\alpha_i\)</span>의 최대값은 <span class="math inline">\(1/N\)</span>으로, <span class="math inline">\(\nu\)</span>를 포함한 제약식을 무시할 때 모든 객체에 대한 <span class="math inline">\(\alpha_i\)</span>값의 합의 이론적 최대치는 1이 되며, <span class="math inline">\(\nu \in (0, 1]\)</span>은 전체 객체 중 서포트 벡터 객체의 개수를 제한하는 개념으로 생각할 수 있다. 식 <a href="svm.html#eq:nonlinear-nu-svc-dual">(4.9)</a>이 실제로 최적해를 가지기 위한 <span class="math inline">\(\nu\)</span>값의 범위는</p>
<span class="math display">\[\begin{equation*}
0 &lt; \nu \le \frac{2}{N} \min \left( \sum_i I(y_i = 1), \sum_i I(y_i = -1) \right)
\end{equation*}\]</span>
<p>으로 <span class="citation">(Chang and Lin <a href="#ref-chang2001training">2001</a>)</span>, 에를 들어 범주 1에 속하는 학습표본 객체 수가 전체의 10% 라면, <span class="math inline">\(\nu\)</span> 값은 최대 0.2 까지 설정할 수 있다. 또한</p>
<p><code>svm</code> 함수가 호출하는 <code>LIBSVM</code> 라이브러리는 위 식 <a href="svm.html#eq:nonlinear-nu-svc-dual">(4.9)</a>을 <span class="math inline">\(N\)</span>이 큰(학습 표본 수가 매우 많은) 경우에도 안정된 결과를 얻을 수 있도록 아래와 같이 변환한 문제를 다룬다.</p>
<span class="math display" id="eq:libsvm-nu-svc-dual">\[\begin{equation}
\begin{split}
\min \text{  } &amp; L_D = \sum_{i = 1}^{N} \sum_{j = 1}^{N} \bar{\alpha}_i \bar{\alpha}_j y_i y_j k_{ij}\\
\text{s.t. } &amp;\\
&amp; \sum_{i = 1}^{N} \bar{\alpha}_i y_i = 0\\
&amp; 0 \le \bar{\alpha}_i \le 1, \text{  } i = 1, \cdots, N\\
&amp; \sum_{i = 1}^{N} \bar{\alpha}_i = \nu N
\end{split}
\tag{4.10}
\end{equation}\]</span>
<p>이 때 <span class="math inline">\(\bar{\alpha}_i = \alpha_i N\)</span>이다.</p>
<p><span class="math inline">\(\nu\)</span>-SVC은 아래와 같이 <code>svm</code> 함수를 호출할 때 <code>type = &quot;nu-classification&quot;</code>과 파라미터 <code>nu</code> 값을 설정함으로써 학습할 수 있다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">svm_model &lt;-<span class="st"> </span><span class="kw">svm</span>(<span class="kw">as.factor</span>(class) <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> nonlinear_train_df, 
                 <span class="dt">type =</span> <span class="st">&quot;nu-classification&quot;</span>, 
                 <span class="dt">kernel =</span> <span class="st">&quot;radial&quot;</span>, <span class="dt">gamma =</span> <span class="dv">1</span>,
                 <span class="dt">nu =</span> <span class="fl">0.5</span>, <span class="dt">scale =</span> <span class="ot">FALSE</span>)
<span class="kw">plot</span>(svm_model, <span class="dt">data =</span> nonlinear_train_df, <span class="dt">formula =</span> x2 <span class="op">~</span><span class="st"> </span>x1, <span class="dt">grid =</span> <span class="dv">200</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:nu-svc-radial"></span>
<img src="data-mining-book_files/figure-html/nu-svc-radial-1.png" alt="가우시안 커널을 이용한 $\nu$-SVC 하이퍼플레인" width="672" />
<p class="caption">
Figure 4.7: 가우시안 커널을 이용한 <span class="math inline">\(\nu\)</span>-SVC 하이퍼플레인
</p>
</div>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-jun2012datamining">
<p>전치혁. 2012. <em>데이터마이닝 기법과 응용</em>. 한나래출판사.</p>
</div>
<div id="ref-goldfarb1983numerically">
<p>Goldfarb, Donald, and Ashok Idnani. 1983. “A Numerically Stable Dual Method for Solving Strictly Convex Quadratic Programs.” <em>Mathematical Programming</em> 27 (1). Springer: 1–33.</p>
</div>
<div id="ref-chang2011libsvm">
<p>Chang, Chih-Chung, and Chih-Jen Lin. 2011. “LIBSVM: A Library for Support Vector Machines.” <em>ACM Transactions on Intelligent Systems and Technology (TIST)</em> 2 (3). Acm: 27.</p>
</div>
<div id="ref-fan2005working">
<p>Fan, Rong-En, Pai-Hsuen Chen, and Chih-Jen Lin. 2005. “Working Set Selection Using Second Order Information for Training Support Vector Machines.” <em>Journal of Machine Learning Research</em> 6 (Dec): 1889–1918.</p>
</div>
<div id="ref-scholkopf2000new">
<p>Schölkopf, Bernhard, Alex J Smola, Robert C Williamson, and Peter L Bartlett. 2000. “New Support Vector Algorithms.” <em>Neural Computation</em> 12 (5). MIT Press: 1207–45.</p>
</div>
<div id="ref-chang2001training">
<p>Chang, Chih-Chung, and Chih-Jen Lin. 2001. “Training V-Support Vector Classifiers: Theory and Algorithms.” <em>Neural Computation</em> 13 (9). MIT Press: 2119–47.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tree-based-method.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="clustering-overview.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
