<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 1 분류분석 개요 | 데이터마이닝 with R</title>
  <meta name="description" content="전치혁 교수님의 책 을 기반으로 한 R 예제">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 1 분류분석 개요 | 데이터마이닝 with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://youngroklee-ml.github.io/data-mining-book/" />
  
  <meta property="og:description" content="전치혁 교수님의 책 을 기반으로 한 R 예제" />
  <meta name="github-repo" content="youngroklee-ml/data-mining-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 분류분석 개요 | 데이터마이닝 with R" />
  
  <meta name="twitter:description" content="전치혁 교수님의 책 을 기반으로 한 R 예제" />
  

<meta name="author" content="전치혁, 이혜선, 이종석, 이영록">


<meta name="date" content="2019-04-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="logistic-regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">데이터마이닝 with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>개요</a></li>
<li class="chapter" data-level="1" data-path="classification-analysis.html"><a href="classification-analysis.html"><i class="fa fa-check"></i><b>1</b> 분류분석 개요</a><ul>
<li class="chapter" data-level="1.1" data-path="classification-analysis.html"><a href="classification-analysis.html#classification-packages-install"><i class="fa fa-check"></i><b>1.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="1.2" data-path="classification-analysis.html"><a href="classification-analysis.html#classification-problem-methods"><i class="fa fa-check"></i><b>1.2</b> 분류문제 및 분류기법</a></li>
<li class="chapter" data-level="1.3" data-path="classification-analysis.html"><a href="classification-analysis.html#simple-classification-methods"><i class="fa fa-check"></i><b>1.3</b> 기본적인 분류기법</a><ul>
<li class="chapter" data-level="1.3.1" data-path="classification-analysis.html"><a href="classification-analysis.html#nearest-neighbor-classification"><i class="fa fa-check"></i><b>1.3.1</b> 인접객체법</a></li>
<li class="chapter" data-level="1.3.2" data-path="classification-analysis.html"><a href="classification-analysis.html#naive-bayes"><i class="fa fa-check"></i><b>1.3.2</b> 나이브 베이지안 분류법</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>2</b> 로지스틱 회귀분석</a><ul>
<li class="chapter" data-level="2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-packages-install"><i class="fa fa-check"></i><b>2.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#binary-logistic-regression"><i class="fa fa-check"></i><b>2.2</b> 이분 로지스틱 회귀모형</a><ul>
<li class="chapter" data-level="2.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#bianry-logistic-reg-basic-script"><i class="fa fa-check"></i><b>2.2.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="2.2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#binary-logistic-reg-model"><i class="fa fa-check"></i><b>2.2.2</b> 회귀모형</a></li>
<li class="chapter" data-level="2.2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#binary-logistic-reg-estimation"><i class="fa fa-check"></i><b>2.2.3</b> 회귀계수 추정</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#nominal-logistic-regression"><i class="fa fa-check"></i><b>2.3</b> 명목 로지스틱 회귀모형</a><ul>
<li class="chapter" data-level="2.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#nominal-logistic-reg-basic-script"><i class="fa fa-check"></i><b>2.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="2.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#baseline-category-logit-model"><i class="fa fa-check"></i><b>2.3.2</b> 기준범주 로짓모형</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="logistic-regression.html"><a href="logistic-regression.html#ordinal-logistic-regression"><i class="fa fa-check"></i><b>2.4</b> 서열 로지스틱 회귀모형</a><ul>
<li class="chapter" data-level="2.4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#ordinal-logistic-basic-script"><i class="fa fa-check"></i><b>2.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="2.4.2" data-path="logistic-regression.html"><a href="logistic-regression.html#cumulative-logit-model"><i class="fa fa-check"></i><b>2.4.2</b> 누적 로짓모형</a></li>
<li class="chapter" data-level="2.4.3" data-path="logistic-regression.html"><a href="logistic-regression.html#adjacent-categories-logit-model"><i class="fa fa-check"></i><b>2.4.3</b> 인근범주 로짓모형</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="da.html"><a href="da.html"><i class="fa fa-check"></i><b>3</b> 판별분석</a><ul>
<li class="chapter" data-level="3.1" data-path="da.html"><a href="da.html#da-overview"><i class="fa fa-check"></i><b>3.1</b> 개요</a></li>
<li class="chapter" data-level="3.2" data-path="da.html"><a href="da.html#da-packages-install"><i class="fa fa-check"></i><b>3.2</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="3.3" data-path="da.html"><a href="da.html#da-fisher"><i class="fa fa-check"></i><b>3.3</b> 피셔 방법</a><ul>
<li class="chapter" data-level="3.3.1" data-path="da.html"><a href="da.html#da-fisher-basic-script"><i class="fa fa-check"></i><b>3.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="3.3.2" data-path="da.html"><a href="da.html#-"><i class="fa fa-check"></i><b>3.3.2</b> 피셔 판별함수</a></li>
<li class="chapter" data-level="3.3.3" data-path="da.html"><a href="da.html#-"><i class="fa fa-check"></i><b>3.3.3</b> 분류 규칙</a></li>
<li class="chapter" data-level="3.3.4" data-path="da.html"><a href="da.html#r----"><i class="fa fa-check"></i><b>3.3.4</b> R 패키지를 이용한 분류규칙 도출</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="da.html"><a href="da.html#lda"><i class="fa fa-check"></i><b>3.4</b> 의사결정론에 의한 선형분류규칙</a><ul>
<li class="chapter" data-level="3.4.1" data-path="da.html"><a href="da.html#lda-basic-script"><i class="fa fa-check"></i><b>3.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="3.4.2" data-path="da.html"><a href="da.html#lda-function"><i class="fa fa-check"></i><b>3.4.2</b> 선형판별함수</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="da.html"><a href="da.html#lda-misclassification-cost"><i class="fa fa-check"></i><b>3.5</b> 오분류비용을 고려한 분류규칙</a></li>
<li class="chapter" data-level="3.6" data-path="da.html"><a href="da.html#qda"><i class="fa fa-check"></i><b>3.6</b> 이차판별분석</a><ul>
<li class="chapter" data-level="3.6.1" data-path="da.html"><a href="da.html#qda-basic-script"><i class="fa fa-check"></i><b>3.6.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="3.6.2" data-path="da.html"><a href="da.html#qda-function"><i class="fa fa-check"></i><b>3.6.2</b> 이차 판별함수</a></li>
<li class="chapter" data-level="3.6.3" data-path="da.html"><a href="da.html#qda-discriminant-rule"><i class="fa fa-check"></i><b>3.6.3</b> 이차판별함수에 의한 분류</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="da.html"><a href="da.html#da-multiclass"><i class="fa fa-check"></i><b>3.7</b> 세 범주 이상의 분류</a><ul>
<li class="chapter" data-level="3.7.1" data-path="da.html"><a href="da.html#mutliclass-da-basic-script"><i class="fa fa-check"></i><b>3.7.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="3.7.2" data-path="da.html"><a href="da.html#mutliclass-generalized-discriminant-function"><i class="fa fa-check"></i><b>3.7.2</b> 일반화된 판별함수</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tree-based-method.html"><a href="tree-based-method.html"><i class="fa fa-check"></i><b>4</b> 트리기반 기법</a><ul>
<li class="chapter" data-level="4.1" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-overview"><i class="fa fa-check"></i><b>4.1</b> CART 개요</a></li>
<li class="chapter" data-level="4.2" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-packages-install"><i class="fa fa-check"></i><b>4.2</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="4.3" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-build"><i class="fa fa-check"></i><b>4.3</b> CART 트리 생성</a><ul>
<li class="chapter" data-level="4.3.1" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-basic-r-script"><i class="fa fa-check"></i><b>4.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="4.3.2" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-notation"><i class="fa fa-check"></i><b>4.3.2</b> 기호 정의</a></li>
<li class="chapter" data-level="4.3.3" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-impurity"><i class="fa fa-check"></i><b>4.3.3</b> 노드 및 트리의 불순도</a></li>
<li class="chapter" data-level="4.3.4" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-split"><i class="fa fa-check"></i><b>4.3.4</b> 분지기준</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-pruning-complete"><i class="fa fa-check"></i><b>4.4</b> 가지치기 및 최종 트리 선정</a><ul>
<li class="chapter" data-level="4.4.1" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-pruning"><i class="fa fa-check"></i><b>4.4.1</b> 가지치기</a></li>
<li class="chapter" data-level="4.4.2" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-best-tree"><i class="fa fa-check"></i><b>4.4.2</b> 최적 트리의 선정</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-r-pkg"><i class="fa fa-check"></i><b>4.5</b> R패키지 내 분류 트리 방법</a><ul>
<li class="chapter" data-level="4.5.1" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-r-pkg-split"><i class="fa fa-check"></i><b>4.5.1</b> 트리 확장</a></li>
<li class="chapter" data-level="4.5.2" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-r-pkg-pruning"><i class="fa fa-check"></i><b>4.5.2</b> 가지치기</a></li>
<li class="chapter" data-level="4.5.3" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-r-pkg-param"><i class="fa fa-check"></i><b>4.5.3</b> 파라미터값 결정</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>5</b> 서포트 벡터 머신</a><ul>
<li class="chapter" data-level="5.1" data-path="svm.html"><a href="svm.html#svm-overview"><i class="fa fa-check"></i><b>5.1</b> 개요</a></li>
<li class="chapter" data-level="5.2" data-path="svm.html"><a href="svm.html#svm-packages-install"><i class="fa fa-check"></i><b>5.2</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="5.3" data-path="svm.html"><a href="svm.html#linear-svm-separable"><i class="fa fa-check"></i><b>5.3</b> 선형 SVM - 분리 가능 경우</a><ul>
<li class="chapter" data-level="5.3.1" data-path="svm.html"><a href="svm.html#linear-svm-separable-basic-script"><i class="fa fa-check"></i><b>5.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="5.3.2" data-path="svm.html"><a href="svm.html#linear-svm-notation"><i class="fa fa-check"></i><b>5.3.2</b> 기호 정의</a></li>
<li class="chapter" data-level="5.3.3" data-path="svm.html"><a href="svm.html#linear-svm-separable-hyperplane"><i class="fa fa-check"></i><b>5.3.3</b> 최적 하이퍼플레인</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="svm.html"><a href="svm.html#linear-svm-inseparable"><i class="fa fa-check"></i><b>5.4</b> 선형 SVM - 분리 불가능 경우</a><ul>
<li class="chapter" data-level="5.4.1" data-path="svm.html"><a href="svm.html#linear-svm-inseparable-basic-script"><i class="fa fa-check"></i><b>5.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="5.4.2" data-path="svm.html"><a href="svm.html#linear-svm-inseparable-hyperplane"><i class="fa fa-check"></i><b>5.4.2</b> 최적 하이퍼플레인</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="svm.html"><a href="svm.html#nonlinear-svm"><i class="fa fa-check"></i><b>5.5</b> 비선형 SVM</a><ul>
<li class="chapter" data-level="5.5.1" data-path="svm.html"><a href="svm.html#nonlinear-svm-basic-script"><i class="fa fa-check"></i><b>5.5.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="5.5.2" data-path="svm.html"><a href="svm.html#nonlinear-svm-hyperplane"><i class="fa fa-check"></i><b>5.5.2</b> 최적 하이퍼플레인</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="svm.html"><a href="svm.html#svm-r-pkg"><i class="fa fa-check"></i><b>5.6</b> R패키지 내 SVM</a><ul>
<li class="chapter" data-level="5.6.1" data-path="svm.html"><a href="svm.html#svm-kernel-function"><i class="fa fa-check"></i><b>5.6.1</b> 커널함수</a></li>
<li class="chapter" data-level="5.6.2" data-path="svm.html"><a href="svm.html#svm-nu-classification"><i class="fa fa-check"></i><b>5.6.2</b> <span class="math inline">\(\nu\)</span>-SVC</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classifier-evaluation.html"><a href="classifier-evaluation.html"><i class="fa fa-check"></i><b>6</b> 분류규칙의 성능 평가</a><ul>
<li class="chapter" data-level="6.1" data-path="classifier-evaluation.html"><a href="classifier-evaluation.html#classifier-evaluation-packages-install"><i class="fa fa-check"></i><b>6.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="6.2" data-path="classifier-evaluation.html"><a href="classifier-evaluation.html#classifier-evaluation-misclassification-rate"><i class="fa fa-check"></i><b>6.2</b> 분류오류율</a></li>
<li class="chapter" data-level="6.3" data-path="classifier-evaluation.html"><a href="classifier-evaluation.html#precision-sensitivity-specificity"><i class="fa fa-check"></i><b>6.3</b> 정확도, 민감도 및 특이도</a><ul>
<li class="chapter" data-level="6.3.1" data-path="classifier-evaluation.html"><a href="classifier-evaluation.html#confusion-matrix-r-package"><i class="fa fa-check"></i><b>6.3.1</b> R 패키지 내 정오분류표</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="classifier-evaluation.html"><a href="classifier-evaluation.html#roc-curve"><i class="fa fa-check"></i><b>6.4</b> ROC 곡선</a></li>
<li class="chapter" data-level="6.5" data-path="classifier-evaluation.html"><a href="classifier-evaluation.html#gain-chart"><i class="fa fa-check"></i><b>6.5</b> 이익도표</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="clustering-overview.html"><a href="clustering-overview.html"><i class="fa fa-check"></i><b>7</b> 군집분석 개요</a><ul>
<li class="chapter" data-level="7.1" data-path="clustering-overview.html"><a href="clustering-overview.html#clustering-overview-packages-install"><i class="fa fa-check"></i><b>7.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="7.2" data-path="clustering-overview.html"><a href="clustering-overview.html#clustering-method"><i class="fa fa-check"></i><b>7.2</b> 군집분석 기법</a></li>
<li class="chapter" data-level="7.3" data-path="clustering-overview.html"><a href="clustering-overview.html#object-similarity-metric"><i class="fa fa-check"></i><b>7.3</b> 객체 간의 유사성 척도</a><ul>
<li class="chapter" data-level="7.3.1" data-path="clustering-overview.html"><a href="clustering-overview.html#object-distance-metric"><i class="fa fa-check"></i><b>7.3.1</b> 거리 관련 척도</a></li>
<li class="chapter" data-level="7.3.2" data-path="clustering-overview.html"><a href="clustering-overview.html#object-correlation-metric"><i class="fa fa-check"></i><b>7.3.2</b> 상관계수 관련 척도</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="clustering-overview.html"><a href="clustering-overview.html#category-similarity-metric"><i class="fa fa-check"></i><b>7.4</b> 범주형 객체의 유사성 척도</a><ul>
<li class="chapter" data-level="7.4.1" data-path="clustering-overview.html"><a href="clustering-overview.html#binary-similarity-metric"><i class="fa fa-check"></i><b>7.4.1</b> 이분형 변수의 경우</a></li>
<li class="chapter" data-level="7.4.2" data-path="clustering-overview.html"><a href="clustering-overview.html#ordinal-similarity-metric"><i class="fa fa-check"></i><b>7.4.2</b> 서열형 변수의 경우</a></li>
<li class="chapter" data-level="7.4.3" data-path="clustering-overview.html"><a href="clustering-overview.html#nominal-similarity-metric"><i class="fa fa-check"></i><b>7.4.3</b> 명목형 변수의 경우</a></li>
<li class="chapter" data-level="7.4.4" data-path="clustering-overview.html"><a href="clustering-overview.html#mixed-similarity-metric"><i class="fa fa-check"></i><b>7.4.4</b> 혼합형의 경우</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>8</b> 계층적 군집방법</a><ul>
<li class="chapter" data-level="8.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#hierarchical-clustering-packages-install"><i class="fa fa-check"></i><b>8.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="8.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#distance-between-clusters"><i class="fa fa-check"></i><b>8.2</b> 군집 간 거리척도 및 연결법</a></li>
<li class="chapter" data-level="8.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#linkage-method"><i class="fa fa-check"></i><b>8.3</b> 연결법의 군집 알고리즘</a><ul>
<li class="chapter" data-level="8.3.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#linkage-method-basic-script"><i class="fa fa-check"></i><b>8.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="8.3.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#linkage-method-algorithm"><i class="fa fa-check"></i><b>8.3.2</b> 연결법 군집 알고리즘</a></li>
<li class="chapter" data-level="8.3.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#hclust"><i class="fa fa-check"></i><b>8.3.3</b> R 패키지 내 연결법</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#ward-method"><i class="fa fa-check"></i><b>8.4</b> 워드 방법</a><ul>
<li class="chapter" data-level="8.4.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#ward-method-basic-script"><i class="fa fa-check"></i><b>8.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="8.4.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#ward-method-algorithm"><i class="fa fa-check"></i><b>8.4.2</b> 워드 군집 알고리즘</a></li>
<li class="chapter" data-level="8.4.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#ward-rpackages"><i class="fa fa-check"></i><b>8.4.3</b> R 패키지 내 워드 방법</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#diana"><i class="fa fa-check"></i><b>8.5</b> 분리적 방법 - 다이아나</a><ul>
<li class="chapter" data-level="8.5.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#diana-basic-script"><i class="fa fa-check"></i><b>8.5.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="8.5.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#diana-algorithm"><i class="fa fa-check"></i><b>8.5.2</b> 다이아나 알고리즘</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#hierarchical-cluster-number"><i class="fa fa-check"></i><b>8.6</b> 군집수의 결정</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html"><i class="fa fa-check"></i><b>9</b> 비계층적 군집방법</a><ul>
<li class="chapter" data-level="9.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#nonhierarchical-clustering-packages-install"><i class="fa fa-check"></i><b>9.1</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="9.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans"><i class="fa fa-check"></i><b>9.2</b> K-means 알고리즘</a><ul>
<li class="chapter" data-level="9.2.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans-basic-script"><i class="fa fa-check"></i><b>9.2.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="9.2.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans-algorithm"><i class="fa fa-check"></i><b>9.2.2</b> 알고리즘</a></li>
<li class="chapter" data-level="9.2.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans-user-defined-functions"><i class="fa fa-check"></i><b>9.2.3</b> R 스크립트 구현</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmedoids"><i class="fa fa-check"></i><b>9.3</b> K-medoids 군집방법</a><ul>
<li class="chapter" data-level="9.3.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#pam"><i class="fa fa-check"></i><b>9.3.1</b> PAM 알고리즘</a></li>
<li class="chapter" data-level="9.3.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#clara"><i class="fa fa-check"></i><b>9.3.2</b> CLARA 알고리즘</a></li>
<li class="chapter" data-level="9.3.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#clarans"><i class="fa fa-check"></i><b>9.3.3</b> CLARANS 알고리즘</a></li>
<li class="chapter" data-level="9.3.4" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans-like"><i class="fa fa-check"></i><b>9.3.4</b> K-means-like 알고리즘</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#fuzzy-kmeans"><i class="fa fa-check"></i><b>9.4</b> 퍼지 K-means 알고리즘</a><ul>
<li class="chapter" data-level="9.4.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#fuzzy-kmeans-basic-script"><i class="fa fa-check"></i><b>9.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="9.4.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#fuzzy-kmeans-algorithm"><i class="fa fa-check"></i><b>9.4.2</b> 알고리즘</a></li>
<li class="chapter" data-level="9.4.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#fuzzy-kmeans-script-implement"><i class="fa fa-check"></i><b>9.4.3</b> R 스크립트 구현</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#model-based-clustering"><i class="fa fa-check"></i><b>9.5</b> 모형기반 군집방법</a><ul>
<li class="chapter" data-level="9.5.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#model-based-clustering-basic-script"><i class="fa fa-check"></i><b>9.5.1</b> 기본 R script</a></li>
<li class="chapter" data-level="9.5.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#model-based-clustering-em"><i class="fa fa-check"></i><b>9.5.2</b> EM 알고리즘</a></li>
<li class="chapter" data-level="9.5.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#model-based-clustering-script-implement"><i class="fa fa-check"></i><b>9.5.3</b> R 스크립트 구현</a></li>
<li class="chapter" data-level="9.5.4" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#r-packages-model-based-clustering"><i class="fa fa-check"></i><b>9.5.4</b> R 패키지 내 모형기반 군집분석</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html"><i class="fa fa-check"></i><b>10</b> 군집해의 평가 및 해석</a><ul>
<li class="chapter" data-level="10.1" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html#cluster-solution-evaluation-packages-install"><i class="fa fa-check"></i><b>10.1</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="10.2" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html#cluster-solution-evaluation-metric"><i class="fa fa-check"></i><b>10.2</b> 군집해의 평가</a><ul>
<li class="chapter" data-level="10.2.1" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html#cluster-evaluation-external-index"><i class="fa fa-check"></i><b>10.2.1</b> 외부평가지수</a></li>
<li class="chapter" data-level="10.2.2" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html#cluster-evaluation-internal-index"><i class="fa fa-check"></i><b>10.2.2</b> 내부평가지수</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html#cluster-solution-interpretation"><i class="fa fa-check"></i><b>10.3</b> 군집해의 해석</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="association-rule.html"><a href="association-rule.html"><i class="fa fa-check"></i><b>11</b> 연관규칙</a><ul>
<li class="chapter" data-level="11.1" data-path="association-rule.html"><a href="association-rule.html#association-packages-install"><i class="fa fa-check"></i><b>11.1</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="11.2" data-path="association-rule.html"><a href="association-rule.html#association-rule-definition-metric"><i class="fa fa-check"></i><b>11.2</b> 연관규칙의 정의 및 성능척도</a><ul>
<li class="chapter" data-level="11.2.1" data-path="association-rule.html"><a href="association-rule.html#association-rule-support"><i class="fa fa-check"></i><b>11.2.1</b> 지지도</a></li>
<li class="chapter" data-level="11.2.2" data-path="association-rule.html"><a href="association-rule.html#association-rule-confidence"><i class="fa fa-check"></i><b>11.2.2</b> 신뢰도</a></li>
<li class="chapter" data-level="11.2.3" data-path="association-rule.html"><a href="association-rule.html#association-rule-lift"><i class="fa fa-check"></i><b>11.2.3</b> 개선도</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="association-rule.html"><a href="association-rule.html#association-rule-exploration"><i class="fa fa-check"></i><b>11.3</b> 연관규칙의 탐사</a><ul>
<li class="chapter" data-level="11.3.1" data-path="association-rule.html"><a href="association-rule.html#apriori-large-itemsets"><i class="fa fa-check"></i><b>11.3.1</b> 빈발항목집합 생성</a></li>
<li class="chapter" data-level="11.3.2" data-path="association-rule.html"><a href="association-rule.html#apriori-rule-exploration"><i class="fa fa-check"></i><b>11.3.2</b> 규칙의 탐사</a></li>
<li class="chapter" data-level="11.3.3" data-path="association-rule.html"><a href="association-rule.html#apriori-r-package"><i class="fa fa-check"></i><b>11.3.3</b> R 패키지 내 Apriori</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="association-rule.html"><a href="association-rule.html#association-sequential-pattern"><i class="fa fa-check"></i><b>11.4</b> 순차적 패턴의 탐사</a><ul>
<li class="chapter" data-level="11.4.1" data-path="association-rule.html"><a href="association-rule.html#association-aprioriall"><i class="fa fa-check"></i><b>11.4.1</b> AprioriAll 알고리즘</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="recommender-system.html"><a href="recommender-system.html"><i class="fa fa-check"></i><b>12</b> 추천시스템</a><ul>
<li class="chapter" data-level="12.1" data-path="recommender-system.html"><a href="recommender-system.html#recommender-packages-install"><i class="fa fa-check"></i><b>12.1</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="12.2" data-path="recommender-system.html"><a href="recommender-system.html#content-based-recommender"><i class="fa fa-check"></i><b>12.2</b> 내용기반 추천시스템</a></li>
<li class="chapter" data-level="12.3" data-path="recommender-system.html"><a href="recommender-system.html#collaborative-filtering"><i class="fa fa-check"></i><b>12.3</b> 협업 필터링</a></li>
<li class="chapter" data-level="12.4" data-path="recommender-system.html"><a href="recommender-system.html#market-basket"><i class="fa fa-check"></i><b>12.4</b> 시장바구니 데이터를 이용한 협업 필터링</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">데이터마이닝 with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification-analysis" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> 분류분석 개요</h1>
<p>분류분석(classification analysis)은 다수의 속성(attribute) 또는 변수를 갖는 객체(object)를 사전에 정해진 그룹 또는 범주(class, category) 중의 하나로 분류하는 것이다. 예를 들어, 기업의 3개의 재무제표를 기준으로 우량 또는 불량으로 분류하는 것은 범주수가 2이고 변수수가 3인 분류분석 문제가 될 것이다. 이를 위해서는 이미 범주(우량 또는 불량)가 알려진 여러 기업에 대하여 3개의 재무제표 데이터를 수집한 후 효율적인 분류규칙(classification rule)을 만들어야 할 것이다. 여기서 효율적이라 함은 기존 객체를 잘 분류할 뿐만 아니라 새로운 객체 역시 잘 분류함을 의미한다. 분류규칙을 만들기 위해서는 기존의 범주가 알려진 객체 데이터를 수집하여야 하며, 이를 학습표본(learning sample)이라 한다.</p>
<div id="classification-packages-install" class="section level2">
<h2><span class="header-section-number">1.1</span> 필요 R 패키지 설치</h2>
<p>본 장에서 필요한 R 패키지들은 아래와 같다.</p>
<table>
<thead>
<tr class="header">
<th align="left">package</th>
<th align="left">version</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">tidyverse</td>
<td align="left">1.2.1</td>
</tr>
<tr class="even">
<td align="left">stats</td>
<td align="left">3.5.3</td>
</tr>
<tr class="odd">
<td align="left">class</td>
<td align="left">7.3-15</td>
</tr>
</tbody>
</table>
</div>
<div id="classification-problem-methods" class="section level2">
<h2><span class="header-section-number">1.2</span> 분류문제 및 분류기법</h2>
<p>분류문제를 설명하기 위하여 <span class="math inline">\(N\)</span>개의 객체로 이루어진 학습데이터 <span class="math inline">\(\{(\mathbf{x}_i, y_i)\}_{i = 1, \cdots, N}\)</span>를 아래와 같이 정의하자.</p>
<ul>
<li><span class="math inline">\(\mathbf{x}_i\)</span>: <span class="math inline">\(p\)</span>개의 독립변수로 이루어진 <span class="math inline">\(i\)</span>번째 객체의 변수벡터 (<span class="math inline">\(\mathbf{x}_i = [x_{i1} \, x_{i2} \, \cdots \, x_{ip}]^\top\)</span>)</li>
<li><span class="math inline">\(J\)</span>: 총 범주 수</li>
<li><span class="math inline">\(y_i\)</span>: <span class="math inline">\(i\)</span>번째 객체의 범주 변수; <span class="math inline">\(y_i \in \{1, 2, \cdots, J\}\)</span></li>
</ul>
<p>이 때 학습표본을 다음과 같이 나타낼 수 있다.</p>
<p><span class="math display">\[\begin{equation*}
\{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \cdots, (\mathbf{x}_N, y_N)\}
\end{equation*}\]</span></p>
<p>분류문제는 새로운 객체를 범주 중의 하나로 분류하기 위하여 학습표본을 바탕으로 분류규칙을 만드는 것이다. 이 때 분류규칙은 객체의 변수벡터의 함수로 도출되므로 이를 <span class="math inline">\(r(\mathbf{x})\)</span>로 나타낸다. 이 때, <span class="math inline">\(r(\mathbf{x})\)</span>는 <span class="math inline">\(1, \cdots, J\)</span> 중 하나의 값을 가지며, 이를 분류기(classifier)라 부르기도 한다. 분류규칙의 성능을 관찰하기 위하여 우선 학습표본에 적용하여 실제범주와 추정된 범주를 비교한다. 즉, <span class="math inline">\(r(\mathbf{x}_i)\)</span>와 <span class="math inline">\(y_i\)</span>를 비교하여 오분류율 등을 분석한다. 다시 말하면, <span class="math inline">\(r(\mathbf{x}_i) = y_i\)</span> 이면 올바르게 분류된 것이나, 그렇지 않으면 잘못 분류된 것이다. 학습표본에 있는 전체 객체는 서로 배타적인 <span class="math inline">\(J\)</span>개의 집합으로 나누어진다. 분류규칙의 성능평가에 대한 보다 자세한 설명은 이후 <a href="classifier-evaluation.html#classifier-evaluation">6</a>장에서 하기로 한다.</p>
<p>분류를 위한 방법론은 무수하게 많은데, 크게 아래와 같이 대별된다.</p>
<ol style="list-style-type: decimal">
<li>통계적 방법: 로지스틱 회귀분석, 반별분석 등 다변량 통계이론에 바탕을 둔 방법</li>
<li>트리기반 기법: CART, C4.5, CHAID 등 트리 형태의 분지방법을 이용하는 기법</li>
<li>비선형 최적화 기법: 서포트 벡터 머신(support vector machine; SVM) 등</li>
<li>기계학습 기법: 신경망(neural network) 등의 블랙박스 형태의 기법</li>
</ol>
<p><a href="logistic-regression.html#logistic-regression">2</a>장에서는 로지스틱 회귀분석을, <a href="da.html#da">3</a>장에서는 판별분석에 의한 분류분석을, <a href="tree-based-method.html#tree-based-method">4</a>장에서는 트리기반 기법을 다루며, <a href="svm.html#svm">5</a>장에서는 서포트 벡터 머신을 다루고자 한다.</p>
</div>
<div id="simple-classification-methods" class="section level2">
<h2><span class="header-section-number">1.3</span> 기본적인 분류기법</h2>
<p>본 절에서는 위에서 언급하지 않은 기본적인 몇 가지 분류기법에 대하여 설명하고자 한다.</p>
<div id="nearest-neighbor-classification" class="section level3">
<h3><span class="header-section-number">1.3.1</span> 인접객체법</h3>
<p>인접객체법(nearest neighbor classification)은 학습 데이터를 활용하지만 규칙을 도출하는 기법은 아니다. 분류하고자 하는 새로운 객체에 대하여 학습 데이터에 있는 가장 가까운 몇 개의 객체들을 찾은 후 이들 인접객체들의 다수 범주로 분류하는 기법이다. <span class="math inline">\(k\)</span>개의 인접객체를 고려할 때, <span class="math inline">\(k\)</span>-인접객체법(k-nearest neighbor method)이라 한다. 가까운 정도의 척도는 유사성 척도 또는 유클리드 거리 등의 비유사성 척도가 사용되는데, 이들에 대한 자세한 설명은 <a href="clustering-overview.html#clustering-overview">7</a>장에서 이루어진다.</p>
<div id="nearest-neighbor-classificaiton-basic-script" class="section level4">
<h4><span class="header-section-number">1.3.1.1</span> 기본 R 스트립트</h4>
<p>다음과 같은 7개의 객체에 대한 학습표본이 있다.</p>
<pre class="sourceCode r"><code class="sourceCode r">train_df &lt;-<span class="st"> </span><span class="kw">tribble</span>(
  <span class="op">~</span>id, <span class="op">~</span>x1, <span class="op">~</span>x2, <span class="op">~</span>y,
  <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">1</span>,
  <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>,
  <span class="dv">3</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">2</span>,
  <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">6</span>, <span class="dv">2</span>,
  <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">1</span>,
  <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">1</span>,
  <span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">6</span>, <span class="dv">2</span>
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">factor</span>(y, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)))

knitr<span class="op">::</span><span class="kw">kable</span>(
  train_df, <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
  <span class="dt">align =</span> <span class="kw">c</span>(<span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>),
  <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;객체번호&#39;</span>, <span class="st">&#39;$x_1$&#39;</span>, <span class="st">&#39;$x_2$&#39;</span>, <span class="st">&#39;범주&#39;</span>),
  <span class="dt">caption =</span> <span class="st">&#39;인접객체법 학습표본&#39;</span>
)</code></pre>
<table>
<caption><span id="tab:knn-classification-data">Table 1.1: </span>인접객체법 학습표본</caption>
<thead>
<tr class="header">
<th align="center">객체번호</th>
<th align="right"><span class="math inline">\(x_1\)</span></th>
<th align="right"><span class="math inline">\(x_2\)</span></th>
<th align="center">범주</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="right">5</td>
<td align="right">7</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="right">7</td>
<td align="right">8</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="right">8</td>
<td align="right">6</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="right">2</td>
<td align="right">5</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="right">9</td>
<td align="right">6</td>
<td align="center">2</td>
</tr>
</tbody>
</table>
<p><code>class</code> 패키지의 <code>knn.cv</code> 함수는 학습표본의 각각의 객체에 대해 그 객체를 제외한 나머지 학습표본 중 객체에서 가장 가까운(유클리드 거리 기반) <span class="math inline">\(k\)</span>개의 객체의 범주값을 이용하여 대상 학습표본의 범주값을 추정하는 leave-one-out cross validation을 수행한다. 아래 스크립트는 Table <a href="classification-analysis.html#tab:knn-classification-data">1.1</a>의 학습표본 데이터에 대해 3-인접객체 leave-one-out cross validation 결과 추정된 범주값을 산출한다.</p>
<pre class="sourceCode r"><code class="sourceCode r">y_hat &lt;-<span class="st"> </span>class<span class="op">::</span><span class="kw">knn.cv</span>(
  <span class="dt">train =</span> train_df[, <span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)],
  <span class="dt">cl =</span> train_df<span class="op">$</span>y,
  <span class="dt">k =</span> <span class="dv">3</span>
)

train_df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> y_hat) <span class="op">%&gt;%</span>
<span class="st">  </span>knitr<span class="op">::</span><span class="kw">kable</span>(
    <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
    <span class="dt">align =</span> <span class="kw">c</span>(<span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>),
    <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;객체번호&#39;</span>, <span class="st">&#39;$x_1$&#39;</span>, <span class="st">&#39;$x_2$&#39;</span>, <span class="st">&#39;실제범주&#39;</span>, <span class="st">&#39;추정범주&#39;</span>),
    <span class="dt">caption =</span> <span class="st">&#39;인접객체법 추정범주 - 학습데이터&#39;</span>
  )</code></pre>
<table>
<caption><span id="tab:knn-classification-cv">Table 1.2: </span>인접객체법 추정범주 - 학습데이터</caption>
<thead>
<tr class="header">
<th align="center">객체번호</th>
<th align="right"><span class="math inline">\(x_1\)</span></th>
<th align="right"><span class="math inline">\(x_2\)</span></th>
<th align="center">실제범주</th>
<th align="center">추정범주</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="right">5</td>
<td align="right">7</td>
<td align="center">1</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="center">2</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="right">7</td>
<td align="right">8</td>
<td align="center">2</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="right">8</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="right">2</td>
<td align="right">5</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="right">9</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="center">2</td>
</tr>
</tbody>
</table>
<p><code>class</code> 패키지의 <code>knn</code> 함수는 새로운 객체에 대해 인접한 학습데이터를 이용하여 범주를 추정하는 함수이다. 아래 스크립트는 두 개의 새로운 객체 <span class="math inline">\((6, 7)^\top\)</span>과 <span class="math inline">\((4, 2)^\top\)</span>에 대해 3-인근객체법으로 추정범주를 구하는 스크립트이다.</p>
<pre class="sourceCode r"><code class="sourceCode r">test_df &lt;-<span class="st"> </span><span class="kw">tribble</span>(
  <span class="op">~</span>id, <span class="op">~</span>x1, <span class="op">~</span>x2,
  <span class="dv">8</span>, <span class="dv">6</span>, <span class="dv">7</span>,
  <span class="dv">9</span>, <span class="dv">4</span>, <span class="dv">2</span>
)

y_hat &lt;-<span class="st"> </span>class<span class="op">::</span><span class="kw">knn</span>(
  <span class="dt">train =</span> train_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(x1, x2),
  <span class="dt">test =</span> test_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(x1, x2),
  <span class="dt">cl =</span> train_df<span class="op">$</span>y,
  <span class="dt">k =</span> <span class="dv">3</span>
)

test_df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> y_hat) <span class="op">%&gt;%</span>
<span class="st">  </span>knitr<span class="op">::</span><span class="kw">kable</span>(
    <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
    <span class="dt">align =</span> <span class="kw">c</span>(<span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>),
    <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;객체번호&#39;</span>, <span class="st">&#39;$x_1$&#39;</span>, <span class="st">&#39;$x_2$&#39;</span>, <span class="st">&#39;추정범주&#39;</span>),
    <span class="dt">caption =</span> <span class="st">&#39;인접객체법 추정범주 - 새로운 객체&#39;</span>
  )</code></pre>
<table>
<caption><span id="tab:knn-classification-test">Table 1.3: </span>인접객체법 추정범주 - 새로운 객체</caption>
<thead>
<tr class="header">
<th align="center">객체번호</th>
<th align="right"><span class="math inline">\(x_1\)</span></th>
<th align="right"><span class="math inline">\(x_2\)</span></th>
<th align="center">추정범주</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">8</td>
<td align="right">6</td>
<td align="right">7</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">9</td>
<td align="right">4</td>
<td align="right">2</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
</div>
<div id="nearest-neighbor-classification-algorithm" class="section level4">
<h4><span class="header-section-number">1.3.1.2</span> 인접객체법 알고리즘</h4>
<p><span class="math inline">\(k\)</span>-인접객체법의 알고리즘은 다음과 같다.</p>
<ul>
<li><strong>[단계 1]</strong> <span class="math inline">\(k\)</span>값을 정한다.</li>
<li><strong>[단계 2]</strong> 분류하고자 하는 새로운 객체 <span class="math inline">\(\mathbf{z}\)</span>에 대하여
<ul>
<li>2-1. 학습표본에 있는 각 객체 <span class="math inline">\(\mathbf{x}_i\)</span>와의 거리 <span class="math inline">\(d(\mathbf{z}, \mathbf{x}_i)\)</span>를 산출한다.</li>
<li>2-2. 위의 거리가 짧은 순으로 <span class="math inline">\(k\)</span>개의 객체를 선정한다.</li>
<li>2-3. <span class="math inline">\(k\)</span>개의 인근객체가 취하는 범주 중 최빈값을 새로운 객체 <span class="math inline">\(\mathbf{z}\)</span>의 범주로 정한다.</li>
</ul></li>
</ul>
<p>위 알고리즘을 학습표본 Table <a href="classification-analysis.html#tab:knn-classification-data">1.1</a>와 두 새로운 객체 <span class="math inline">\((6, 7)^\top\)</span> 및 <span class="math inline">\((4, 2)^\top\)</span>에 적용해보자.</p>
<p>[단계 1] 우선 각 학습표본 객체에 대해 <span class="math inline">\(k\)</span>값을 변화시키며 인접객체법으로 분류해보자. 이 때, 각 객체 스스로는 인접객체에 포함되지 않는다.</p>
<p>우선 아래 스크립트는 각 학습 객체간 유클리드 거리를 구한다.</p>
<pre class="sourceCode r"><code class="sourceCode r">train_pairwise_dist &lt;-<span class="st"> </span><span class="kw">dist</span>(train_df[, <span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)], <span class="dt">upper =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span>broom<span class="op">::</span><span class="kw">tidy</span>()

train_pairwise_dist</code></pre>
<pre><code>## # A tibble: 42 x 3
##    item1 item2 distance
##    &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;
##  1     2     1     4.12
##  2     3     1     2.24
##  3     4     1     3.16
##  4     5     1     2.24
##  5     6     1     3.61
##  6     7     1     4.12
##  7     1     2     4.12
##  8     3     2     5.83
##  9     4     2     5   
## 10     5     2     3.16
## # … with 32 more rows</code></pre>
<p>각 객체별로 가장 인접한 객체 순으로 순서(rank)를 구한다.</p>
<pre class="sourceCode r"><code class="sourceCode r">train_nn_rank &lt;-<span class="st"> </span>train_pairwise_dist <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(item1) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">nn_rank =</span> <span class="kw">rank</span>(distance, <span class="dt">ties.method =</span> <span class="st">&quot;random&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(item1, nn_rank)

train_nn_rank</code></pre>
<pre><code>## # A tibble: 42 x 4
##    item1 item2 distance nn_rank
##    &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;   &lt;int&gt;
##  1     1     5     2.24       1
##  2     1     3     2.24       2
##  3     1     4     3.16       3
##  4     1     6     3.61       4
##  5     1     2     4.12       5
##  6     1     7     4.12       6
##  7     2     6     2.83       1
##  8     2     5     3.16       2
##  9     2     1     4.12       3
## 10     2     4     5          4
## # … with 32 more rows</code></pre>
<p>이후 각 <span class="math inline">\(k\)</span>값에 대하여 각 객체 대해 <span class="math inline">\(k\)</span>-인접객체법에 대한 추정범주를 구해보자.</p>
<pre class="sourceCode r"><code class="sourceCode r">loo_cv &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(
  train_nn_rank <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(item1, nn_rank),
  <span class="kw">map2_dfr</span>(
    train_nn_rank<span class="op">$</span>item1,
    train_nn_rank<span class="op">$</span>nn_rank,
    <span class="cf">function</span>(.x, .y, df, y) {
      df <span class="op">%&gt;%</span>
<span class="st">        </span><span class="kw">filter</span>(
          item1 <span class="op">==</span><span class="st"> </span>.x,
          nn_rank <span class="op">&lt;=</span><span class="st"> </span>.y
        ) <span class="op">%&gt;%</span>
<span class="st">        </span><span class="kw">mutate</span>(<span class="dt">y =</span> y[item2]) <span class="op">%&gt;%</span>
<span class="st">        </span><span class="kw">count</span>(y) <span class="op">%&gt;%</span>
<span class="st">        </span><span class="kw">slice</span>(<span class="kw">which.max</span>(n))
    },
    <span class="dt">df =</span> train_nn_rank,
    <span class="dt">y =</span> train_df<span class="op">$</span>y
  )
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">rename</span>(<span class="dt">k =</span> nn_rank, <span class="dt">y_hat =</span> y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> train_df<span class="op">$</span>y[item1])

loo_cv</code></pre>
<pre><code>## # A tibble: 42 x 5
##    item1     k y_hat     n y    
##    &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;
##  1     1     1 1         1 1    
##  2     1     2 1         1 1    
##  3     1     3 2         2 1    
##  4     1     4 1         2 1    
##  5     1     5 2         3 1    
##  6     1     6 2         4 1    
##  7     2     1 1         1 2    
##  8     2     2 1         2 2    
##  9     2     3 1         3 2    
## 10     2     4 1         3 2    
## # … with 32 more rows</code></pre>
<p>학습객체들의 <span class="math inline">\(k\)</span>-인접객체법 추정범주와 실제범주가 같은 비율을 정확도라 하여, 각 <span class="math inline">\(k\)</span>값에 대해 정확도를 계산해보자.</p>
<pre class="sourceCode r"><code class="sourceCode r">loo_cv <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">is_correct =</span> (y <span class="op">==</span><span class="st"> </span>y_hat)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(k) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">accuracy =</span> <span class="kw">mean</span>(is_correct)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(accuracy))</code></pre>
<pre><code>## # A tibble: 6 x 2
##       k accuracy
##   &lt;int&gt;    &lt;dbl&gt;
## 1     1    0.714
## 2     2    0.714
## 3     3    0.714
## 4     4    0.714
## 5     5    0.429
## 6     6    0</code></pre>
<p>위의 결과에 기반하여, 정확도가 가장 높은 경우의 <span class="math inline">\(k\)</span>들 중 가장 큰 값인 <span class="math inline">\(k = 3\)</span> 을 최적 <span class="math inline">\(k\)</span>값으로 선정하자.</p>
<p>[단계 2] 두 새로운 객체에 대한 3-인접객체법 추정범주를 구해보자.</p>
<p>우선 새로운 객체들과 기존 학습표본 객체들간의 거리를 구해보자.</p>
<pre class="sourceCode r"><code class="sourceCode r">test_df &lt;-<span class="st"> </span><span class="kw">tribble</span>(
  <span class="op">~</span>id, <span class="op">~</span>x1, <span class="op">~</span>x2,
  <span class="dv">8</span>, <span class="dv">6</span>, <span class="dv">7</span>,
  <span class="dv">9</span>, <span class="dv">4</span>, <span class="dv">2</span>
)

test_train_dist &lt;-<span class="st"> </span>flexclust<span class="op">::</span><span class="kw">dist2</span>(
  test_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(x1, x2), 
  train_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(x1, x2)
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span>
<span class="st">  `</span><span class="dt">names&lt;-</span><span class="st">`</span>(<span class="kw">seq_len</span>(<span class="kw">nrow</span>(train_df))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">item1 =</span> <span class="kw">seq_len</span>(<span class="kw">nrow</span>(test_df))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(<span class="dt">key =</span> <span class="st">&quot;item2&quot;</span>, <span class="dt">value =</span> <span class="st">&quot;distance&quot;</span>, <span class="op">-</span>item1) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">item2 =</span> <span class="kw">as.numeric</span>(item2))

test_train_dist</code></pre>
<pre><code>## # A tibble: 14 x 3
##    item1 item2 distance
##    &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1     1     1     1   
##  2     2     1     5.10
##  3     1     2     4.47
##  4     2     2     1   
##  5     1     3     1.41
##  6     2     3     6.71
##  7     1     4     2.24
##  8     2     4     5.66
##  9     1     5     3.16
## 10     2     5     4.12
## 11     1     6     4.47
## 12     2     6     3.61
## 13     1     7     3.16
## 14     2     7     6.40</code></pre>
<p>각 새로운 객체에 대하여 가장 인접한 3개의 학습표본만 남긴다.</p>
<pre class="sourceCode r"><code class="sourceCode r">test_nn &lt;-<span class="st"> </span>test_train_dist <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(item1) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(distance) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">nn_rank =</span> <span class="kw">row_number</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(nn_rank <span class="op">&lt;=</span><span class="st"> </span><span class="dv">3</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>()

test_nn</code></pre>
<pre><code>## # A tibble: 6 x 4
##   item1 item2 distance nn_rank
##   &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt;
## 1     1     1     1          1
## 2     2     2     1          1
## 3     1     3     1.41       2
## 4     1     4     2.24       3
## 5     2     6     3.61       2
## 6     2     5     4.12       3</code></pre>
<p>해당 인접 학습표본들의 범주값을 관측하여, 가장 자주 발견되는 범주값을 새로운 객체의 범주값으로 추정한다.</p>
<pre class="sourceCode r"><code class="sourceCode r">test_yhat &lt;-<span class="st"> </span>test_nn <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">id =</span> test_df<span class="op">$</span>id[item1],
    <span class="dt">y =</span> train_df<span class="op">$</span>y[item2]
    ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(id, y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">n =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(n)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">rename</span>(<span class="dt">y_hat =</span> y)

test_yhat</code></pre>
<pre><code>## # A tibble: 2 x 3
##      id y_hat     n
##   &lt;dbl&gt; &lt;fct&gt; &lt;int&gt;
## 1     8 2         2
## 2     9 1         2</code></pre>
<p>위 결과, 객체 <span class="math inline">\((6, 7)^\top\)</span>는 범주 2로, 객체 <span class="math inline">\((4, 2)^\top\)</span>는 범주 1로 분류된다.</p>
</div>
</div>
<div id="naive-bayes" class="section level3">
<h3><span class="header-section-number">1.3.2</span> 나이브 베이지안 분류법</h3>
<p>나이브 베이지안(Naive Bayesian) 분류법이란 속성변수들과 범주변수가 확률분포를 따른다고 간주하여 베이즈 정리와 조건부 독립성을 활용한 분류기법이다. 속성변수들이 범주형일 때 주로 사용되나, 연속형인 경우에도 확률분포의 형태를 가정하여 사용할 수 있다. 본 장에서는 범주형 변수인 경우를 설명한다.</p>
<div id="naive-bayes-basic-script" class="section level4">
<h4><span class="header-section-number">1.3.2.1</span> 기본 R 스크립트</h4>
<p>아래와 같은 9명의 고객에 대한 학습표본이 있다.</p>
<pre class="sourceCode r"><code class="sourceCode r">train_df &lt;-<span class="st"> </span><span class="kw">tribble</span>(
  <span class="op">~</span>id, <span class="op">~</span>x1, <span class="op">~</span>x2, <span class="op">~</span>y,
  <span class="dv">1</span>, <span class="st">&quot;남&quot;</span>, <span class="st">&quot;20대&quot;</span>, <span class="dv">1</span>,
  <span class="dv">2</span>, <span class="st">&quot;남&quot;</span>, <span class="st">&quot;20대&quot;</span>, <span class="dv">2</span>,
  <span class="dv">3</span>, <span class="st">&quot;남&quot;</span>, <span class="st">&quot;30대&quot;</span>, <span class="dv">1</span>,
  <span class="dv">4</span>, <span class="st">&quot;남&quot;</span>, <span class="st">&quot;40대&quot;</span>, <span class="dv">1</span>,
  <span class="dv">5</span>, <span class="st">&quot;여&quot;</span>, <span class="st">&quot;10대&quot;</span>, <span class="dv">1</span>,
  <span class="dv">6</span>, <span class="st">&quot;여&quot;</span>, <span class="st">&quot;20대&quot;</span>, <span class="dv">2</span>,
  <span class="dv">7</span>, <span class="st">&quot;여&quot;</span>, <span class="st">&quot;20대&quot;</span>, <span class="dv">1</span>,
  <span class="dv">8</span>, <span class="st">&quot;여&quot;</span>, <span class="st">&quot;30대&quot;</span>, <span class="dv">2</span>,
  <span class="dv">9</span>, <span class="st">&quot;여&quot;</span>, <span class="st">&quot;40대&quot;</span>, <span class="dv">2</span>
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">factor</span>(y, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)))

knitr<span class="op">::</span><span class="kw">kable</span>(
  train_df, <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
  <span class="dt">align =</span> <span class="kw">c</span>(<span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>),
  <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;고객번호&#39;</span>, <span class="st">&#39;성별 ($x_1$)&#39;</span>, <span class="st">&#39;나이 ($x_2$)&#39;</span>, <span class="st">&#39;범주 ($y$)&#39;</span>),
  <span class="dt">caption =</span> <span class="st">&#39;나이브 베이지안 분류법 학습표본&#39;</span>
)</code></pre>
<table>
<caption><span id="tab:naive-bayes-data">Table 1.4: </span>나이브 베이지안 분류법 학습표본</caption>
<thead>
<tr class="header">
<th align="center">고객번호</th>
<th align="center">성별 (<span class="math inline">\(x_1\)</span>)</th>
<th align="center">나이 (<span class="math inline">\(x_2\)</span>)</th>
<th align="center">범주 (<span class="math inline">\(y\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">남</td>
<td align="center">20대</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">남</td>
<td align="center">20대</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">남</td>
<td align="center">30대</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">남</td>
<td align="center">40대</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">여</td>
<td align="center">10대</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">여</td>
<td align="center">20대</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="center">여</td>
<td align="center">20대</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="center">여</td>
<td align="center">30대</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="center">여</td>
<td align="center">40대</td>
<td align="center">2</td>
</tr>
</tbody>
</table>
<p><code>e1071</code> 패키지의 <code>naiveBayes</code> 함수를 이용하면, 객체가 각 범주에 속할 조건부 확률분포 모델을 추정할 수 있다.</p>
<pre class="sourceCode r"><code class="sourceCode r">nb_fit &lt;-<span class="st"> </span>e1071<span class="op">::</span><span class="kw">naiveBayes</span>(<span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> train_df)

<span class="kw">print</span>(nb_fit)</code></pre>
<pre><code>## 
## Naive Bayes Classifier for Discrete Predictors
## 
## Call:
## naiveBayes.default(x = X, y = Y, laplace = laplace)
## 
## A-priori probabilities:
## Y
##         1         2 
## 0.5555556 0.4444444 
## 
## Conditional probabilities:
##    x1
## Y     남   여
##   1 0.60 0.40
##   2 0.25 0.75
## 
##    x2
## Y   10대 20대 30대 40대
##   1 0.20 0.40 0.20 0.20
##   2 0.00 0.50 0.25 0.25</code></pre>
<p>추정된 모델을 학습표본에 적용하여 범주를 추정해보자.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 범주 추정값</span>
y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(nb_fit, train_df)

<span class="co"># 사후확률 추정값</span>
nb_posterior &lt;-<span class="st"> </span><span class="kw">predict</span>(nb_fit, train_df, <span class="dt">type =</span> <span class="st">&quot;raw&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span>
<span class="st">  `</span><span class="dt">colnames&lt;-</span><span class="st">`</span>(<span class="kw">str_c</span>(<span class="st">&quot;p&quot;</span>, <span class="kw">levels</span>(train_df<span class="op">$</span>y)))

train_df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> y_hat) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(nb_posterior) <span class="op">%&gt;%</span>
<span class="st">  </span>knitr<span class="op">::</span><span class="kw">kable</span>(
    <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
    <span class="dt">align =</span> <span class="kw">c</span>(<span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>),
    <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;고객번호&#39;</span>, <span class="st">&#39;성별 ($x_1$)&#39;</span>, <span class="st">&#39;나이 ($x_2$)&#39;</span>, 
                  <span class="st">&#39;실제범주 ($y$)&#39;</span>, <span class="st">&#39;추정범주 ($</span><span class="ch">\\</span><span class="st">hat{y}$)&#39;</span>, 
                  <span class="kw">str_c</span>(<span class="st">&#39;사후확률 ($y$ = &#39;</span>, <span class="kw">levels</span>(train_df<span class="op">$</span>y), <span class="st">&#39;)&#39;</span>)),
    <span class="dt">caption =</span> <span class="st">&#39;나이브 베이지안 분류법에 의한 추정 범주&#39;</span>
  )</code></pre>
<table>
<caption><span id="tab:naive-bayes-posterior">Table 1.5: </span>나이브 베이지안 분류법에 의한 추정 범주</caption>
<thead>
<tr class="header">
<th align="center">고객번호</th>
<th align="center">성별 (<span class="math inline">\(x_1\)</span>)</th>
<th align="center">나이 (<span class="math inline">\(x_2\)</span>)</th>
<th align="center">실제범주 (<span class="math inline">\(y\)</span>)</th>
<th align="center">추정범주 (<span class="math inline">\(\hat{y}\)</span>)</th>
<th align="center">사후확률 (<span class="math inline">\(y\)</span> = 1)</th>
<th align="center">사후확률 (<span class="math inline">\(y\)</span> = 2)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">남</td>
<td align="center">20대</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">0.7058824</td>
<td align="center">0.2941176</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">남</td>
<td align="center">20대</td>
<td align="center">2</td>
<td align="center">1</td>
<td align="center">0.7058824</td>
<td align="center">0.2941176</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">남</td>
<td align="center">30대</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">0.7058824</td>
<td align="center">0.2941176</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">남</td>
<td align="center">40대</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">0.7058824</td>
<td align="center">0.2941176</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">여</td>
<td align="center">10대</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">0.9925558</td>
<td align="center">0.0074442</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">여</td>
<td align="center">20대</td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center">0.3478261</td>
<td align="center">0.6521739</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="center">여</td>
<td align="center">20대</td>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">0.3478261</td>
<td align="center">0.6521739</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="center">여</td>
<td align="center">30대</td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center">0.3478261</td>
<td align="center">0.6521739</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="center">여</td>
<td align="center">40대</td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center">0.3478261</td>
<td align="center">0.6521739</td>
</tr>
</tbody>
</table>
<p>또한, 학습표본에 포함되지 않은 10대 남자인 새로운 고객에 대한 범주가 아래와 같이 추정된다.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(nb_fit, <span class="kw">tibble</span>(<span class="dt">x1 =</span> <span class="st">&quot;남&quot;</span>, <span class="dt">x2 =</span> <span class="st">&quot;10대&quot;</span>))</code></pre>
<pre><code>## [1] 1
## Levels: 1 2</code></pre>
</div>
<div id="naive-bayes-algorithm" class="section level4">
<h4><span class="header-section-number">1.3.2.2</span> 알고리즘</h4>
<p>어떤 객체 <span class="math inline">\(\mathbf{x}\)</span>에 대해 범주가 <span class="math inline">\(y\)</span>일 조건부 확률분포는 베이즈 정리에 의하여 다음과 같이 표현된다.</p>
<p><span class="math display" id="eq:bayes-posterior">\[\begin{equation}
P(y \, | \, \mathbf{x}) \propto P(y) P(\mathbf{x} \, | \, y), \, y = 1, \cdots, J \tag{1.1}
\end{equation}\]</span></p>
<p>여기서 <span class="math inline">\(P(y)\)</span>는 임의의 객체가 범주 <span class="math inline">\(y\)</span>에 속할 사전확률을 의미하며, <span class="math inline">\(P(y \, | \, \mathbf{x})\)</span>는 객체 속성변수 <span class="math inline">\(\mathbf{x}\)</span>의 관측값에 따른 범주 <span class="math inline">\(y\)</span>의 사후확률을 나타낸다. 그리고 <span class="math inline">\(P(\mathbf{x} \, | \, y)\)</span>는 범주 <span class="math inline">\(y\)</span>에 속한 객체들의 속성변수 분포를 나타낸다.</p>
<p>나이브 베이지안 분류법에서는 속성변수들의 조건부 결합확률분포 <span class="math inline">\(P(\mathbf{x} \, | \, y)\)</span>에 대한 조건부 독립성을 가정하여, <span class="math inline">\(p\)</span>개의 변수로 이루어진 객체 속성변수 벡터 <span class="math inline">\(\mathbf{x} = (x_1, x_2, \cdots, x_p)\)</span>에 대하여 다음이 성립한고 가정한다.</p>
<p><span class="math display">\[\begin{equation*}
P(x_a \, | x_{a + 1}, x_{a + 2}, \cdots, x_p, y) = P(x_a \,|\, y)
\end{equation*}\]</span></p>
<p>이 때, 식 <a href="classification-analysis.html#eq:bayes-posterior">(1.1)</a>는 아래와 같이 표현될 수 있다.</p>
<p><span class="math display" id="eq:naive-bayes-posterior">\[\begin{equation}
P(y \, | \, \mathbf{x}) \propto P(y) \prod_{a = 1}^{p} P(x_a \, | \, y), \, y = 1, \cdots, J \tag{1.2}
\end{equation}\]</span></p>
<p>우선, 학습표본 <a href="classification-analysis.html#tab:naive-bayes-data">1.4</a>을 이용하여 범주의 사전확률 <span class="math inline">\(P(y)\)</span>를 추정해보자.</p>
<pre class="sourceCode r"><code class="sourceCode r">prior_prob &lt;-<span class="st"> </span>train_df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">n =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prior =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>n)

prior_prob</code></pre>
<pre><code>## # A tibble: 2 x 2
##   y     prior
##   &lt;fct&gt; &lt;dbl&gt;
## 1 1     0.556
## 2 2     0.444</code></pre>
<p>또한, 학습표본 <a href="classification-analysis.html#tab:naive-bayes-data">1.4</a>에 대해 각 변수의 조건부 확률 <span class="math inline">\(P(x_a \,|\, y)\)</span>를 추정해보자.</p>
<pre class="sourceCode r"><code class="sourceCode r">condition_prob &lt;-<span class="st"> </span>train_df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(<span class="dt">key =</span> <span class="st">&quot;variable&quot;</span>, <span class="dt">value =</span> <span class="st">&quot;value&quot;</span>, x1, x2) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(y, variable, value) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">n =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">cond_prob =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>n) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">complete</span>(y, <span class="kw">nesting</span>(variable, value), <span class="dt">fill =</span> <span class="kw">list</span>(<span class="dt">cond_prob =</span> <span class="dv">0</span>))

condition_prob</code></pre>
<pre><code>## # A tibble: 12 x 4
##    y     variable value cond_prob
##    &lt;fct&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;
##  1 1     x1       남         0.6 
##  2 1     x1       여         0.4 
##  3 1     x2       10대       0.2 
##  4 1     x2       20대       0.4 
##  5 1     x2       30대       0.2 
##  6 1     x2       40대       0.2 
##  7 2     x1       남         0.25
##  8 2     x1       여         0.75
##  9 2     x2       10대       0   
## 10 2     x2       20대       0.5 
## 11 2     x2       30대       0.25
## 12 2     x2       40대       0.25</code></pre>
<p>추정된 확률을 식 <a href="classification-analysis.html#eq:naive-bayes-posterior">(1.2)</a>에 적용하여, 각 학습데이터에 대한 범주의 사후확률을 구해보자.</p>
<pre class="sourceCode r"><code class="sourceCode r">posterior_prob &lt;-<span class="st"> </span>train_df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(<span class="dt">key =</span> <span class="st">&quot;variable&quot;</span>, <span class="dt">value =</span> <span class="st">&quot;value&quot;</span>, x1, x2) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">inner_join</span>(condition_prob, <span class="dt">by =</span> <span class="kw">c</span>(<span class="st">&quot;variable&quot;</span>, <span class="st">&quot;value&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(id, y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">cond_prob =</span> <span class="kw">reduce</span>(cond_prob, <span class="st">`</span><span class="dt">*</span><span class="st">`</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">inner_join</span>(prior_prob, <span class="dt">by =</span> <span class="st">&quot;y&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">posterior_unadjust =</span> prior <span class="op">*</span><span class="st"> </span>cond_prob) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">posterior =</span> posterior_unadjust <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(posterior_unadjust)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(id, y, posterior) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>()

posterior_prob <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread</span>(<span class="dt">key =</span> y, <span class="dt">value =</span> posterior)</code></pre>
<pre><code>## # A tibble: 9 x 3
##      id   `1`   `2`
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     1 0.706 0.294
## 2     2 0.706 0.294
## 3     3 0.706 0.294
## 4     4 0.706 0.294
## 5     5 1     0    
## 6     6 0.348 0.652
## 7     7 0.348 0.652
## 8     8 0.348 0.652
## 9     9 0.348 0.652</code></pre>
<p>추정범주는 사후확률이 가장 큰 범주를 선택한다.</p>
<pre class="sourceCode r"><code class="sourceCode r">posterior_prob <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(id) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">1</span>, posterior) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span>)</code></pre>
<pre><code>## # A tibble: 9 x 3
## # Groups:   id [9]
##      id y     posterior
##   &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;
## 1     1 1         0.706
## 2     2 1         0.706
## 3     3 1         0.706
## 4     4 1         0.706
## 5     5 1         1    
## 6     6 2         0.652
## 7     7 2         0.652
## 8     8 2         0.652
## 9     9 2         0.652</code></pre>
</div>
<div id="naive-bayes-pkg" class="section level4">
<h4><span class="header-section-number">1.3.2.3</span> R 패키지 내 나이브 베이지안 분류법</h4>
<p>위 <a href="classification-analysis.html#naive-bayes-basic-script">1.3.2.1</a>절에서 살펴본 바와 같이 <code>e1071</code> 패키지 내의 <code>naiveBayes</code> 함수를 이용하여 분류 모델을 추정할 수 있다.</p>
<pre class="sourceCode r"><code class="sourceCode r">nb_fit &lt;-<span class="st"> </span>e1071<span class="op">::</span><span class="kw">naiveBayes</span>(<span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> train_df)</code></pre>
<p>위 <code>naiveBayes</code> 모델 객체의 component 중 <code>apriori</code>는 객체가 각 범주에 속할 사전분포를 나타내는 <code>table</code> 형태의 객체로, 본 예에서 학습표본 중 각 범주에 속한 객체 수를 나타낸다.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(nb_fit<span class="op">$</span>apriori)</code></pre>
<pre><code>##  &#39;table&#39; int [1:2(1d)] 5 4
##  - attr(*, &quot;dimnames&quot;)=List of 1
##   ..$ Y: chr [1:2] &quot;1&quot; &quot;2&quot;</code></pre>
<p>아래와 같이, 각 범주에 속한 객체 수를 전체 객체 수로 나눔으로써 추정된 사전분포(prior distribution)을 확인할 수 있다.</p>
<pre class="sourceCode r"><code class="sourceCode r">nb_fit<span class="op">$</span>apriori <span class="op">%&gt;%</span>
<span class="st">  </span>broom<span class="op">::</span><span class="kw">tidy</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">p =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n))</code></pre>
<pre><code>## # A tibble: 2 x 3
##   Y         n     p
##   &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;
## 1 1         5 0.556
## 2 2         4 0.444</code></pre>
<p>각 변수별 조건부 확률은 <code>tables</code>라는 리스트 객체에서 변수별로 확인할 수 있다.</p>
<pre class="sourceCode r"><code class="sourceCode r">nb_fit<span class="op">$</span>tables</code></pre>
<pre><code>## $x1
##    x1
## Y     남   여
##   1 0.60 0.40
##   2 0.25 0.75
## 
## $x2
##    x2
## Y   10대 20대 30대 40대
##   1 0.20 0.40 0.20 0.20
##   2 0.00 0.50 0.25 0.25</code></pre>
<p><code>predict</code> 함수를 이용하여 사후확률을 구할 때, <code>threshold</code> 파라미터값을 이용하여 최소 사후확률값을 지정할 수 있다. 기본값은 0.001로, 추정 사후확률값이 최소 0.1%보다 커야한다는 것을 의미한다.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(nb_fit, <span class="dt">newdata =</span> train_df[<span class="dv">5</span>, ], <span class="dt">type =</span> <span class="st">&quot;raw&quot;</span>)</code></pre>
<pre><code>##              1           2
## [1,] 0.9925558 0.007444169</code></pre>
<p>해당 파라미터값을 0.01으로 지정할 경우, 위에서 범주 2에 속할 사후확률이 보다 크게 얻어짐을 확인할 수 있다.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(nb_fit, <span class="dt">newdata =</span> train_df[<span class="dv">5</span>, ], <span class="dt">type =</span> <span class="st">&quot;raw&quot;</span>, <span class="dt">threshold =</span> <span class="fl">0.01</span>)</code></pre>
<pre><code>##              1          2
## [1,] 0.9302326 0.06976744</code></pre>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logistic-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
