<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 3 판별분석 | 데이터마이닝 with R</title>
  <meta name="description" content="전치혁 교수님의 책 을 기반으로 한 R 예제">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 3 판별분석 | 데이터마이닝 with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://youngroklee-ml.github.io/data-mining-book/" />
  
  <meta property="og:description" content="전치혁 교수님의 책 을 기반으로 한 R 예제" />
  <meta name="github-repo" content="youngroklee-ml/data-mining-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 판별분석 | 데이터마이닝 with R" />
  
  <meta name="twitter:description" content="전치혁 교수님의 책 을 기반으로 한 R 예제" />
  

<meta name="author" content="전치혁, 이혜선, 이종석, 이영록">


<meta name="date" content="2019-03-17">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="logistic-regression.html">
<link rel="next" href="tree-based-method.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">데이터마이닝 with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>개요</a></li>
<li class="chapter" data-level="1" data-path="classification-analysis.html"><a href="classification-analysis.html"><i class="fa fa-check"></i><b>1</b> 분류분석 개요</a><ul>
<li class="chapter" data-level="1.1" data-path="classification-analysis.html"><a href="classification-analysis.html#classification-packages-install"><i class="fa fa-check"></i><b>1.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="1.2" data-path="classification-analysis.html"><a href="classification-analysis.html#classification-problem-methods"><i class="fa fa-check"></i><b>1.2</b> 분류문제 및 분류기법</a></li>
<li class="chapter" data-level="1.3" data-path="classification-analysis.html"><a href="classification-analysis.html#simple-classification-methods"><i class="fa fa-check"></i><b>1.3</b> 기본적인 분류기법</a><ul>
<li class="chapter" data-level="1.3.1" data-path="classification-analysis.html"><a href="classification-analysis.html#nearest-neighbor-classification"><i class="fa fa-check"></i><b>1.3.1</b> 인접객체법</a></li>
<li class="chapter" data-level="1.3.2" data-path="classification-analysis.html"><a href="classification-analysis.html#naive-bayes"><i class="fa fa-check"></i><b>1.3.2</b> 나이브 베이지안 분류법</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>2</b> 로지스틱 회귀분석</a><ul>
<li class="chapter" data-level="2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-packages-install"><i class="fa fa-check"></i><b>2.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#binary-logistic-regression"><i class="fa fa-check"></i><b>2.2</b> 이분 로지스틱 회귀모형</a><ul>
<li class="chapter" data-level="2.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#bianry-logistic-reg-basic-script"><i class="fa fa-check"></i><b>2.2.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="2.2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#binary-logistic-reg-model"><i class="fa fa-check"></i><b>2.2.2</b> 회귀모형</a></li>
<li class="chapter" data-level="2.2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#binary-logistic-reg-estimation"><i class="fa fa-check"></i><b>2.2.3</b> 회귀계수 추정</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#nominal-logistic-regression"><i class="fa fa-check"></i><b>2.3</b> 명목 로지스틱 회귀모형</a><ul>
<li class="chapter" data-level="2.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#nominal-logistic-reg-basic-script"><i class="fa fa-check"></i><b>2.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="2.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#baseline-category-logit-model"><i class="fa fa-check"></i><b>2.3.2</b> 기준범주 로짓모형</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="logistic-regression.html"><a href="logistic-regression.html#ordinal-logistic-regression"><i class="fa fa-check"></i><b>2.4</b> 서열 로지스틱 회귀모형</a><ul>
<li class="chapter" data-level="2.4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#ordinal-logistic-basic-script"><i class="fa fa-check"></i><b>2.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="2.4.2" data-path="logistic-regression.html"><a href="logistic-regression.html#cumulative-logit-model"><i class="fa fa-check"></i><b>2.4.2</b> 누적 로짓모형</a></li>
<li class="chapter" data-level="2.4.3" data-path="logistic-regression.html"><a href="logistic-regression.html#adjacent-categories-logit-model"><i class="fa fa-check"></i><b>2.4.3</b> 인근범주 로짓모형</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="da.html"><a href="da.html"><i class="fa fa-check"></i><b>3</b> 판별분석</a><ul>
<li class="chapter" data-level="3.1" data-path="da.html"><a href="da.html#da-overview"><i class="fa fa-check"></i><b>3.1</b> 개요</a></li>
<li class="chapter" data-level="3.2" data-path="da.html"><a href="da.html#da-packages-install"><i class="fa fa-check"></i><b>3.2</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="3.3" data-path="da.html"><a href="da.html#da-fisher"><i class="fa fa-check"></i><b>3.3</b> 피셔 방법</a><ul>
<li class="chapter" data-level="3.3.1" data-path="da.html"><a href="da.html#da-fisher-basic-script"><i class="fa fa-check"></i><b>3.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="3.3.2" data-path="da.html"><a href="da.html#-"><i class="fa fa-check"></i><b>3.3.2</b> 피셔 판별함수</a></li>
<li class="chapter" data-level="3.3.3" data-path="da.html"><a href="da.html#-"><i class="fa fa-check"></i><b>3.3.3</b> 분류 규칙</a></li>
<li class="chapter" data-level="3.3.4" data-path="da.html"><a href="da.html#r----"><i class="fa fa-check"></i><b>3.3.4</b> R 패키지를 이용한 분류규칙 도출</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="da.html"><a href="da.html#lda"><i class="fa fa-check"></i><b>3.4</b> 의사결정론에 의한 선형분류규칙</a><ul>
<li class="chapter" data-level="3.4.1" data-path="da.html"><a href="da.html#lda-basic-script"><i class="fa fa-check"></i><b>3.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="3.4.2" data-path="da.html"><a href="da.html#lda-function"><i class="fa fa-check"></i><b>3.4.2</b> 선형판별함수</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="da.html"><a href="da.html#lda-misclassification-cost"><i class="fa fa-check"></i><b>3.5</b> 오분류비용을 고려한 분류규칙</a></li>
<li class="chapter" data-level="3.6" data-path="da.html"><a href="da.html#qda"><i class="fa fa-check"></i><b>3.6</b> 이차판별분석</a><ul>
<li class="chapter" data-level="3.6.1" data-path="da.html"><a href="da.html#qda-basic-script"><i class="fa fa-check"></i><b>3.6.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="3.6.2" data-path="da.html"><a href="da.html#qda-function"><i class="fa fa-check"></i><b>3.6.2</b> 이차 판별함수</a></li>
<li class="chapter" data-level="3.6.3" data-path="da.html"><a href="da.html#qda-discriminant-rule"><i class="fa fa-check"></i><b>3.6.3</b> 이차판별함수에 의한 분류</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="da.html"><a href="da.html#da-multiclass"><i class="fa fa-check"></i><b>3.7</b> 세 범주 이상의 분류</a><ul>
<li class="chapter" data-level="3.7.1" data-path="da.html"><a href="da.html#mutliclass-da-basic-script"><i class="fa fa-check"></i><b>3.7.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="3.7.2" data-path="da.html"><a href="da.html#mutliclass-generalized-discriminant-function"><i class="fa fa-check"></i><b>3.7.2</b> 일반화된 판별함수</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tree-based-method.html"><a href="tree-based-method.html"><i class="fa fa-check"></i><b>4</b> 트리기반 기법</a><ul>
<li class="chapter" data-level="4.1" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-overview"><i class="fa fa-check"></i><b>4.1</b> CART 개요</a></li>
<li class="chapter" data-level="4.2" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-packages-install"><i class="fa fa-check"></i><b>4.2</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="4.3" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-build"><i class="fa fa-check"></i><b>4.3</b> CART 트리 생성</a><ul>
<li class="chapter" data-level="4.3.1" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-basic-r-script"><i class="fa fa-check"></i><b>4.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="4.3.2" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-notation"><i class="fa fa-check"></i><b>4.3.2</b> 기호 정의</a></li>
<li class="chapter" data-level="4.3.3" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-impurity"><i class="fa fa-check"></i><b>4.3.3</b> 노드 및 트리의 불순도</a></li>
<li class="chapter" data-level="4.3.4" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-split"><i class="fa fa-check"></i><b>4.3.4</b> 분지기준</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-pruning-complete"><i class="fa fa-check"></i><b>4.4</b> 가지치기 및 최종 트리 선정</a><ul>
<li class="chapter" data-level="4.4.1" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-pruning"><i class="fa fa-check"></i><b>4.4.1</b> 가지치기</a></li>
<li class="chapter" data-level="4.4.2" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-best-tree"><i class="fa fa-check"></i><b>4.4.2</b> 최적 트리의 선정</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-r-pkg"><i class="fa fa-check"></i><b>4.5</b> R패키지 내 분류 트리 방법</a><ul>
<li class="chapter" data-level="4.5.1" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-r-pkg-split"><i class="fa fa-check"></i><b>4.5.1</b> 트리 확장</a></li>
<li class="chapter" data-level="4.5.2" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-r-pkg-pruning"><i class="fa fa-check"></i><b>4.5.2</b> 가지치기</a></li>
<li class="chapter" data-level="4.5.3" data-path="tree-based-method.html"><a href="tree-based-method.html#cart-r-pkg-param"><i class="fa fa-check"></i><b>4.5.3</b> 파라미터값 결정</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>5</b> 서포트 벡터 머신</a><ul>
<li class="chapter" data-level="5.1" data-path="svm.html"><a href="svm.html#svm-overview"><i class="fa fa-check"></i><b>5.1</b> 개요</a></li>
<li class="chapter" data-level="5.2" data-path="svm.html"><a href="svm.html#svm-packages-install"><i class="fa fa-check"></i><b>5.2</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="5.3" data-path="svm.html"><a href="svm.html#linear-svm-separable"><i class="fa fa-check"></i><b>5.3</b> 선형 SVM - 분리 가능 경우</a><ul>
<li class="chapter" data-level="5.3.1" data-path="svm.html"><a href="svm.html#linear-svm-separable-basic-script"><i class="fa fa-check"></i><b>5.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="5.3.2" data-path="svm.html"><a href="svm.html#linear-svm-notation"><i class="fa fa-check"></i><b>5.3.2</b> 기호 정의</a></li>
<li class="chapter" data-level="5.3.3" data-path="svm.html"><a href="svm.html#linear-svm-separable-hyperplane"><i class="fa fa-check"></i><b>5.3.3</b> 최적 하이퍼플레인</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="svm.html"><a href="svm.html#linear-svm-inseparable"><i class="fa fa-check"></i><b>5.4</b> 선형 SVM - 분리 불가능 경우</a><ul>
<li class="chapter" data-level="5.4.1" data-path="svm.html"><a href="svm.html#linear-svm-inseparable-basic-script"><i class="fa fa-check"></i><b>5.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="5.4.2" data-path="svm.html"><a href="svm.html#linear-svm-inseparable-hyperplane"><i class="fa fa-check"></i><b>5.4.2</b> 최적 하이퍼플레인</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="svm.html"><a href="svm.html#nonlinear-svm"><i class="fa fa-check"></i><b>5.5</b> 비선형 SVM</a><ul>
<li class="chapter" data-level="5.5.1" data-path="svm.html"><a href="svm.html#nonlinear-svm-basic-script"><i class="fa fa-check"></i><b>5.5.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="5.5.2" data-path="svm.html"><a href="svm.html#nonlinear-svm-hyperplane"><i class="fa fa-check"></i><b>5.5.2</b> 최적 하이퍼플레인</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="svm.html"><a href="svm.html#svm-r-pkg"><i class="fa fa-check"></i><b>5.6</b> R패키지 내 SVM</a><ul>
<li class="chapter" data-level="5.6.1" data-path="svm.html"><a href="svm.html#svm-kernel-function"><i class="fa fa-check"></i><b>5.6.1</b> 커널함수</a></li>
<li class="chapter" data-level="5.6.2" data-path="svm.html"><a href="svm.html#svm-nu-classification"><i class="fa fa-check"></i><b>5.6.2</b> <span class="math inline">\(\nu\)</span>-SVC</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classifier-evaluation.html"><a href="classifier-evaluation.html"><i class="fa fa-check"></i><b>6</b> 분류규칙의 성능 평가</a></li>
<li class="chapter" data-level="7" data-path="clustering-overview.html"><a href="clustering-overview.html"><i class="fa fa-check"></i><b>7</b> 군집분석 개요</a><ul>
<li class="chapter" data-level="7.1" data-path="clustering-overview.html"><a href="clustering-overview.html#clustering-overview-packages-install"><i class="fa fa-check"></i><b>7.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="7.2" data-path="clustering-overview.html"><a href="clustering-overview.html#clustering-method"><i class="fa fa-check"></i><b>7.2</b> 군집분석 기법</a></li>
<li class="chapter" data-level="7.3" data-path="clustering-overview.html"><a href="clustering-overview.html#object-similarity-metric"><i class="fa fa-check"></i><b>7.3</b> 객체 간의 유사성 척도</a><ul>
<li class="chapter" data-level="7.3.1" data-path="clustering-overview.html"><a href="clustering-overview.html#object-distance-metric"><i class="fa fa-check"></i><b>7.3.1</b> 거리 관련 척도</a></li>
<li class="chapter" data-level="7.3.2" data-path="clustering-overview.html"><a href="clustering-overview.html#object-correlation-metric"><i class="fa fa-check"></i><b>7.3.2</b> 상관계수 관련 척도</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="clustering-overview.html"><a href="clustering-overview.html#category-similarity-metric"><i class="fa fa-check"></i><b>7.4</b> 범주형 객체의 유사성 척도</a><ul>
<li class="chapter" data-level="7.4.1" data-path="clustering-overview.html"><a href="clustering-overview.html#binary-similarity-metric"><i class="fa fa-check"></i><b>7.4.1</b> 이분형 변수의 경우</a></li>
<li class="chapter" data-level="7.4.2" data-path="clustering-overview.html"><a href="clustering-overview.html#ordinal-similarity-metric"><i class="fa fa-check"></i><b>7.4.2</b> 서열형 변수의 경우</a></li>
<li class="chapter" data-level="7.4.3" data-path="clustering-overview.html"><a href="clustering-overview.html#nominal-similarity-metric"><i class="fa fa-check"></i><b>7.4.3</b> 명목형 변수의 경우</a></li>
<li class="chapter" data-level="7.4.4" data-path="clustering-overview.html"><a href="clustering-overview.html#mixed-similarity-metric"><i class="fa fa-check"></i><b>7.4.4</b> 혼합형의 경우</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>8</b> 계층적 군집방법</a><ul>
<li class="chapter" data-level="8.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#hierarchical-clustering-packages-install"><i class="fa fa-check"></i><b>8.1</b> 필요 R 패키지 설치</a></li>
<li class="chapter" data-level="8.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#distance-between-clusters"><i class="fa fa-check"></i><b>8.2</b> 군집 간 거리척도 및 연결법</a></li>
<li class="chapter" data-level="8.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#linkage-method"><i class="fa fa-check"></i><b>8.3</b> 연결법의 군집 알고리즘</a><ul>
<li class="chapter" data-level="8.3.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#linkage-method-basic-script"><i class="fa fa-check"></i><b>8.3.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="8.3.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#linkage-method-algorithm"><i class="fa fa-check"></i><b>8.3.2</b> 연결법 군집 알고리즘</a></li>
<li class="chapter" data-level="8.3.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#hclust"><i class="fa fa-check"></i><b>8.3.3</b> R 패키지 내 연결법</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#ward-method"><i class="fa fa-check"></i><b>8.4</b> 워드 방법</a><ul>
<li class="chapter" data-level="8.4.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#ward-method-basic-script"><i class="fa fa-check"></i><b>8.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="8.4.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#ward-method-algorithm"><i class="fa fa-check"></i><b>8.4.2</b> 워드 군집 알고리즘</a></li>
<li class="chapter" data-level="8.4.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#ward-rpackages"><i class="fa fa-check"></i><b>8.4.3</b> R 패키지 내 워드 방법</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#diana"><i class="fa fa-check"></i><b>8.5</b> 분리적 방법 - 다이아나</a><ul>
<li class="chapter" data-level="8.5.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#diana-basic-script"><i class="fa fa-check"></i><b>8.5.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="8.5.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#diana-algorithm"><i class="fa fa-check"></i><b>8.5.2</b> 다이아나 알고리즘</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#hierarchical-cluster-number"><i class="fa fa-check"></i><b>8.6</b> 군집수의 결정</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html"><i class="fa fa-check"></i><b>9</b> 비계층적 군집방법</a><ul>
<li class="chapter" data-level="9.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#nonhierarchical-clustering-packages-install"><i class="fa fa-check"></i><b>9.1</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="9.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans"><i class="fa fa-check"></i><b>9.2</b> K-means 알고리즘</a><ul>
<li class="chapter" data-level="9.2.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans-basic-script"><i class="fa fa-check"></i><b>9.2.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="9.2.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans-algorithm"><i class="fa fa-check"></i><b>9.2.2</b> 알고리즘</a></li>
<li class="chapter" data-level="9.2.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans-user-defined-functions"><i class="fa fa-check"></i><b>9.2.3</b> R 스크립트 구현</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmedoids"><i class="fa fa-check"></i><b>9.3</b> K-medoids 군집방법</a><ul>
<li class="chapter" data-level="9.3.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#pam"><i class="fa fa-check"></i><b>9.3.1</b> PAM 알고리즘</a></li>
<li class="chapter" data-level="9.3.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#clara"><i class="fa fa-check"></i><b>9.3.2</b> CLARA 알고리즘</a></li>
<li class="chapter" data-level="9.3.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#clarans"><i class="fa fa-check"></i><b>9.3.3</b> CLARANS 알고리즘</a></li>
<li class="chapter" data-level="9.3.4" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#kmeans-like"><i class="fa fa-check"></i><b>9.3.4</b> K-means-like 알고리즘</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#fuzzy-kmeans"><i class="fa fa-check"></i><b>9.4</b> 퍼지 K-means 알고리즘</a><ul>
<li class="chapter" data-level="9.4.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#fuzzy-kmeans-basic-script"><i class="fa fa-check"></i><b>9.4.1</b> 기본 R 스크립트</a></li>
<li class="chapter" data-level="9.4.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#fuzzy-kmeans-algorithm"><i class="fa fa-check"></i><b>9.4.2</b> 알고리즘</a></li>
<li class="chapter" data-level="9.4.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#fuzzy-kmeans-script-implement"><i class="fa fa-check"></i><b>9.4.3</b> R 스크립트 구현</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#model-based-clustering"><i class="fa fa-check"></i><b>9.5</b> 모형기반 군집방법</a><ul>
<li class="chapter" data-level="9.5.1" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#model-based-clustering-basic-script"><i class="fa fa-check"></i><b>9.5.1</b> 기본 R script</a></li>
<li class="chapter" data-level="9.5.2" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#model-based-clustering-em"><i class="fa fa-check"></i><b>9.5.2</b> EM 알고리즘</a></li>
<li class="chapter" data-level="9.5.3" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#model-based-clustering-script-implement"><i class="fa fa-check"></i><b>9.5.3</b> R 스크립트 구현</a></li>
<li class="chapter" data-level="9.5.4" data-path="nonhierarchical-clustering.html"><a href="nonhierarchical-clustering.html#r-packages-model-based-clustering"><i class="fa fa-check"></i><b>9.5.4</b> R 패키지 내 모형기반 군집분석</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html"><i class="fa fa-check"></i><b>10</b> 군집해의 평가 및 해석</a><ul>
<li class="chapter" data-level="10.1" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html#cluster-solution-evaluation-packages-install"><i class="fa fa-check"></i><b>10.1</b> 필요 R package 설치</a></li>
<li class="chapter" data-level="10.2" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html#cluster-solution-evaluation-metric"><i class="fa fa-check"></i><b>10.2</b> 군집해의 평가</a><ul>
<li class="chapter" data-level="10.2.1" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html#cluster-evaluation-external-index"><i class="fa fa-check"></i><b>10.2.1</b> 외부평가지수</a></li>
<li class="chapter" data-level="10.2.2" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html#cluster-evaluation-internal-index"><i class="fa fa-check"></i><b>10.2.2</b> 내부평가지수</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="cluster-solution-evaluation.html"><a href="cluster-solution-evaluation.html#cluster-solution-interpretation"><i class="fa fa-check"></i><b>10.3</b> 군집해의 해석</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">데이터마이닝 with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="da" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> 판별분석</h1>
<div id="da-overview" class="section level2">
<h2><span class="header-section-number">3.1</span> 개요</h2>
<p>판별분석(discriminant analysis)은 범주들을 가장 잘 구별하는 변수들의 하나 또는 다수의 함수를 도출하여 이를 기반으로 분류규칙을 제시한다. 본 장에서는 변수의 분포에 대한 가정이 필요 없는 피셔(Fisher) 방법과 다변량 정규분포를 가정하는 선형 및 비선형 판별분석을 설명한다.</p>
</div>
<div id="da-packages-install" class="section level2">
<h2><span class="header-section-number">3.2</span> 필요 R 패키지 설치</h2>
<p>본 장에서 필요한 R 패키지들은 아래와 같다.</p>
<table>
<thead>
<tr class="header">
<th align="left">package</th>
<th align="left">version</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">tidyverse</td>
<td align="left">1.2.1</td>
</tr>
<tr class="even">
<td align="left">MASS</td>
<td align="left">7.3-51.1</td>
</tr>
<tr class="odd">
<td align="left">mvtnorm</td>
<td align="left">1.0-8</td>
</tr>
</tbody>
</table>
</div>
<div id="da-fisher" class="section level2">
<h2><span class="header-section-number">3.3</span> 피셔 방법</h2>
<div id="da-fisher-basic-script" class="section level3">
<h3><span class="header-section-number">3.3.1</span> 기본 R 스크립트</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_df &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">id =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">9</span>),
  <span class="dt">x1 =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">9</span>, <span class="dv">5</span>),
  <span class="dt">x2 =</span> <span class="kw">c</span>(<span class="dv">7</span>, <span class="dv">3</span>, <span class="dv">8</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">4</span>),
  <span class="dt">class =</span> <span class="kw">factor</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>), <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
)

knitr<span class="op">::</span><span class="kw">kable</span>(train_df, <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
             <span class="dt">align =</span> <span class="kw">c</span>(<span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>),
             <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;객체번호&#39;</span>, <span class="st">&#39;$x_1$&#39;</span>, <span class="st">&#39;$x_2$&#39;</span>, <span class="st">&#39;범주&#39;</span>),
             
             <span class="dt">caption =</span> <span class="st">&#39;판별분석 학습표본 데이터&#39;</span>)</code></pre></div>
<table>
<caption><span id="tab:da-train-data-table">Table 3.1: </span>판별분석 학습표본 데이터</caption>
<thead>
<tr class="header">
<th align="center">객체번호</th>
<th align="right"><span class="math inline">\(x_1\)</span></th>
<th align="right"><span class="math inline">\(x_2\)</span></th>
<th align="center">범주</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="right">5</td>
<td align="right">7</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="right">7</td>
<td align="right">8</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="right">8</td>
<td align="right">6</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="right">2</td>
<td align="right">5</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="right">9</td>
<td align="right">6</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="center">2</td>
</tr>
</tbody>
</table>
<p>Table <a href="da.html#tab:da-train-data-table">3.1</a>와 같이 두 독립변수 <em>x1</em>, <em>x2</em>와 이분형 종속변수 <em>class</em>의 관측값으로 이루어진 9개의 학습표본을 <em>train_df</em>라는 data frame에 저장한다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fisher_da &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">lda</span>(class <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, train_df)

<span class="kw">print</span>(fisher_da)</code></pre></div>
<pre><code>## Call:
## lda(class ~ x1 + x2, data = train_df)
## 
## Prior probabilities of groups:
##         1         2 
## 0.4444444 0.5555556 
## 
## Group means:
##    x1  x2
## 1 4.0 6.0
## 2 6.6 5.4
## 
## Coefficients of linear discriminants:
##           LD1
## x1  0.6850490
## x2 -0.7003859</code></pre>
</div>
<div id="-" class="section level3">
<h3><span class="header-section-number">3.3.2</span> 피셔 판별함수</h3>
<p>각 객체는 변수벡터 <span class="math inline">\(\mathbf{x} \in \mathbb{R}^p\)</span>와 범주 <span class="math inline">\(y \in \{1, 2\}\)</span>로 이루어진다고 하자. 아래는 변수 <span class="math inline">\(\mathbf{x}\)</span>의 기대치와 분산-공분산행렬(varinace-covariance matrix)을 나타낸다.</p>
<span class="math display">\[\begin{eqnarray*}
\boldsymbol\mu_1 = E(\mathbf{x} | y = 1)\\
\boldsymbol\mu_2 = E(\mathbf{x} | y = 2)\\
\boldsymbol\Sigma = Var(\mathbf{x} | y = 1) = Var(\mathbf{x} | y = 2)
\end{eqnarray*}\]</span>
<p>다음과 같이 변수들의 선형조합으로 새로운 변수 <span class="math inline">\(z\)</span>를 형성하는 함수를 피셔 판별함수(Fisher’s discriminant function)라 한다.</p>
<span class="math display" id="eq:fisher-discriminant-function">\[\begin{equation}
z = \mathbf{w}^\top \mathbf{x} \tag{3.1}
\end{equation}\]</span>
<p>여기서 계수벡터 <span class="math inline">\(\mathbf{w} \in \mathbb{R}^p\)</span>는 통상 아래와 같이 변수 <span class="math inline">\(z\)</span>의 범주간 평균 차이 대 변수 <span class="math inline">\(z\)</span>의 분산의 비율을 최대화하는 것으로 결정한다.</p>
<span class="math display" id="eq:fisher-discriminant-function-coef">\[\begin{equation}
{\arg\!\min}_{\mathbf{w}} \frac{\mathbf{w}^\top ( \boldsymbol\mu_1 - \boldsymbol\mu_2 )}{\mathbf{w}^\top \boldsymbol\Sigma \mathbf{w}} \tag{3.2}
\end{equation}\]</span>
<p>위 식 <a href="da.html#eq:fisher-discriminant-function-coef">(3.2)</a>의 해는</p>
<span class="math display">\[\begin{equation*}
\mathbf{w} \propto \boldsymbol\Sigma^{-1}(\boldsymbol\mu_1 - \boldsymbol\mu_2)
\end{equation*}\]</span>
<p>의 조건을 만족하며, 편의상 비례상수를 1로 두면 아래와 같은 해가 얻어진다.</p>
<span class="math display" id="eq:fisher-discriminant-function-coef-sol">\[\begin{equation}
\mathbf{w} = \boldsymbol\Sigma^{-1}(\boldsymbol\mu_1 - \boldsymbol\mu_2) \tag{3.3}
\end{equation}\]</span>
<p>실제 모집단의 평균 및 분산을 알지 못하는 경우, 학습표본으로부터 <span class="math inline">\(\boldsymbol\mu_1, \boldsymbol\mu_2, \boldsymbol\Sigma\)</span>의 추정치를 얻어 식 <a href="da.html#eq:fisher-discriminant-function-coef-sol">(3.3)</a>에 대입하는 방식으로 판별계수를 추정한다. 자세한 내용은 교재 <span class="citation">(전치혁 <a href="#ref-jun2012datamining">2012</a>)</span> 참조.</p>
<p>Table <a href="da.html#tab:da-train-data-table">3.1</a>에 주어진 학습표본을 이용하여 피셔 판별함수를 구해보도록 하자. 우선 각 범주별 평균벡터 <span class="math inline">\(\hat{\boldsymbol\mu}_1, \hat{\boldsymbol\mu}_2\)</span>를 아래와 같이 구한다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu_hat &lt;-<span class="st"> </span>train_df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(class) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">x1 =</span> <span class="kw">mean</span>(x1),
            <span class="dt">x2 =</span> <span class="kw">mean</span>(x2)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(class)

<span class="kw">print</span>(mu_hat)</code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   class    x1    x2
##   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 1       4     6  
## 2 2       6.6   5.4</code></pre>
<p>또한 범주별 표본 분산-공분산행렬 <span class="math inline">\(\mathbf{S}_1, \mathbf{S}_2\)</span>를 다음과 같이 구한다. 리스트 <code>S_within_group</code>의 첫번째 원소는 범주 1의 분산-공분산행렬 <span class="math inline">\(\mathbf{S}_1\)</span>, 두번째 원소는 범주 2의 분산-공분산행렬 <span class="math inline">\(\mathbf{S}_2\)</span>를 나타낸다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">S_within_group &lt;-<span class="st"> </span><span class="kw">lapply</span>(
  <span class="kw">unique</span>(train_df<span class="op">$</span>class) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sort</span>(), <span class="cf">function</span>(x) {
    train_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(class <span class="op">==</span><span class="st"> </span>x) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(x1, x2) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">var</span>()
  }
)

<span class="kw">print</span>(S_within_group)</code></pre></div>
<pre><code>## [[1]]
##          x1        x2
## x1 3.333333 1.0000000
## x2 1.000000 0.6666667
## 
## [[2]]
##      x1   x2
## x1 4.30 2.95
## x2 2.95 3.80</code></pre>
<p>위에서 얻은 범주별 표본 분산-공분산행렬을 이용하여 합동 분산-공분산행렬을 아래와 같이 추정한다.</p>
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol\Sigma} = \mathbf{S}_p = \frac{(n_1 - 1)\mathbf{S}_1 + (n_2 - 1)\mathbf{S}_2}{n_1 + n_2 - 2}
\end{equation*}\]</span>
<p>이 때 <span class="math inline">\(n_1, n_2\)</span>는 각각 범주 1, 2에 속한 학습표본 객체의 수를 나타낸다. 아래 R 스크립트에서는 임의의 범주 표본수 벡터 <code>n</code>과 범주별 표본 분산-공분산행렬 리스트 <code>S</code>에 대해 합동 분산-공분산행렬을 구하는 함수 <code>pooled_variance</code>를 정의하고, 주어진 학습표본에 대한 입력값을 대입하여 합동 분산-공분산행렬 추정치 <code>Sigma_hat</code>을 구한다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pooled_variance &lt;-<span class="st"> </span><span class="cf">function</span>(n, S) {
  <span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n), <span class="cf">function</span>(i) (n[i] <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)<span class="op">*</span>S[[i]]) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">Reduce</span>(<span class="st">`</span><span class="dt">+</span><span class="st">`</span>, .) <span class="op">%&gt;%</span>
<span class="st">    `</span><span class="dt">/</span><span class="st">`</span>(<span class="kw">sum</span>(n) <span class="op">-</span><span class="st"> </span><span class="kw">length</span>(n))
}

n_obs &lt;-<span class="st"> </span>train_df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(class) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pi =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(class)

Sigma_hat &lt;-<span class="st"> </span><span class="kw">pooled_variance</span>(n_obs<span class="op">$</span>n, S_within_group)

<span class="kw">print</span>(Sigma_hat)</code></pre></div>
<pre><code>##          x1       x2
## x1 3.885714 2.114286
## x2 2.114286 2.457143</code></pre>
<p>위에서 구한 추정치들을 이용하여 아래와 같이 판별함수 계수 추정치 <span class="math inline">\(\hat{\mathbf{w}}\)</span>를 구한다.</p>
<span class="math display">\[\begin{equation*}
\hat{\mathbf{w}} = \hat{\boldsymbol\Sigma}^{-1}(\hat{\boldsymbol\mu}_1 - \hat{\boldsymbol\mu}_2) 
\end{equation*}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">w_hat &lt;-<span class="st"> </span><span class="kw">solve</span>(Sigma_hat) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(mu_hat[<span class="dv">1</span>, <span class="kw">c</span>(<span class="st">&#39;x1&#39;</span>, <span class="st">&#39;x2&#39;</span>)] <span class="op">-</span><span class="st"> </span>mu_hat[<span class="dv">2</span>, <span class="kw">c</span>(<span class="st">&#39;x1&#39;</span>, <span class="st">&#39;x2&#39;</span>)])

<span class="kw">print</span>(w_hat)</code></pre></div>
<pre><code>##         [,1]
## x1 -1.508039
## x2  1.541801</code></pre>
</div>
<div id="-" class="section level3">
<h3><span class="header-section-number">3.3.3</span> 분류 규칙</h3>
<p>피셔 판별함수에 따른 분류 경계값은 학습표본에 대한 판별함수값의 평균으로 아래와 같이 구할 수 있다.</p>
<span class="math display">\[\begin{equation*}
\bar{z} = \frac{1}{N} \sum_i^N \hat{\mathbf{w}}^\top \mathbf{x}_i
\end{equation*}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z_mean &lt;-<span class="st"> </span><span class="kw">t</span>(w_hat) <span class="op">%*%</span><span class="st"> </span>(train_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(x1, x2) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">colMeans</span>()) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">drop</span>()

<span class="kw">print</span>(z_mean)</code></pre></div>
<pre><code>## [1] 0.526438</code></pre>
<p>위 결과를 통해, 분류규칙은 다음과 같이 주어진다.</p>
<ul>
<li><span class="math inline">\(\hat{\mathbf{w}}^\top \mathbf{x} \ge \bar{z}\)</span> 이면, <span class="math inline">\(\mathbf{x}\)</span>를 범주 1로 분류</li>
<li><span class="math inline">\(\hat{\mathbf{w}}^\top \mathbf{x} &lt; \bar{z}\)</span> 이면, <span class="math inline">\(\mathbf{x}\)</span>를 범주 2로 분류</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_prediction_df &lt;-<span class="st"> </span>train_df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">z =</span> w_hat[<span class="dv">1</span>]<span class="op">*</span>x1 <span class="op">+</span><span class="st"> </span>w_hat[<span class="dv">2</span>]<span class="op">*</span>x2,
    <span class="dt">predicted_class =</span> <span class="kw">factor</span>(<span class="kw">if_else</span>(z <span class="op">&gt;=</span><span class="st"> </span>z_mean, <span class="dv">1</span>, <span class="dv">2</span>), <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
    )

knitr<span class="op">::</span><span class="kw">kable</span>(train_prediction_df, <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
             <span class="dt">align =</span> <span class="kw">c</span>(<span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>),
             <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;객체번호&#39;</span>, <span class="st">&#39;$x_1$&#39;</span>, <span class="st">&#39;$x_2$&#39;</span>, <span class="st">&#39;실제범주&#39;</span>, <span class="st">&#39;$z$&#39;</span>, <span class="st">&#39;추정범주&#39;</span>),
             <span class="dt">caption =</span> <span class="st">&#39;학습표본에 대한 피셔 분류 결과&#39;</span>)</code></pre></div>
<table>
<caption><span id="tab:fisher-da-result">Table 3.2: </span>학습표본에 대한 피셔 분류 결과</caption>
<thead>
<tr class="header">
<th align="center">객체번호</th>
<th align="right"><span class="math inline">\(x_1\)</span></th>
<th align="right"><span class="math inline">\(x_2\)</span></th>
<th align="center">실제범주</th>
<th align="right"><span class="math inline">\(z\)</span></th>
<th align="center">추정범주</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="right">5</td>
<td align="right">7</td>
<td align="center">1</td>
<td align="right">3.2524116</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="center">2</td>
<td align="right">-1.4067524</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="right">7</td>
<td align="right">8</td>
<td align="center">2</td>
<td align="right">1.7781350</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="right">8</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="right">-2.8135048</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="right">4.7266881</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="right">2</td>
<td align="right">5</td>
<td align="center">1</td>
<td align="right">4.6929260</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="right">0.2025723</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="right">9</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="right">-4.3215434</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="center">2</td>
<td align="right">-1.3729904</td>
<td align="center">2</td>
</tr>
</tbody>
</table>
<p>위 결과 객체 3, 7가 오분류된다.</p>
</div>
<div id="r----" class="section level3">
<h3><span class="header-section-number">3.3.4</span> R 패키지를 이용한 분류규칙 도출</h3>
<p>패키지 <code>MASS</code>내의 함수 <code>lda</code> 수행 시 얻어지는 판별계수 <span class="math inline">\(\hat{\mathbf{w}}\)</span>는 위 결과와는 사뭇 다른데, <code>lda</code> 함수의 경우 아래와 같이 1) 제약식을 포함하여 비례계수를 구하기 때문에 계수의 크기가 달라지며, 2) 목적함수를 최소화하는 대신 최대화하는 값을 찾기 때문에 부호가 달라진다.</p>
<span class="math display">\[\begin{equation*}
\begin{split}
\max \text{  } &amp; \mathbf{w}^\top ( \boldsymbol\mu_1 - \boldsymbol\mu_2 )\\
\text{s.t. } &amp; \mathbf{w}^\top \boldsymbol\Sigma \mathbf{w} = 1
\end{split}
\end{equation*}\]</span>
<p>이에 따른 <code>lda</code> 함수의 계수 추정 결과는 아래와 같다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fisher_da &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">lda</span>(class <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, train_df)

w_hat_lda &lt;-<span class="st"> </span>fisher_da<span class="op">$</span>scaling
<span class="kw">print</span>(w_hat_lda)</code></pre></div>
<pre><code>##           LD1
## x1  0.6850490
## x2 -0.7003859</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z_mean_lda &lt;-<span class="st"> </span><span class="kw">t</span>(fisher_da<span class="op">$</span>scaling) <span class="op">%*%</span><span class="st"> </span>(train_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(x1, x2) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">colMeans</span>()) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">drop</span>()
<span class="kw">print</span>(z_mean_lda)</code></pre></div>
<pre><code>##        LD1 
## -0.2391423</code></pre>
<p>위 결과는 아래와 같은 계산을 통해 앞 장에서 보았던 결과와 동일한 분류 경계식으로 표현될 수 있음을 볼 수 있다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">scale_adjust &lt;-<span class="st"> </span><span class="kw">t</span>(w_hat) <span class="op">%*%</span><span class="st"> </span>Sigma_hat <span class="op">%*%</span><span class="st"> </span>w_hat <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">drop</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sqrt</span>()
sign_adjust &lt;-<span class="st"> </span><span class="op">-</span><span class="dv">1</span>

w_hat &lt;-<span class="st"> </span>w_hat_lda <span class="op">*</span><span class="st"> </span>scale_adjust <span class="op">*</span><span class="st"> </span>sign_adjust
<span class="kw">print</span>(w_hat)</code></pre></div>
<pre><code>##          LD1
## x1 -1.508039
## x2  1.541801</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z_mean &lt;-<span class="st"> </span>z_mean_lda <span class="op">*</span><span class="st"> </span>scale_adjust <span class="op">*</span><span class="st"> </span>sign_adjust 
<span class="kw">print</span>(z_mean)</code></pre></div>
<pre><code>##      LD1 
## 0.526438</code></pre>
<p>아래 스크립트는 위 <code>lda</code> 함수로부터의 경계식 추정을 기반으로 아래 수식값을 계산한다.</p>
<span class="math display">\[\begin{equation*}
\hat{\mathbf{w}}^\top \mathbf{x} - \bar{z}
\end{equation*}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(fisher_da, train_df)<span class="op">$</span>x</code></pre></div>
<pre><code>##          LD1
## 1 -1.2383140
## 2  0.8781805
## 3 -0.5686020
## 4  1.5172187
## 5 -1.9080261
## 6 -1.8926892
## 7  0.1471208
## 8  2.2022677
## 9  0.8628436</code></pre>
<p>피셔 분류규칙에 따라 해당 값이 0보다 작으면 범주 1, 0보다 크면 범주 2로 분류한다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">centered_z =</span> <span class="kw">predict</span>(fisher_da, .)<span class="op">$</span>x,
    <span class="dt">predicted_class =</span> <span class="kw">factor</span>(<span class="kw">if_else</span>(centered_z <span class="op">&lt;=</span><span class="st"> </span><span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>), <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
    ) <span class="op">%&gt;%</span>
<span class="st">  </span>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
             <span class="dt">align =</span> <span class="kw">c</span>(<span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>, <span class="st">&#39;r&#39;</span>),
             <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;객체번호&#39;</span>, <span class="st">&#39;$x_1$&#39;</span>, <span class="st">&#39;$x_2$&#39;</span>, 
                           <span class="st">&#39;실제범주&#39;</span>, <span class="st">&#39;$z - </span><span class="ch">\\</span><span class="st">bar{z}$&#39;</span>, <span class="st">&#39;추정범주&#39;</span>),
             <span class="dt">caption =</span> <span class="st">&#39;학습표본에 대한 피셔 분류 결과 - `MASS::lda` 분류 경계식 기준&#39;</span>)</code></pre></div>
<table>
<caption><span id="tab:fisher-da-result-lda">Table 3.3: </span>학습표본에 대한 피셔 분류 결과 - <code>MASS::lda</code> 분류 경계식 기준</caption>
<thead>
<tr class="header">
<th align="center">객체번호</th>
<th align="right"><span class="math inline">\(x_1\)</span></th>
<th align="right"><span class="math inline">\(x_2\)</span></th>
<th align="center">실제범주</th>
<th align="right"><span class="math inline">\(z - \bar{z}\)</span></th>
<th align="center">추정범주</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="right">5</td>
<td align="right">7</td>
<td align="center">1</td>
<td align="right">-1.2383140</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="center">2</td>
<td align="right">0.8781805</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="right">7</td>
<td align="right">8</td>
<td align="center">2</td>
<td align="right">-0.5686020</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="right">8</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="right">1.5172187</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="right">-1.9080261</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="right">2</td>
<td align="right">5</td>
<td align="center">1</td>
<td align="right">-1.8926892</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="right">0.1471208</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="right">9</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="right">2.2022677</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="center">2</td>
<td align="right">0.8628436</td>
<td align="center">2</td>
</tr>
</tbody>
</table>
<p>Table <a href="da.html#tab:fisher-da-result-lda">3.3</a>는 Table <a href="da.html#tab:fisher-da-result">3.2</a>와 동일한 범주 추정 결과를 보인다.</p>
</div>
</div>
<div id="lda" class="section level2">
<h2><span class="header-section-number">3.4</span> 의사결정론에 의한 선형분류규칙</h2>
<p>다음과 같이 객체가 각 범주에 속할 사전확률과 각 범주 내에서의 분류변수의 확률밀도함수에 대한 기호를 정의한다.</p>
<ul>
<li><span class="math inline">\(\pi_k\)</span>: 임의의 객체가 범주 <span class="math inline">\(k\)</span>에 속할 사전확률</li>
<li><span class="math inline">\(f_k(\mathbf{x})\)</span>: 범주 <span class="math inline">\(k\)</span>에 대한 변수의 확률밀도함수</li>
</ul>
<p>이 때 통상적으로 <span class="math inline">\(\mathbf{x}\)</span>는 다변량 정규분포를 따르는 것으로 가정하여 아래와 같이 평균벡터 <span class="math inline">\(\boldsymbol\mu_k\)</span>와 분산-공분산행렬 <span class="math inline">\(\boldsymbol\Sigma\)</span>로 확률밀도함수를 정의할 수 있다. 이 때 분산-공분산행렬 <span class="math inline">\(\boldsymbol\Sigma\)</span>는 모든 범주에 대해 동일하다고 가정한다.</p>
<span class="math display" id="eq:mv-gaussian-dist">\[\begin{equation}
f_k(\mathbf{x}) = \frac{1}{(2\pi)^{p/2}|\boldsymbol\Sigma|^{1/2}} \exp \{ -\frac{1}{2} \left(\mathbf{x} - \boldsymbol\mu_k\right)^\top \boldsymbol\Sigma^{-1} \left(\mathbf{x} - \boldsymbol\mu_k\right) \}
\tag{3.4}
\end{equation}\]</span>
<p>본 장에서는 두 범주(<span class="math inline">\(k = 1, 2\)</span>) 분류 문제만 다루며, 세 범주 이상에 대한 분류 문제는 뒷 장에서 추가적으로 다루기로 한다.</p>
<div id="lda-basic-script" class="section level3">
<h3><span class="header-section-number">3.4.1</span> 기본 R 스크립트</h3>
<p>Table <a href="da.html#tab:da-train-data-table">3.1</a>의 학습표본에 대해 선형판별분석을 적용하는 R 스크립트는 아래에 보이는 것처럼 피셔 판별함수를 구하기 위한 동일하며, <code>prior</code> 파라미터를 정의하지 않음으로써 <span class="math inline">\(\pi_1\)</span>과 <span class="math inline">\(\pi_2\)</span>를 학습표본의 범주 1, 2의 비율로 설정한다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lda_fit &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">lda</span>(class <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, train_df)

<span class="kw">print</span>(lda_fit)</code></pre></div>
<pre><code>## Call:
## lda(class ~ x1 + x2, data = train_df)
## 
## Prior probabilities of groups:
##         1         2 
## 0.4444444 0.5555556 
## 
## Group means:
##    x1  x2
## 1 4.0 6.0
## 2 6.6 5.4
## 
## Coefficients of linear discriminants:
##           LD1
## x1  0.6850490
## x2 -0.7003859</code></pre>
</div>
<div id="lda-function" class="section level3">
<h3><span class="header-section-number">3.4.2</span> 선형판별함수</h3>
<p>두 범주 문제에 있어서, 범주를 알지 못하는 변수 <span class="math inline">\(\mathbf{x}\)</span>에 대한 확률밀도함수는 아래와 같다.</p>
<span class="math display">\[\begin{equation*}
f(\mathbf{x}) = \pi_1 f_1(\mathbf{x}) + \pi_2 f_2(\mathbf{x})
\end{equation*}\]</span>
<p>베이즈 정리(Bayes’s theorem)에 따라 변수 <span class="math inline">\(\mathbf{x}\)</span>값이 주어졌을 때 범주 <span class="math inline">\(k\)</span>에 속할 사후확률(posterior)은 아래와 같이 구할 수 있다.</p>
<span class="math display" id="eq:lda-posterior">\[\begin{equation}
P(y = k \, | \, \mathbf{x}) = \frac{\pi_k f_k(\mathbf{x})}{f(\mathbf{x})}
\tag{3.5}
\end{equation}\]</span>
<p>각 범주에 대한 사후확률을 계산하여, 확률이 높은 쪽으로 범주를 추정한다.</p>
<span class="math display" id="eq:lda-posterior-rule">\[\begin{equation}
\hat{y} = \begin{cases}
    1, &amp; \text{if } P(y = 1 \, | \, \mathbf{x}) \ge P(y = 2 \, | \, \mathbf{x})\\
    2, &amp; \text{otherwise}
\end{cases}
\tag{3.6}
\end{equation}\]</span>
<p>이를 다시 정리하면 아래와 같다.</p>
<span class="math display">\[\begin{equation*}
\hat{y} = \begin{cases}
    1, &amp; \text{if } \frac{f_1(\mathbf{x})}{f_2(\mathbf{x})} \ge \frac{\pi_2}{\pi_1}\\
    2, &amp; \text{otherwise}
\end{cases}
\end{equation*}\]</span>
<p>위 분류규칙에 식 <a href="da.html#eq:mv-gaussian-dist">(3.4)</a>을 대입하여 정리하면 다음과 같다. 보다 자세한 내용은 교재 <span class="citation">(전치혁 <a href="#ref-jun2012datamining">2012</a>)</span> 참조.</p>
<span class="math display">\[\begin{equation*}
\hat{y} = \begin{cases}
    1, &amp; \text{if } \boldsymbol\mu_1^\top \boldsymbol\Sigma^{-1}\mathbf{x} - \frac{1}{2} \boldsymbol\mu_1^\top \boldsymbol\Sigma^{-1} \boldsymbol\mu_1 + \ln \pi_1 \ge \boldsymbol\mu_2^\top \boldsymbol\Sigma^{-1}\mathbf{x} - \frac{1}{2} \boldsymbol\mu_2^\top \boldsymbol\Sigma^{-1} \boldsymbol\mu_2 + \ln \pi_2  \\
    2, &amp; \text{otherwise}
\end{cases}
\end{equation*}\]</span>
<p>따라서, 각 범주에 대한 판별함수를</p>
<span class="math display" id="eq:lda-discriminant-function">\[\begin{equation}
u_k(\mathbf{x}) = \boldsymbol\mu_k^\top \boldsymbol\Sigma^{-1}\mathbf{x} - \frac{1}{2} \boldsymbol\mu_k^\top \boldsymbol\Sigma^{-1} \boldsymbol\mu_k + \ln \pi_k
\tag{3.7}
\end{equation}\]</span>
<p>라 하면, 아래와 같이 분류규칙을 정의할 수 있다.</p>
<span class="math display" id="eq:lda-discriminant-rule">\[\begin{equation}
\hat{y} = \begin{cases}
    1, &amp; \text{if } u_1(\mathbf{x}) \ge u_2(\mathbf{x})  \\
    2, &amp; \text{otherwise}
\end{cases}
\tag{3.8}
\end{equation}\]</span>
<p>Table <a href="da.html#tab:da-train-data-table">3.1</a>의 학습표본에 대해 판별함수값을 계산하고 범주를 추정하면 아래와 같다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">discriminant_func &lt;-<span class="st"> </span><span class="cf">function</span>(X, mu, Sigma, pi) {
  Sigma_inv &lt;-<span class="st"> </span><span class="kw">solve</span>(Sigma)
  (<span class="kw">t</span>(mu) <span class="op">%*%</span><span class="st"> </span>Sigma_inv <span class="op">%*%</span><span class="st"> </span>X <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">drop</span>()) <span class="op">-</span><span class="st">  </span>
<span class="st">    </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">t</span>(mu) <span class="op">%*%</span><span class="st"> </span>Sigma_inv <span class="op">%*%</span><span class="st"> </span>mu <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">drop</span>()) <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">log</span>(pi)
}

lda_discriminant_result_df &lt;-<span class="st"> </span>train_df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">u1 =</span> <span class="kw">discriminant_func</span>(
      .[<span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">t</span>(),
      mu_hat[<span class="dv">1</span>, <span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unlist</span>(),
      Sigma_hat,
      n_obs<span class="op">$</span>pi[<span class="dv">1</span>]
      ),
    <span class="dt">u2 =</span> <span class="kw">discriminant_func</span>(
      .[<span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">t</span>(),
      mu_hat[<span class="dv">2</span>, <span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unlist</span>(),
      Sigma_hat,
      n_obs<span class="op">$</span>pi[<span class="dv">2</span>]
      )
    ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">predicted_class =</span> <span class="kw">factor</span>(<span class="kw">if_else</span>(u1 <span class="op">&gt;=</span><span class="st"> </span>u2, <span class="dv">1</span>, <span class="dv">2</span>), <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
    )

knitr<span class="op">::</span><span class="kw">kable</span>(
  lda_discriminant_result_df,
  <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
  <span class="dt">align =</span> <span class="kw">rep</span>(<span class="st">&#39;r&#39;</span>, <span class="kw">dim</span>(lda_discriminant_result_df)[<span class="dv">2</span>]),
  <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;객체번호&#39;</span>, <span class="st">&#39;$x_1$&#39;</span>, <span class="st">&#39;$x_2$&#39;</span>,
                <span class="st">&#39;실제범주&#39;</span>, <span class="st">&#39;$u_1(</span><span class="ch">\\</span><span class="st">mathbf{x})$&#39;</span>, <span class="st">&#39;$u_2(</span><span class="ch">\\</span><span class="st">mathbf{x})$&#39;</span>,
                <span class="st">&#39;추정범주&#39;</span>),
  <span class="dt">caption =</span> <span class="st">&#39;학습표본에 대한 LDA 적용 결과: 판별함수값 및 추정범주&#39;</span>)</code></pre></div>
<table>
<caption><span id="tab:lda-disriminant-result">Table 3.4: </span>학습표본에 대한 LDA 적용 결과: 판별함수값 및 추정범주</caption>
<thead>
<tr class="header">
<th align="center">객체번호</th>
<th align="right"><span class="math inline">\(x_1\)</span></th>
<th align="right"><span class="math inline">\(x_2\)</span></th>
<th align="center">실제범주</th>
<th align="right"><span class="math inline">\(u_1(\mathbf{x})\)</span></th>
<th align="right"><span class="math inline">\(u_2(\mathbf{x})\)</span></th>
<th align="center">추정범주</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="right">5</td>
<td align="right">7</td>
<td align="center">1</td>
<td align="right">9.2051470</td>
<td align="right">6.971538</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="center">2</td>
<td align="right">-1.9363321</td>
<td align="right">0.489223</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="right">7</td>
<td align="right">8</td>
<td align="center">2</td>
<td align="right">11.0057900</td>
<td align="right">10.246458</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="right">8</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="right">4.5909990</td>
<td align="right">8.423307</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="right">7.4045039</td>
<td align="right">3.696619</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="right">2</td>
<td align="right">5</td>
<td align="center">1</td>
<td align="right">5.0411598</td>
<td align="right">1.367036</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="right">5.7164010</td>
<td align="right">6.532631</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="right">9</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="right">4.0282981</td>
<td align="right">9.368644</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="center">2</td>
<td align="right">0.4270119</td>
<td align="right">2.818805</td>
<td align="center">2</td>
</tr>
</tbody>
</table>
<p>또한 식 <a href="da.html#eq:lda-posterior">(3.5)</a>에 따른 사후확률과 식 <a href="da.html#eq:lda-posterior-rule">(3.6)</a>에 따른 추정범주는 아래와 같이 얻어진다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lda_posterior_result_df &lt;-<span class="st"> </span>train_df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">f1 =</span> mvtnorm<span class="op">::</span><span class="kw">dmvnorm</span>(
      .[<span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)],
      mu_hat[<span class="dv">1</span>, <span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unlist</span>(),
      Sigma_hat),
    <span class="dt">f2 =</span> mvtnorm<span class="op">::</span><span class="kw">dmvnorm</span>(
      .[<span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)],
      mu_hat[<span class="dv">2</span>, <span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unlist</span>(),
      Sigma_hat),
    <span class="dt">f =</span> n_obs<span class="op">$</span>pi[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>f1 <span class="op">+</span><span class="st"> </span>n_obs<span class="op">$</span>pi[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>f2
    ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">p1 =</span> n_obs<span class="op">$</span>pi[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>f1 <span class="op">/</span><span class="st"> </span>f,
    <span class="dt">p2 =</span> n_obs<span class="op">$</span>pi[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>f2 <span class="op">/</span><span class="st"> </span>f
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">predicted_class =</span> <span class="kw">factor</span>(<span class="kw">if_else</span>(p1 <span class="op">&gt;=</span><span class="st"> </span>p2, <span class="dv">1</span>, <span class="dv">2</span>), <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(
    id, x1, x2, class, p1, p2, predicted_class
  )

knitr<span class="op">::</span><span class="kw">kable</span>(
  lda_posterior_result_df,
  <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
  <span class="dt">align =</span> <span class="kw">rep</span>(<span class="st">&#39;r&#39;</span>, <span class="kw">dim</span>(lda_posterior_result_df)[<span class="dv">2</span>]),
  <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;객체번호&#39;</span>, <span class="st">&#39;$x_1$&#39;</span>, <span class="st">&#39;$x_2$&#39;</span>,
                <span class="st">&#39;실제범주&#39;</span>, 
                <span class="st">&#39;$P(y = 1 | </span><span class="ch">\\</span><span class="st">mathbf{x})$&#39;</span>, 
                <span class="st">&#39;$P(y = 2 | </span><span class="ch">\\</span><span class="st">mathbf{x})$&#39;</span>,
                <span class="st">&#39;추정범주&#39;</span>),
  <span class="dt">caption =</span> <span class="st">&#39;학습표본에 대한 LDA 적용 결과: 사후확률 및 추정범주&#39;</span>)</code></pre></div>
<table>
<caption><span id="tab:lda-posterior-result">Table 3.5: </span>학습표본에 대한 LDA 적용 결과: 사후확률 및 추정범주</caption>
<thead>
<tr class="header">
<th align="center">객체번호</th>
<th align="right"><span class="math inline">\(x_1\)</span></th>
<th align="right"><span class="math inline">\(x_2\)</span></th>
<th align="center">실제범주</th>
<th align="right"><span class="math inline">\(P(y = 1 | \mathbf{x})\)</span></th>
<th align="right"><span class="math inline">\(P(y = 2 | \mathbf{x})\)</span></th>
<th align="center">추정범주</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="right">5</td>
<td align="right">7</td>
<td align="center">1</td>
<td align="right">0.9032273</td>
<td align="right">0.0967727</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="center">2</td>
<td align="right">0.0812446</td>
<td align="right">0.9187554</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="right">7</td>
<td align="right">8</td>
<td align="center">2</td>
<td align="right">0.6812088</td>
<td align="right">0.3187912</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="right">8</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="right">0.0212004</td>
<td align="right">0.9787996</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="right">0.9760579</td>
<td align="right">0.0239421</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="right">2</td>
<td align="right">5</td>
<td align="center">1</td>
<td align="right">0.9752562</td>
<td align="right">0.0247438</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="right">0.3065644</td>
<td align="right">0.6934356</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="right">9</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="right">0.0047713</td>
<td align="right">0.9952287</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="center">2</td>
<td align="right">0.0838007</td>
<td align="right">0.9161993</td>
<td align="center">2</td>
</tr>
</tbody>
</table>
<p>패키지 <code>MASS</code>내의 함수 <code>lda</code>를 통해 위 Table <a href="da.html#tab:lda-posterior-result">3.5</a> 결과를 간편하게 얻을 수 있다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lda_fit &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">lda</span>(class <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, train_df)

train_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_cols</span>(
  <span class="kw">predict</span>(lda_fit, train_df)<span class="op">$</span>posterior <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    `</span><span class="dt">colnames&lt;-</span><span class="st">`</span>(<span class="kw">paste0</span>(<span class="st">&quot;p&quot;</span>, <span class="kw">colnames</span>(.))) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_data_frame</span>()
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">predicted_class =</span> <span class="kw">predict</span>(lda_fit, .)<span class="op">$</span>class
    )</code></pre></div>
<pre><code>## # A tibble: 9 x 7
##      id    x1    x2 class      p1     p2 predicted_class
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;          
## 1     1     5     7 1     0.903   0.0968 1              
## 2     2     4     3 2     0.0812  0.919  2              
## 3     3     7     8 2     0.681   0.319  1              
## 4     4     8     6 2     0.0212  0.979  2              
## 5     5     3     6 1     0.976   0.0239 1              
## 6     6     2     5 1     0.975   0.0247 1              
## 7     7     6     6 1     0.307   0.693  2              
## 8     8     9     6 2     0.00477 0.995  2              
## 9     9     5     4 2     0.0838  0.916  2</code></pre>
<p>위 결과들은 교재 <span class="citation">(전치혁 <a href="#ref-jun2012datamining">2012</a>)</span>의 예제 결과와는 다소 차이가 있는데, 이는 교재에서는 사전확률을 학습표본 내 비율 대신 <span class="math inline">\(\pi_1 = \pi_2 = 0.5\)</span>로 지정하였기 때문이다. 교재와 동일한 결과는 아래의 스크립트처럼 <code>lda</code> 함수 실행 시 사전확률 파리미터 <code>prior</code>의 값을 지정함으로써 얻을 수 있다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lda_fit_equal_prior &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">lda</span>(class <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, train_df, <span class="dt">prior =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>))

train_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_cols</span>(
  <span class="kw">predict</span>(lda_fit_equal_prior, train_df)<span class="op">$</span>posterior <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    `</span><span class="dt">colnames&lt;-</span><span class="st">`</span>(<span class="kw">paste0</span>(<span class="st">&quot;p&quot;</span>, <span class="kw">colnames</span>(.))) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_data_frame</span>()
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">predicted_class =</span> <span class="kw">predict</span>(lda_fit_equal_prior, .)<span class="op">$</span>class
    ) <span class="op">%&gt;%</span>
<span class="st">  </span>knitr<span class="op">::</span><span class="kw">kable</span>(
    <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
    <span class="dt">align =</span> <span class="kw">rep</span>(<span class="st">&#39;r&#39;</span>, <span class="kw">dim</span>(lda_posterior_result_df)[<span class="dv">2</span>]),
    <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;객체번호&#39;</span>, <span class="st">&#39;$x_1$&#39;</span>, <span class="st">&#39;$x_2$&#39;</span>,
                <span class="st">&#39;실제범주&#39;</span>, 
                <span class="st">&#39;$P(y = 1 | </span><span class="ch">\\</span><span class="st">mathbf{x})$&#39;</span>, 
                <span class="st">&#39;$P(y = 2 | </span><span class="ch">\\</span><span class="st">mathbf{x})$&#39;</span>,
                <span class="st">&#39;추정범주&#39;</span>),
    <span class="dt">caption =</span> <span class="st">&#39;학습표본에 대한 LDA 적용 결과: 사후확률 및 추정범주 (사전확률 = 0.5)&#39;</span>)</code></pre></div>
<table>
<caption><span id="tab:lda-posterior-result-equal-prior">Table 3.6: </span>학습표본에 대한 LDA 적용 결과: 사후확률 및 추정범주 (사전확률 = 0.5)</caption>
<thead>
<tr class="header">
<th align="center">객체번호</th>
<th align="right"><span class="math inline">\(x_1\)</span></th>
<th align="right"><span class="math inline">\(x_2\)</span></th>
<th align="center">실제범주</th>
<th align="right"><span class="math inline">\(P(y = 1 | \mathbf{x})\)</span></th>
<th align="right"><span class="math inline">\(P(y = 2 | \mathbf{x})\)</span></th>
<th align="center">추정범주</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="right">5</td>
<td align="right">7</td>
<td align="center">1</td>
<td align="right">0.9210538</td>
<td align="right">0.0789462</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="center">2</td>
<td align="right">0.0995341</td>
<td align="right">0.9004659</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="right">7</td>
<td align="right">8</td>
<td align="center">2</td>
<td align="right">0.7275992</td>
<td align="right">0.2724008</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="right">8</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="right">0.0263608</td>
<td align="right">0.9736392</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="right">0.9807542</td>
<td align="right">0.0192458</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="right">2</td>
<td align="right">5</td>
<td align="center">1</td>
<td align="right">0.9801065</td>
<td align="right">0.0198935</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="right">0.3559269</td>
<td align="right">0.6440731</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="right">9</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="right">0.0059571</td>
<td align="right">0.9940429</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="center">2</td>
<td align="right">0.1026013</td>
<td align="right">0.8973987</td>
<td align="center">2</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="lda-misclassification-cost" class="section level2">
<h2><span class="header-section-number">3.5</span> 오분류비용을 고려한 분류규칙</h2>
<p>위 Table <a href="da.html#tab:lda-posterior-result">3.5</a>의 객체 3, 7와 같이 선형분류함수가 모든 객체의 범주를 정확하게 추정하지 못하고 오분류가 발생하는 경우가 있다. 이 때 다음과 같이 두 종류의 오분류 비용이 있다고 가정하자.</p>
<ul>
<li><span class="math inline">\(C(1 \, | \, 2)\)</span>: 범주 2를 1로 잘못 분류 시 초래 비용</li>
<li><span class="math inline">\(C(2 \, | \, 1)\)</span>: 범주 1를 2로 잘못 분류 시 초래 비용</li>
</ul>
<p>이 때 총 기대 오분류 비용은 다음과 같다.</p>
<span class="math display" id="eq:expected-misclassification-cost">\[\begin{equation}
C(1 \, | \, 2) \pi_2 \int_{\mathbf{x} \in R_1} f_2(\mathbf{x}) d\mathbf{x} + C(2 \, | \, 1) \pi_1 \int_{\mathbf{x} \in R_2} f_1(\mathbf{x}) d\mathbf{x}
\tag{3.9}
\end{equation}\]</span>
<p>여기에서 <span class="math inline">\(R_1 \subset \mathbb{R}^p, R_2 = \mathbb{R}^p - R_{1}\)</span>는 판별함수에 의해 각각 범주 1, 2로 분류되는 판별변수 영역을 나타낸다. 즉,</p>
<span class="math display">\[\begin{equation*}
\hat{y} = \begin{cases}
    1, &amp; \text{if } \mathbf{x} \in R_1  \\
    2, &amp; \text{otherwise}
\end{cases}
\end{equation*}\]</span>
<p>식 <a href="da.html#eq:expected-misclassification-cost">(3.9)</a>을 최소화하는 영역 <span class="math inline">\(R_1, R_2\)</span>는 아래와 같다.</p>
<span class="math display">\[\begin{eqnarray*}
R_1 &amp;=&amp; \left\{\mathbf{x} \in \mathbb{R}^p \, :  \, \frac{f_1(\mathbf{x})}{f_2(\mathbf{x})} \ge \frac{\pi_2}{\pi_1} \left( \frac{C(1 \, | \, 2)}{C(2 \, | \, 1)} \right) \right\}\\
R_2 &amp;=&amp; \left\{\mathbf{x} \in \mathbb{R}^p \, :  \, \frac{f_1(\mathbf{x})}{f_2(\mathbf{x})} &lt; \frac{\pi_2}{\pi_1} \left( \frac{C(1 \, | \, 2)}{C(2 \, | \, 1)} \right) \right\}
\end{eqnarray*}\]</span>
<p>위 중 <span class="math inline">\(R_1\)</span>에 대한 식을 아래와 같이 단계적으로 전개할 수 있다.</p>
<span class="math display">\[\begin{eqnarray*}
R_1 &amp;=&amp; \left\{\mathbf{x} \in \mathbb{R}^p \, :  \, \frac{\pi_1 f_1(\mathbf{x})}{\pi_2 f_2(\mathbf{x})} \ge \frac{C(1 \, | \, 2)}{C(2 \, | \, 1)} \right\}\\
&amp;=&amp; \left\{\mathbf{x} \in \mathbb{R}^p \, :  \, \frac{\frac{\pi_1 f_1(\mathbf{x})}{\pi_1 f_1(\mathbf{x}) + \pi_2 f_2(\mathbf{x})}}{\frac{\pi_2 f_2(\mathbf{x})}{\pi_1 f_1(\mathbf{x}) + \pi_2 f_2(\mathbf{x})}} \ge \frac{C(1 \, | \, 2)}{C(2 \, | \, 1)} \right\}\\
&amp;=&amp; \left\{\mathbf{x} \in \mathbb{R}^p \, :  \, \frac{P(y = 1 \, | \, \mathbf{x})}{P(y = 2 \, | \, \mathbf{x})} \ge \frac{C(1 \, | \, 2)}{C(2 \, | \, 1)} \right\}\\
&amp;=&amp; \left\{\mathbf{x} \in \mathbb{R}^p \, :  \, C(2 \, | \, 1) P(y = 1 \, | \, \mathbf{x}) \ge C(1 \, | \, 2) P(y = 2 \, | \, \mathbf{x}) \right\}
\end{eqnarray*}\]</span>
<p>따라서 오분류비용을 고려한 분류규칙은 1) 사후확률에 오분류 비용을 곱한 뒤, 2) 그 값이 큰 범주로 분류하여 오분류비용을 최소화한다.</p>
<p>Table <a href="da.html#tab:lda-posterior-result">3.5</a>에 오분류비용 <span class="math inline">\(C(1 \, | \, 2) = 1, C(2 \, | \, 1) = 5\)</span>를 적용한 결과는 아래와 같이 구할 수 있다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lda_fit &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">lda</span>(class <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, train_df)

misclassification_cost &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">1</span>)

lda_unequal_cost_result_df &lt;-<span class="st"> </span>train_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_cols</span>(
  <span class="kw">predict</span>(lda_fit, train_df)<span class="op">$</span>posterior <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(misclassification_cost) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">as_data_frame</span>() <span class="op">%&gt;%</span>
<span class="st">    `</span><span class="dt">names&lt;-</span><span class="st">`</span>(<span class="kw">paste0</span>(<span class="st">&quot;s&quot;</span>, lda_fit<span class="op">$</span>lev)) 
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">predicted_class =</span> <span class="kw">factor</span>(<span class="kw">if_else</span>(s1 <span class="op">&gt;=</span><span class="st"> </span>s2, <span class="dv">1</span>, <span class="dv">2</span>), <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
    )

knitr<span class="op">::</span><span class="kw">kable</span>(
  lda_unequal_cost_result_df,
  <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
  <span class="dt">align =</span> <span class="kw">rep</span>(<span class="st">&#39;r&#39;</span>, <span class="kw">dim</span>(lda_unequal_cost_result_df)[<span class="dv">2</span>]),
  <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;객체번호&#39;</span>, <span class="st">&#39;$x_1$&#39;</span>, <span class="st">&#39;$x_2$&#39;</span>,
                <span class="st">&#39;실제범주&#39;</span>, 
                <span class="st">&#39;$C(2 </span><span class="ch">\\</span><span class="st">, | </span><span class="ch">\\</span><span class="st">, 1) P(y = 1 | </span><span class="ch">\\</span><span class="st">mathbf{x})$&#39;</span>, 
                <span class="st">&#39;$C(1 </span><span class="ch">\\</span><span class="st">, | </span><span class="ch">\\</span><span class="st">, 2) P(y = 2 | </span><span class="ch">\\</span><span class="st">mathbf{x})$&#39;</span>,
                <span class="st">&#39;추정범주&#39;</span>),
  <span class="dt">caption =</span> <span class="st">&#39;학습표본에 대한 오분류 비용을 고려한 LDA 적용 결과&#39;</span>)</code></pre></div>
<table>
<caption><span id="tab:lda-unequal-cost-result">Table 3.7: </span>학습표본에 대한 오분류 비용을 고려한 LDA 적용 결과</caption>
<thead>
<tr class="header">
<th align="center">객체번호</th>
<th align="right"><span class="math inline">\(x_1\)</span></th>
<th align="right"><span class="math inline">\(x_2\)</span></th>
<th align="center">실제범주</th>
<th align="right"><span class="math inline">\(C(2 \, | \, 1) P(y = 1 | \mathbf{x})\)</span></th>
<th align="right"><span class="math inline">\(C(1 \, | \, 2) P(y = 2 | \mathbf{x})\)</span></th>
<th align="center">추정범주</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="right">5</td>
<td align="right">7</td>
<td align="center">1</td>
<td align="right">4.5161363</td>
<td align="right">0.0967727</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="center">2</td>
<td align="right">0.4062232</td>
<td align="right">0.9187554</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="right">7</td>
<td align="right">8</td>
<td align="center">2</td>
<td align="right">3.4060438</td>
<td align="right">0.3187912</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="right">8</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="right">0.1060019</td>
<td align="right">0.9787996</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="right">4.8802897</td>
<td align="right">0.0239421</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="right">2</td>
<td align="right">5</td>
<td align="center">1</td>
<td align="right">4.8762808</td>
<td align="right">0.0247438</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="right">1.5328222</td>
<td align="right">0.6934356</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="right">9</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="right">0.0238567</td>
<td align="right">0.9952287</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="center">2</td>
<td align="right">0.4190033</td>
<td align="right">0.9161993</td>
<td align="center">2</td>
</tr>
</tbody>
</table>
<p>위 Table <a href="da.html#tab:lda-unequal-cost-result">3.7</a>에서 보는 바와 같이 오분류 객체는 3로, 이전 장의 Table <a href="da.html#tab:lda-posterior-result">3.5</a>에 비해 실제범주가 1인 객체를 더 정확하게 분류함을 확인할 수 있다. 범주 1인 객체를 범주 2로 분류할 때 발생하는 비용이 범주 2인 객체를 범주 1로 분류할 때 발생하는 비용보다 다섯 배나 크기 때문에, 오분류비용을 고려한 분류규칙은 실제 범주가 2인 객체를 범주 2로 정확하게 분류할 확률이 줄어든다 할지라도, 실제 범주가 1인 객체를 범주 1로 정확하게 분류하는 확률을 높이는 방향으로 학습된다.</p>
</div>
<div id="qda" class="section level2">
<h2><span class="header-section-number">3.6</span> 이차판별분석</h2>
<p>이차판별분석은 판별함수가 변수들에 대한 이차함수로 표현되는 경우인데, 각 범주에 대한 변수벡터 <span class="math inline">\(\mathbf{x}\)</span>가 서로 다른 분산-공분산행렬을 갖는 다변량 정규분포를 따를 때 의사결정론에 의한 분류규칙으로부터 유도된다.</p>
<div id="qda-basic-script" class="section level3">
<h3><span class="header-section-number">3.6.1</span> 기본 R 스크립트</h3>
<p>Table <a href="da.html#tab:da-train-data-table">3.1</a>의 학습표본에 대해 이차판별분석을 적용하는 R 스크립트는 아래에 보이는 것과 같이 <code>MASS</code> 패키지의 <code>qda</code> 함수를 사용한다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">qda_fit &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">qda</span>(class <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, train_df)

<span class="kw">print</span>(qda_fit)</code></pre></div>
<pre><code>## Call:
## qda(class ~ x1 + x2, data = train_df)
## 
## Prior probabilities of groups:
##         1         2 
## 0.4444444 0.5555556 
## 
## Group means:
##    x1  x2
## 1 4.0 6.0
## 2 6.6 5.4</code></pre>
</div>
<div id="qda-function" class="section level3">
<h3><span class="header-section-number">3.6.2</span> 이차 판별함수</h3>
<p>각 범주의 확률밀도함수는 아래와 같이 다변량 정규분포로 정의된다.</p>
<span class="math display" id="eq:qda-mv-gaussian-dist">\[\begin{equation}
f_k(\mathbf{x}) = \frac{1}{(2\pi)^{p/2}|\boldsymbol\Sigma_k|^{1/2}} \exp \{ -\frac{1}{2} \left(\mathbf{x} - \boldsymbol\mu_k\right)^\top \boldsymbol\Sigma_k^{-1} \left(\mathbf{x} - \boldsymbol\mu_k\right) \}
\tag{3.10}
\end{equation}\]</span>
<p>위 식 <a href="da.html#eq:qda-mv-gaussian-dist">(3.10)</a>이 선형판별함수에서 사용한 식 <a href="da.html#eq:mv-gaussian-dist">(3.4)</a>과 다른 부분은 분산-공분산분포 <span class="math inline">\(\boldsymbol\Sigma_k\)</span>가 범주 <span class="math inline">\(k\)</span>에 대해 각각 정의된다는 점이다.</p>
<p>이 경우 각 범주에 대한 판별함수는 아래와 같이 정의된다.</p>
<span class="math display" id="eq:qda-discriminant-function">\[\begin{equation}
u_k(\mathbf{x}) = - \frac{1}{2} (\mathbf{x} - \boldsymbol\mu_k)^\top \boldsymbol\Sigma^{-1} (\mathbf{x} - \boldsymbol\mu_k) - \frac{1}{2} \ln \left| \boldsymbol\Sigma_k \right| + \ln \pi_k
\tag{3.11}
\end{equation}\]</span>
<p>데이터 행렬 <span class="math inline">\(X = (\mathbf{x}_1, \mathbf{x}_2, \cdots , \mathbf{x}_N)\)</span>의 각 객체에 대한 판별함수값을 얻는 함수를 아래와 같이 구현할 수 있다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">qda_discriminant_func &lt;-<span class="st"> </span><span class="cf">function</span>(X, mu, Sigma, pi) {
  Sigma_inv_sqrt &lt;-<span class="st"> </span><span class="kw">chol</span>(<span class="kw">solve</span>(Sigma))
  <span class="op">-</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">rowSums</span>((<span class="kw">t</span>(X <span class="op">-</span><span class="st"> </span>mu) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Sigma_inv_sqrt))<span class="op">^</span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">det</span>(Sigma)) <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(pi)
}</code></pre></div>
</div>
<div id="qda-discriminant-rule" class="section level3">
<h3><span class="header-section-number">3.6.3</span> 이차판별함수에 의한 분류</h3>
<p>분류기준은 선형판별분석과 마찬가지로 판별함수값이 큰 범주로 분류한다.</p>
<span class="math display">\[\begin{equation*}
\hat{y} = \begin{cases}
    1, &amp; \text{if } u_1(\mathbf{x}) \ge u_2(\mathbf{x})  \\
    2, &amp; \text{otherwise}
\end{cases}
\end{equation*}\]</span>
<p>Table <a href="da.html#tab:da-train-data-table">3.1</a>의 학습표본에 대해 이차판별함수값을 계산하고 범주를 추정하면 아래와 같다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">qda_discriminant_result_df &lt;-<span class="st"> </span>train_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(
  <span class="dt">u1 =</span> <span class="kw">qda_discriminant_func</span>(
      .[<span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">t</span>(),
      mu_hat[<span class="dv">1</span>, <span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unlist</span>(),
      S_within_group[[<span class="dv">1</span>]],
      n_obs<span class="op">$</span>pi[<span class="dv">1</span>]
      ),
  <span class="dt">u2 =</span> <span class="kw">qda_discriminant_func</span>(
      .[<span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">t</span>(),
      mu_hat[<span class="dv">2</span>, <span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unlist</span>(),
      S_within_group[[<span class="dv">2</span>]],
      n_obs<span class="op">$</span>pi[<span class="dv">2</span>]
      )
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">predicted_class =</span> <span class="kw">factor</span>(<span class="kw">if_else</span>(u1 <span class="op">&gt;=</span><span class="st"> </span>u2, <span class="dv">1</span>, <span class="dv">2</span>), <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
  )

knitr<span class="op">::</span><span class="kw">kable</span>(
  qda_discriminant_result_df,
  <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
  <span class="dt">align =</span> <span class="kw">rep</span>(<span class="st">&#39;r&#39;</span>, <span class="kw">dim</span>(qda_discriminant_result_df)[<span class="dv">2</span>]),
  <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;객체번호&#39;</span>, <span class="st">&#39;$x_1$&#39;</span>, <span class="st">&#39;$x_2$&#39;</span>,
                <span class="st">&#39;실제범주&#39;</span>, <span class="st">&#39;$u_1(</span><span class="ch">\\</span><span class="st">mathbf{x})$&#39;</span>, <span class="st">&#39;$u_2(</span><span class="ch">\\</span><span class="st">mathbf{x})$&#39;</span>,
                <span class="st">&#39;추정범주&#39;</span>),
  <span class="dt">caption =</span> <span class="st">&#39;학습표본에 대한 QDA 적용 결과: 판별함수값 및 추정범주&#39;</span>)</code></pre></div>
<table>
<caption><span id="tab:qda-discriminant-result">Table 3.8: </span>학습표본에 대한 QDA 적용 결과: 판별함수값 및 추정범주</caption>
<thead>
<tr class="header">
<th align="center">객체번호</th>
<th align="right"><span class="math inline">\(x_1\)</span></th>
<th align="right"><span class="math inline">\(x_2\)</span></th>
<th align="center">실제범주</th>
<th align="right"><span class="math inline">\(u_1(\mathbf{x})\)</span></th>
<th align="right"><span class="math inline">\(u_2(\mathbf{x})\)</span></th>
<th align="center">추정범주</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="right">5</td>
<td align="right">7</td>
<td align="center">1</td>
<td align="right">-1.729447</td>
<td align="right">-3.950639</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="center">2</td>
<td align="right">-13.183993</td>
<td align="right">-2.497284</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="right">7</td>
<td align="right">8</td>
<td align="center">2</td>
<td align="right">-3.911266</td>
<td align="right">-3.145402</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="right">8</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="right">-5.274902</td>
<td align="right">-1.868806</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="right">-1.183993</td>
<td align="right">-5.764060</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="right">2</td>
<td align="right">5</td>
<td align="center">1</td>
<td align="right">-1.729447</td>
<td align="right">-6.202685</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="right">-2.002175</td>
<td align="right">-1.934273</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="right">9</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="right">-7.729447</td>
<td align="right">-2.582391</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="center">2</td>
<td align="right">-8.274902</td>
<td align="right">-1.927726</td>
<td align="center">2</td>
</tr>
</tbody>
</table>
<p>위 Table <a href="da.html#tab:qda-discriminant-result">3.8</a>에서 보듯이 모든 학습객체가 올바로 분류되고 있다.</p>
<p>또한 선형판별분석의 경우와 마찬가지로 사후확률 비교를 통한 범주 분류를 수행할 수 있다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">qda_posterior_result_df &lt;-<span class="st"> </span>train_df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">f1 =</span> mvtnorm<span class="op">::</span><span class="kw">dmvnorm</span>(
      .[<span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)],
      mu_hat[<span class="dv">1</span>, <span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unlist</span>(),
      S_within_group[[<span class="dv">1</span>]]),
    <span class="dt">f2 =</span> mvtnorm<span class="op">::</span><span class="kw">dmvnorm</span>(
      .[<span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)],
      mu_hat[<span class="dv">2</span>, <span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>)] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unlist</span>(),
      S_within_group[[<span class="dv">2</span>]]),
    <span class="dt">f =</span> n_obs<span class="op">$</span>pi[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>f1 <span class="op">+</span><span class="st"> </span>n_obs<span class="op">$</span>pi[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>f2
    ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">p1 =</span> n_obs<span class="op">$</span>pi[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>f1 <span class="op">/</span><span class="st"> </span>f,
    <span class="dt">p2 =</span> n_obs<span class="op">$</span>pi[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>f2 <span class="op">/</span><span class="st"> </span>f
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">predicted_class =</span> <span class="kw">factor</span>(<span class="kw">if_else</span>(p1 <span class="op">&gt;=</span><span class="st"> </span>p2, <span class="dv">1</span>, <span class="dv">2</span>), <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(
    id, x1, x2, class, p1, p2, predicted_class
  )

knitr<span class="op">::</span><span class="kw">kable</span>(
  qda_posterior_result_df,
  <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
  <span class="dt">align =</span> <span class="kw">rep</span>(<span class="st">&#39;r&#39;</span>, <span class="kw">dim</span>(qda_posterior_result_df)[<span class="dv">2</span>]),
  <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;객체번호&#39;</span>, <span class="st">&#39;$x_1$&#39;</span>, <span class="st">&#39;$x_2$&#39;</span>,
                <span class="st">&#39;실제범주&#39;</span>, 
                <span class="st">&#39;$P(y = 1 | </span><span class="ch">\\</span><span class="st">mathbf{x})$&#39;</span>, 
                <span class="st">&#39;$P(y = 2 | </span><span class="ch">\\</span><span class="st">mathbf{x})$&#39;</span>,
                <span class="st">&#39;추정범주&#39;</span>),
  <span class="dt">caption =</span> <span class="st">&#39;학습표본에 대한 QDA 적용 결과: 사후확률 및 추정범주&#39;</span>)</code></pre></div>
<table>
<caption><span id="tab:qda-posterior-result">Table 3.9: </span>학습표본에 대한 QDA 적용 결과: 사후확률 및 추정범주</caption>
<thead>
<tr class="header">
<th align="center">객체번호</th>
<th align="right"><span class="math inline">\(x_1\)</span></th>
<th align="right"><span class="math inline">\(x_2\)</span></th>
<th align="center">실제범주</th>
<th align="right"><span class="math inline">\(P(y = 1 | \mathbf{x})\)</span></th>
<th align="right"><span class="math inline">\(P(y = 2 | \mathbf{x})\)</span></th>
<th align="center">추정범주</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="right">5</td>
<td align="right">7</td>
<td align="center">1</td>
<td align="right">0.9021365</td>
<td align="right">0.0978635</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="center">2</td>
<td align="right">0.0000228</td>
<td align="right">0.9999772</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="right">7</td>
<td align="right">8</td>
<td align="center">2</td>
<td align="right">0.3173746</td>
<td align="right">0.6826254</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="right">8</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="right">0.0321055</td>
<td align="right">0.9678945</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="right">0.9898499</td>
<td align="right">0.0101501</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="right">2</td>
<td align="right">5</td>
<td align="center">1</td>
<td align="right">0.9887184</td>
<td align="right">0.0112816</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="center">1</td>
<td align="right">0.4830310</td>
<td align="right">0.5169690</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="right">9</td>
<td align="right">6</td>
<td align="center">2</td>
<td align="right">0.0057829</td>
<td align="right">0.9942171</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="center">2</td>
<td align="right">0.0017486</td>
<td align="right">0.9982514</td>
<td align="center">2</td>
</tr>
</tbody>
</table>
<p>이 또한 <code>MASS</code> 패키지의 <code>predict.qda</code> 함수를 통해 아래와 같이 동일한 결과값을 보다 간편하게 얻을 수 있다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_cols</span>(
  <span class="kw">predict</span>(qda_fit, train_df)<span class="op">$</span>posterior <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    `</span><span class="dt">colnames&lt;-</span><span class="st">`</span>(<span class="kw">paste0</span>(<span class="st">&quot;p&quot;</span>, <span class="kw">colnames</span>(.))) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_data_frame</span>()
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">predicted_class =</span> <span class="kw">predict</span>(qda_fit, .)<span class="op">$</span>class
    )</code></pre></div>
<pre><code>## # A tibble: 9 x 7
##      id    x1    x2 class        p1     p2 predicted_class
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;          
## 1     1     5     7 1     0.902     0.0979 1              
## 2     2     4     3 2     0.0000228 1.000  2              
## 3     3     7     8 2     0.317     0.683  2              
## 4     4     8     6 2     0.0321    0.968  2              
## 5     5     3     6 1     0.990     0.0102 1              
## 6     6     2     5 1     0.989     0.0113 1              
## 7     7     6     6 1     0.483     0.517  2              
## 8     8     9     6 2     0.00578   0.994  2              
## 9     9     5     4 2     0.00175   0.998  2</code></pre>
</div>
</div>
<div id="da-multiclass" class="section level2">
<h2><span class="header-section-number">3.7</span> 세 범주 이상의 분류</h2>
<div id="mutliclass-da-basic-script" class="section level3">
<h3><span class="header-section-number">3.7.1</span> 기본 R 스크립트</h3>
<p>3개의 범주를 지닌 붓꽃(iris) 데이터에 대해 선형판별분석을 적용하는 R 스크립트는 아래와 같다. 본 예제에서는 각 범주별 50개 데이터 중 첫 30개 관측치만을 학습표본으로 삼아 판별함수를 유도한다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_train_df &lt;-<span class="st"> </span>datasets<span class="op">::</span>iris <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">rename</span>(<span class="dt">x1 =</span> Sepal.Length,
         <span class="dt">x2 =</span> Sepal.Width,
         <span class="dt">x3 =</span> Petal.Length,
         <span class="dt">x4 =</span> Petal.Width,
         <span class="dt">class =</span> Species) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(class) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">30</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">id =</span> <span class="kw">row_number</span>())

iris_lda_fit &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">lda</span>(class <span class="op">~</span><span class="st"> </span>. <span class="op">-</span>id, iris_train_df)

<span class="kw">print</span>(iris_lda_fit)</code></pre></div>
<pre><code>## Call:
## lda(class ~ . - id, data = iris_train_df)
## 
## Prior probabilities of groups:
##     setosa versicolor  virginica 
##  0.3333333  0.3333333  0.3333333 
## 
## Group means:
##                  x1       x2       x3        x4
## setosa     5.026667 3.450000 1.473333 0.2466667
## versicolor 6.070000 2.790000 4.333333 1.3533333
## virginica  6.583333 2.933333 5.603333 2.0066667
## 
## Coefficients of linear discriminants:
##           LD1        LD2
## x1  0.5711419 -1.2397647
## x2  1.8752911  3.0223980
## x3 -1.7361767  0.3159667
## x4 -3.4672646  1.3954748
## 
## Proportion of trace:
##    LD1    LD2 
## 0.9929 0.0071</code></pre>
</div>
<div id="mutliclass-generalized-discriminant-function" class="section level3">
<h3><span class="header-section-number">3.7.2</span> 일반화된 판별함수</h3>
<p><span class="math inline">\(K (&gt; 2)\)</span>개의 범주가 있는 경우에 대한 판별분석은 아래와 같이 일반화된다.</p>
<ul>
<li><span class="math inline">\(\pi_k\)</span>: 범주 <span class="math inline">\(k\)</span>에 속할 사전확률, <span class="math inline">\(k = 1, 2, \cdots, K\)</span></li>
<li><span class="math inline">\(C(k&#39; \, | \, k) \ge 0\)</span>: 실제 범주 <span class="math inline">\(k\)</span>에 속하는 데 범주 <span class="math inline">\(k&#39;\)</span>로 분류할 때 소요 비용 (<span class="math inline">\(C(k&#39; \, | \, k) = 0 \text{ if } k&#39; = k\)</span>)</li>
<li><span class="math inline">\(f_k(\mathbf{x})\)</span>: 범주 <span class="math inline">\(k\)</span>에 속하는 <span class="math inline">\(\mathbf{x}\)</span>의 확률밀도함수</li>
<li><span class="math inline">\(R_k \subset \mathbb{R}^p\)</span>: 범주 <span class="math inline">\(k\)</span>로 분류되는 <span class="math inline">\(\mathbf{x}\)</span>의 영역</li>
<li><span class="math inline">\(P(k&#39; \, | \, k) = \int_{\mathbf{x} \in R_{k&#39;}} f_k(\mathbf{x}) d\mathbf{x}\)</span>: 실제범주 <span class="math inline">\(k\)</span>에 속하는 데 범주 <span class="math inline">\(k&#39;\)</span>로 분류할 확률</li>
</ul>
<p>이 때, 총 기대 오분류 비용은 아래와 같다.</p>
<span class="math display">\[\begin{equation*}
\sum_{k = 1}^{K} \pi_k \sum_{k&#39; \neq k} C(k&#39; \, | \, k) \int_{\mathbf{x} \in R_{k&#39;}} f_k(\mathbf{x}) d\mathbf{x}
\end{equation*}\]</span>
<p>따라서 분류문제는 위 총 기대 오분류 비용을 최소화하는 <span class="math inline">\(R_1, \cdots, R_K\)</span>를 찾는 것이다.</p>
<p>우선, 범주를 고려하지 않은 <span class="math inline">\(\mathbf{x}\)</span>의 확률밀도함수는 아래와 같이 정의된다.</p>
<span class="math display">\[\begin{equation*}
f(\mathbf{x}) = \sum_{k=1}^{K} \pi_k f_k(\mathbf{x})
\end{equation*}\]</span>
<p>베이즈 정리에 의하여, 변수 <span class="math inline">\(\mathbf{x}\)</span>가 주어졌을 때 범주 <span class="math inline">\(k\)</span>에 속할 사후확률은 아래와 같다.</p>
<span class="math display">\[\begin{equation*}
P(y = k \,|\, \mathbf{x}) = \frac{\pi_k f_k(\mathbf{x})}{f(\mathbf{x})}
\end{equation*}\]</span>
<p>오분류비용이 동일한 경우에는 각 객체에 대해 위의 사후확률이 가장 큰 범주로 추정한다. 위 식에서</p>
<span class="math display">\[\begin{equation*}
P(y = k \,|\, \mathbf{x}) \propto \pi_k f_k(\mathbf{x})
\end{equation*}\]</span>
<p>이므로, 아래와 같이 범주가 추정된다.</p>
<span class="math display">\[\begin{equation*}
\hat{y} = {arg\,max}_{k} \pi_k f_k(\mathbf{x})
\end{equation*}\]</span>
<p>앞 장들에서 살펴본 것과 마찬가지로, 선형판별분석의 경우 각 범주의 확률밀도함수 <span class="math inline">\(f_k(\mathbf{x})\)</span>가 동일 분산-공분산행렬을 가정하며, 이차판별분석의 경우 서로 다른 분산-공분산행렬을 가정한다.</p>
<p>아래 스크립트는 <code>MASS</code> 패키지의 <code>lda</code> 함수를 통해 각 범주에 속할 사후확률과 범주 추정값을 얻는 과정을 보여준다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_lda_fit &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">lda</span>(class <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span>x3 <span class="op">+</span><span class="st"> </span>x4, iris_train_df)

iris_lda_result &lt;-<span class="st"> </span>iris_train_df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(
    <span class="kw">predict</span>(iris_lda_fit, .)<span class="op">$</span>posterior <span class="op">%&gt;%</span>
<span class="st">      </span><span class="kw">as_data_frame</span>()
    ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">predicted_class =</span> <span class="kw">predict</span>(iris_lda_fit, .)<span class="op">$</span>class
  )

<span class="kw">print</span>(iris_lda_result)</code></pre></div>
<pre><code>## # A tibble: 90 x 10
##       x1    x2    x3    x4 class    id setosa versicolor virginica
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt;  &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;
##  1   5.1   3.5   1.4   0.2 seto…     1  1       5.57e-22  6.34e-42
##  2   4.9   3     1.4   0.2 seto…     2  1       3.32e-17  5.67e-36
##  3   4.7   3.2   1.3   0.2 seto…     3  1       2.75e-19  2.14e-38
##  4   4.6   3.1   1.5   0.2 seto…     4  1       8.12e-17  5.62e-35
##  5   5     3.6   1.4   0.2 seto…     5  1       1.14e-22  1.25e-42
##  6   5.4   3.9   1.7   0.4 seto…     6  1       3.20e-21  4.38e-40
##  7   4.6   3.4   1.4   0.3 seto…     7  1       8.74e-19  4.07e-37
##  8   5     3.4   1.5   0.2 seto…     8  1       3.27e-20  1.62e-39
##  9   4.4   2.9   1.4   0.2 seto…     9  1.000   2.22e-15  3.42e-33
## 10   4.9   3.1   1.5   0.1 seto…    10  1       9.36e-19  4.85e-38
## # … with 80 more rows, and 1 more variable: predicted_class &lt;fct&gt;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knitr<span class="op">::</span><span class="kw">kable</span>(
  iris_lda_result <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">select</span>(id, class, predicted_class,
           setosa, versicolor, virginica) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">filter</span>(class <span class="op">!=</span><span class="st"> </span>predicted_class),
  <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
  <span class="dt">align =</span> <span class="kw">rep</span>(<span class="st">&#39;r&#39;</span>, <span class="dv">6</span>),
  <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;객체번호&#39;</span>, <span class="st">&#39;실제범주&#39;</span>, <span class="st">&#39;추정범주&#39;</span>,
                <span class="st">&#39;setosa&#39;</span>, <span class="st">&#39;versicolor&#39;</span>, <span class="st">&#39;virginica&#39;</span>),
  <span class="dt">caption =</span> <span class="st">&#39;붓꽃 학습표본에 대한 LDA 적용 결과 - 오분류 객체 사후 확률&#39;</span>)</code></pre></div>
<table>
<caption><span id="tab:iris-lda">Table 3.10: </span>붓꽃 학습표본에 대한 LDA 적용 결과 - 오분류 객체 사후 확률</caption>
<thead>
<tr class="header">
<th align="center">객체번호</th>
<th align="center">실제범주</th>
<th align="center">추정범주</th>
<th align="right">setosa</th>
<th align="right">versicolor</th>
<th align="right">virginica</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">51</td>
<td align="center">versicolor</td>
<td align="center">virginica</td>
<td align="right">0</td>
<td align="right">0.3088912</td>
<td align="right">0.6911088</td>
</tr>
</tbody>
</table>
<p>아래 스크립트는 <code>MASS</code> 패키지의 <code>qda</code> 함수를 통해 각 범주에 속할 사후확률과 범주 추정값을 얻는 과정을 보여준다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_qda_fit &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">qda</span>(class <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span>x3 <span class="op">+</span><span class="st"> </span>x4, iris_train_df)

iris_qda_result &lt;-<span class="st"> </span>iris_train_df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(
    <span class="kw">predict</span>(iris_qda_fit, .)<span class="op">$</span>posterior <span class="op">%&gt;%</span>
<span class="st">      </span><span class="kw">as_data_frame</span>()
    ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">predicted_class =</span> <span class="kw">predict</span>(iris_qda_fit, .)<span class="op">$</span>class
  )

knitr<span class="op">::</span><span class="kw">kable</span>(
  iris_qda_result <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">select</span>(id, class, predicted_class,
           setosa, versicolor, virginica) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">filter</span>(class <span class="op">!=</span><span class="st"> </span>predicted_class),
  <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,
  <span class="dt">align =</span> <span class="kw">rep</span>(<span class="st">&#39;r&#39;</span>, <span class="dv">6</span>),
  <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;객체번호&#39;</span>, <span class="st">&#39;실제범주&#39;</span>, <span class="st">&#39;추정범주&#39;</span>,
                <span class="st">&#39;setosa&#39;</span>, <span class="st">&#39;versicolor&#39;</span>, <span class="st">&#39;virginica&#39;</span>),
  <span class="dt">caption =</span> <span class="st">&#39;붓꽃 학습표본에 대한 QDA 적용 결과 - 오분류 객체 사후 확률&#39;</span>)</code></pre></div>
<table>
<caption><span id="tab:iris-qda">Table 3.11: </span>붓꽃 학습표본에 대한 QDA 적용 결과 - 오분류 객체 사후 확률</caption>
<thead>
<tr class="header">
<th align="center">객체번호</th>
<th align="center">실제범주</th>
<th align="center">추정범주</th>
<th align="right">setosa</th>
<th align="right">versicolor</th>
<th align="right">virginica</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">51</td>
<td align="center">versicolor</td>
<td align="center">virginica</td>
<td align="right">0</td>
<td align="right">0.4274712</td>
<td align="right">0.5725288</td>
</tr>
</tbody>
</table>
<p>위 결과에서 선형판별분석과 이차판별분석은 동일한 객체를 오분류한다. 해당 객체의 실제 범주에 대한 사후확률은 이차판별분석 결과에서 보다 높게 나타난다.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-jun2012datamining">
<p>전치혁. 2012. <em>데이터마이닝 기법과 응용</em>. 한나래출판사.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="logistic-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tree-based-method.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
