# 서포트 벡터 머신 {#svm}


## 개요 {#svm-overview}

서포트 벡터 머신(suuport vector machine; 이하 SVM)은 기본적으로 두 범주를 갖는 객체들을 분류하는 방법이다. 물론 세 범주 이상의 경우로 확장이 가능하다.

## 필요 R package 설치

본 장에서 필요한 R 패키지들은 아래와 같다.

```{r echo=FALSE, warning=FALSE, message=FALSE}
required_packages <- c("tidyverse", "e1071", "Matrix", "quadprog")
lapply(required_packages, function(x) {
  tibble(package=x, version=packageDescription(x, fields = c("Version"))[1])
}) %>% 
  bind_rows() %>%
  knitr::kable()
```


## 선형 SVM - 분리 가능 경우 {#linear-svm-separable}

### 기본 R 스크립트 {#linear-svm-separable-basic-script}

```{r svm-train-data-table}
train_df <- tibble(
  x1 = c(5, 4, 7, 8, 3, 2, 6, 9, 5),
  x2 = c(7, 3, 8, 6, 6, 5, 6, 6, 4),
  class = c(1, -1, 1, 1, -1, -1, 1, 1, -1)
)

knitr::kable(train_df, booktabs = TRUE,
             align = c('r', 'r', 'r'),
             caption = '학습표본 데이터')
```

Table \@ref(tab:svm-train-data-table)와 같이 두 독립변수 *x1*, *x2*와 이분형 종속변수 *class*의 관측값으로 이루어진 `r dim(train_df)[1]`개의 학습표본을 *train_df*라는 data frame에 저장한다.

```{r linear-svm-basic, fig.cap='선형 SVM 분리 하이퍼플레인', fig.align='center'}
library(e1071)
svm_model <- svm(as.factor(class) ~ x1 + x2, data = train_df, kernel = "linear",
                 cost = 100, scale = FALSE)
plot(svm_model, data = train_df, formula = x2 ~ x1, grid = 200)
```

그림 \@ref(fig:linear-svm-basic)에서 분리 하이퍼플레인은 아래와 같다.

```{r, echo = FALSE}
w <- t(svm_model$coefs) %*% svm_model$SV
b <- (1 - train_df$class[svm_model$index] * (svm_model$SV %*% t(w))) %>%
  `/`(train_df$class[svm_model$index]) %>%
  mean()
```

\[
`r w[1]` x_{1} + `r w[2]` x_{2} = `r -b`
\]


### 기호 정의 {#linear-svm-notation}

본 장에서 사용될 수학적 기호는 아래와 같다.

- $\mathbf{x} \in \mathbb{R}^p$: p차원 변수벡터
- $y \in \{-1, 1\}$: 범주
- $N$: 객체 수
- $(\mathbf{x}_i, y_i)$: $i$번째 객체의 변수벡터와 범주값


### 최적 하이퍼플레인 {#linear-svm-separable-hyperplane}

선형 SVM은 주어진 객체들의 두 범주를 완벽하게 분리하는 하이퍼플레인 중 각 범주의 서포트 벡터들로부터의 거리가 최대가 되는 하이퍼플레인을 찾는 문제로 귀착된다.

우선 아래와 같이 하이퍼플레인을 정의한다.

\begin{equation}
\mathbf{w}^\top \mathbf{x} + b = 0 (\#eq:linear-svm-hyperplane)
\end{equation}

여기서 $\mathbf{w} \in \mathbb{R}^p$와 $b \in \mathbb{R}$이 하이퍼플레인의 계수이다.

범주값이 1인 객체들 중 하이퍼플레인에서 가장 가까운 객체에 대해 다음과 같은 조건이 만족한다고 가정하자.

\[
H_1: \mathbf{w}^\top \mathbf{x} + b = 1 
\]

또한 범주값이 -1인 객체들 중 하이퍼플레인에서 가장 가까운 객체에 대해 다음과 같은 조건이 만족한다고 가정하자.

\[
H_2: \mathbf{w}^\top \mathbf{x} + b = -1
\]

이 때 두 하이퍼플레인 $H_1$과 $H_2$ 간의 거리(margin)는 $2 / \lVert \mathbf{w} \rVert$이다. 선형 SVM은 아래와 같이 $H_1$과 $H_2$ 간의 거리를 최대로 하는 최적화 문제가 된다.

\begin{equation*}
\begin{split}
\max \text{  } & \frac{2}{\mathbf{w}^\top \mathbf{w}}\\
\text{s.t.}& \\
& \mathbf{w}^\top \mathbf{x}_i + b \ge 1 \text{ for } y_i = 1\\
& \mathbf{w}^\top \mathbf{x}_i + b \le -1 \text{ for } y_i = -1
\end{split}
\end{equation*}

이를 간략히 정리하면

\begin{equation*}
\begin{split}
\min \text{  } & \frac{\mathbf{w}^\top \mathbf{w}}{2}\\
\text{s.t.}& \\
& y_i \left( \mathbf{w}^\top \mathbf{x}_i + b \right) \ge 1
\end{split}
\end{equation*}

과 같이 정리할 수 있으며, 각 객체 $i$에 대한 제약조건에 라그랑지 계수(Lagrange multiplier) $\alpha_i \ge 0$를 도입하여 라그랑지 함수를 유도하면 식 \@ref(eq:linear-svm-primal)과 같은 최적화 문제가 된다. 이를 원문제(primal problem)라 하자.

\begin{equation}
\begin{split}
\min \text{  } & L_p = \frac{1}{2} \mathbf{w}^\top \mathbf{w} + \sum_{i = 1}^{N} \alpha_i \left[ y_i \left( \mathbf{w}^\top \mathbf{x}_i + b \right) - 1 \right]\\
\text{s.t.  } & \alpha_i \ge 0, \text{  } i = 1, \cdots, N
\end{split}
(\#eq:linear-svm-primal)
\end{equation}

원문제 식 \@ref(eq:linear-svm-primal)에 대한 울프쌍대문제(Wolfe dual problem)는 아래 식 \@ref(eq:linear-svm-dual)과 같이 도출된다. 보다 자세한 내용은 교재[@jun2012datamining] 참고.

\begin{equation}
\begin{split}
\max \text{  } & L_p = \sum_{i = 1}^{N} \alpha_i - \frac{1}{2} \sum_{i = 1}^{N} \sum_{j = 1}^{N} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j\\
\text{s.t. } &\\
& \sum_{i = 1}^{N} \alpha_i y_i = 0\\
& \alpha_i \ge 0, \text{  } i = 1, \cdots, N
\end{split}
(\#eq:linear-svm-dual)
\end{equation}

식 \@ref(eq:linear-svm-dual)은 이차계획(quadratic programming) 문제로, 각종 소프트웨어와 알고리즘을 이용하여 구할 수 있다. 본 장에서는 `quadprog` 패키지를 이용하여 해를 구하기로 한다. 이는 실제로 `e1071`의 `svm` 함수 호출 시 사용하는 방법은 아니며, 실제 `svm` 함수가 호출하는 알고리즘은 다음 장에서 다시 설명하기로 한다.

`quadprog`의 `solve.QP` 함수는 아래와 같은 형태로 formulation된 문제[@goldfarb1983numerically]에 대한 최적해를 구한다.

\begin{equation}
\begin{split}
\min \text{  } & -\mathbf{d}^{\top}\boldsymbol{\alpha} + \frac{1}{2} \boldsymbol{\alpha}^{\top}\mathbf{D}\boldsymbol{\alpha}\\
\text{s.t. } & \mathbf{A}^{\top}\boldsymbol{\alpha} \ge \mathbf{b}_0
\end{split}
(\#eq:quadprog)
\end{equation}

식 \@ref(eq:quadprog)과 식 \@ref(eq:linear-svm-dual)이 동일한 문제를 나타내도록 아래와 같이 목적함수에 필요한 벡터 및 행렬을 정의한다.

\begin{eqnarray*}
\mathbf{d} &=& \mathbf{1}_{N \times 1}\\
\mathbf{D} &=& \mathbf{y}\mathbf{y}^{\top}\mathbf{X}\mathbf{X}^{\top}
\end{eqnarray*}
where
\begin{eqnarray*}
\mathbf{y} &=& \left[ \begin{array}{c c c c} y_1 & y_2 & \cdots & y_N \end{array} \right]^\top\\
\mathbf{X} &=& \left[ \begin{array}{c c c c} \mathbf{x}_1 & \mathbf{x}_2 & \cdots & \mathbf{x}_N \end{array} \right]^{\top}
\end{eqnarray*}

```{r}
N <- dim(train_df)[1]
X <- train_df[c('x1', 'x2')] %>% as.matrix()
y <- train_df[['class']] %>% as.numeric()

d <- rep(1, N)
D <- (y %*% t(y)) * (X %*% t(X))
```


여기에서 행렬 $\mathbf{D}$의 determinant 값은 0으로, @goldfarb1983numerically 가 가정하는 symmetric positive definite matrix 조건에 위배되어 `solve.QP` 함수 실행 시 오류가 발생한다. 이를 방지하기 위해 아래 예에서는 `Matrix` 패키지의 `nearPD`함수를 이용하여 행렬 $\mathbf{D}$와 근사한 symmetric positive definite matrix를 아래와 같이 찾는다.

```{r}
D_pd <- Matrix::nearPD(D, doSym = T)$mat %>% as.matrix()
```

식 \@ref(eq:quadprog)의 제약식은 모두 inequality 형태로, 식 \@ref(eq:linear-svm-dual)의 equality constraint $\sum_{i = 1}^{N} \alpha_i y_i = 0$를 표현하기 위해서 두 개의 제약식 $\sum_{i = 1}^{N} \alpha_i y_i \ge 0$와 $\sum_{i = 1}^{N} - \alpha_i y_i \ge 0$를 생성한다.

\begin{equation*}
\mathbf{A}^\top = \left[ 
\begin{array}{c c c c}
y_1 & y_2 & \cdots & y_N\\
-y_1 & -y_2 & \cdots & -y_N\\
1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\cdots & \cdots & \cdots & \cdots \\
0 & 0 & \cdots & 1
\end{array}
\right],
\mathbf{b}_0 = \left[ \begin{array}{c}
0 \\ 0 \\ 0 \\ 0 \\ \cdots \\ 0
\end{array}
\right]
\end{equation*}

```{r}
A <- cbind(
  y,
  -y,
  diag(N)
)
b_zero <- rep(0, 2 + N)
```

이제 위에서 구한 행렬과 벡터들을 `solve.QP` 함수에 입력하여 최적해를 구한다.

```{r}
res <- quadprog::solve.QP(D_pd, d, A, b_zero)
alpha_sol <- res$solution
obj_val <- -res$value
```

```{r svm-separable-alpha, echo=FALSE}
knitr::kable(
  tibble(
    variable = paste0("alpha_", 1:N),
    solution = round(res$solution, digits = 4) 
    ),
  booktabs = TRUE,
  align = c('c', 'c'),
  caption = '이차계획문제의 최적해')
```

표 \@ref(tab:svm-separable-alpha)의 결과는 교재[@jun2012datamining]에 나타난 최적해와는 다소 차이가 있으나, 결과적으로 목적함수값은 `r signif(obj_val, digits = 4)`로 동일하다.

위의 과정으로 최적해 $\alpha_{i}^{*}$를 구한 뒤, 아래와 같이 분리 하이퍼플레인의 계수를 결정할 수 있다.

\begin{eqnarray*}
\mathbf{w} &=& \sum_{i = 1}^{N} \alpha_{i}^{*} y_{i} \mathbf{x}_{i}\\
b &=& \sum_{i: \alpha_{i}^{*} > 0} \frac{1 - y_{i} \mathbf{w}^{\top} \mathbf{x}_{i}}{y_{i}} \left/ \sum_{i: \alpha_{i}^{*} > 0} 1 \right. 
\end{eqnarray*}

```{r}
w <- colSums(alpha_sol * y * X)

sv_ind <- which(round(alpha_sol, digits = 4) > 0)
b <- mean((1 - y[sv_ind] * (X[sv_ind, ] %*% w)) / y[sv_ind])
```

위 결과 분리 하이퍼플레인은 교재와 동일하게 얻어진다.


