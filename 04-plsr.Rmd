# 부분최소자승법 {#plsr}

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
```

회귀분석에서와 같이 하나의 종속변수에 영향을 주는 $k$개의 독립변수가 있다고 하자. 모든 변수는 평균조정되었다고 간주한다. 본 장에서 다루고자 하는 부분최소자승법(partial least squares: PLS)는 앞에서 다룬 주성분 회귀분석(PCR)과 유사하나, 도출되는 새로운 잠재변수들이 다르다.

독립변수와 종속변수간의 관계를 설명하기 위해, 우선 독립변수 행렬과 종속변수 행렬(또는 벡터)가 각각 서로 다른 잠재변수들에 의해 설명된다고 가정한 뒤, 두 잠재변수들간의 관계에 대한 모형을 세운다. 이 때, 본 장에서는 두 잠재변수들간의 관계가 선형인 모형(선형 PLS)만을 살펴본다.



## 필요 R 패키지 설치 {#plsr-packages-install}

본 장에서 필요한 R 패키지들은 아래와 같다.

```{r echo=FALSE, warning=FALSE, message=FALSE}
tibble(package = c("tidyverse", 
                   "pls")) %>%
  mutate(version = map_chr(
    package, 
    ~ packageDescription(.x, fields = c("Version"))[1])) %>%
  knitr::kable()
```


## 하나의 종속변수의 경우 {#plsr-single-target}


### 기본 R 스크립트 {#plsr-basic-script}

앞 장의 주성분 회귀분석에서 사용했던 데이터에 대해 부분최소자승 회귀분석을 수행해보도록 하자.

```{r plsr-example-data}
train_df <- tribble(
  ~x1, ~x2, ~x3, ~y,
  -3, -3, 5, -30,
  -2, -3, 7, -20,
  0, 0, 4, 0,
  1, 2, 0, 5,
  2, 2, -5, 10,
  2, 2, -11, 35
)

knitr::kable(
  train_df, booktabs = TRUE,
  align = rep("r", ncol(train_df)),
  caption = "부분최소자승 회귀분석 예제 데이터"
)
```


```{r}
plsr_fit <- pls::plsr(y ~ x1 + x2 + x3, data = train_df, ncomp = 2)
coef(plsr_fit, intercept = TRUE)
```


```{r}
summary(plsr_fit)
```

위 요약표는 하나의 잠재변수와 두 개의 잠재변수를 이용하였을 때 추정된 회귀모형들이 종속변수의 총 변량을 각각 `r 100 * (1 - sum(plsr_fit$residuals[,,1] ^ 2) / sum((plsr_fit$fitted.values[, , 1] + plsr_fit$residuals[, , 1]) ^ 2))`%와 `r 100 * (1 - sum(plsr_fit$residuals[, , 2] ^ 2) / sum((plsr_fit$fitted.values[, , 2] + plsr_fit$residuals[, , 2]) ^ 2))`% 만큼을 설명함을 알려준다. 이는 앞 장에서 살펴보았던 주성분 회귀모형보다 더 높은 수치이다.


### PLS 모형 {#plsr-model}

종속변수가 하나만 존재하는 경우에는 데이터 행렬 $\mathbf{X}$와 종속변수벡터 $\mathbf{y}$가 동일한 잠재변수로 설명된다고 가정할 수 있다. ($n \times k$) 데이터 행렬 $\mathbf{X}$와 종속변수벡터 $\mathbf{y}$에 대하여 동시에 $A$개의 잠재변수벡터 $\mathbf{t}_1, \cdots, \mathbf{t}_A$로 설명하는 모형을 아래와 같이 기술해보자.

\begin{eqnarray}
\mathbf{X} &=& \mathbf{t}_1 \mathbf{p}_1^\top + \mathbf{t}_2 \mathbf{p}_2^\top + \cdots + \mathbf{t}_A \mathbf{p}_A^\top + \mathbf{E} (\#eq:plsr-x-single)\\
\mathbf{y} &=& \mathbf{t}_1 b_1 + \mathbf{t}_2 b_2 + \cdots + \mathbf{t}_A b_A + \mathbf{f} (\#eq:plsr-y-single)
\end{eqnarray}

여기서 계수벡터 $\mathbf{p}_a$는 $\mathbf{X}$에 해당하는 로딩(loading)을, 그리고 계수 $b_a$는 $\mathbf{y}$에 해당하는 로딩을 나타내며, $\mathbf{E}$와 $\mathbf{f}$는 각 모형에 해당하는 오차항(행렬 또는 벡터)이다.


### NIPALS 알고리즘 {#plsr-single-nipals}


- **[단계 0]** 반복알고리즘 수행을 위한 초기화를 한다. $h \leftarrow 1$,  $\mathbf{X}_h \leftarrow \mathbf{X}$, $\mathbf{y}_h \leftarrow \mathbf{y}$.
- **[단계 1]** $\mathbf{X}_h$을 다중종속변수 행렬으로, $\mathbf{y}_h$를 독립변수 벡터로 하는 회귀모형으로부터 기울기 $\mathbf{w}_h = [w_{h1} \, \cdots \, w_{hk}]^\top$를 산출한다.
\[\mathbf{w}_h \leftarrow \left. \mathbf{X}_h^\top \mathbf{y}_h \middle/ \mathbf{y}_h^\top \mathbf{y}_h \right.  \]
- **[단계 2]** 기울기 벡터 $\mathbf{w}_h$의 크기가 1이 되도록 한다. 
\[\left. \mathbf{w}_h \leftarrow \mathbf{w}_h \middle/ \sqrt{\mathbf{w}_h^\top \mathbf{w}_h} \right.\]
- **[단계 3]** 잠재변수 $\mathbf{t}_h$를 행렬 $\mathbf{X}_h$의 각 열의 가중평균으로 구한다. 이 때, 가중치는 기울기 벡터 $\mathbf{w}_h$를 이용한다.
\[\mathbf{t}_h \leftarrow \mathbf{X}_h \mathbf{w}_h\]
- **[단계 4]** 식 \@ref(eq:plsr-x-single)와 같이 $\mathbf{X}_h$을 다중종속변수 행렬으로, $\mathbf{t}_h$를 독립변수 벡터로 하는 회귀모형으로부터 로딩벡터 $\mathbf{p}_h$를 구한다.
\[\mathbf{p}_h \leftarrow \left. \mathbf{X}_h^\top \mathbf{t}_h \middle/ \mathbf{t}_h^\top \mathbf{t}_h \right.\]
- **[단계 5]** 로딩벡터 $\mathbf{p}_h$의 크기를 1로 조정하고, 잠재변수 벡터 $\mathbf{t}_h$와 기울기 벡터 $\mathbf{w}_h$의 크기를 그에 따라 보정한다.
\[c \leftarrow \sqrt{\mathbf{p}_h^\top \mathbf{p}_h}, \, \mathbf{t}_h \leftarrow \mathbf{t}_h c, \, \mathbf{w}_h \leftarrow \mathbf{w}_h c, \, \mathbf{p}_h \leftarrow \frac{1}{c} \mathbf{p}_h \]
- **[단계 6]** 식 \@ref(eq:plsr-y-single)와 같이 잠재변수 $\mathbf{t}_h$를 종속변수 $\mathbf{y}_h$에 회귀시킬 때 계수 $b_h$를 산출한다.
\[b_h \leftarrow \left. \mathbf{y}_h^\top \mathbf{t}_h \middle/ \mathbf{t}_h^\top \mathbf{t}_h \right. \]
- **[단계 7]** 독립변수 행렬 $\mathbf{X}_h$와 종속변수벡터 $\mathbf{y}_h$로부터 새로 얻어진 잠재변수 벡터 $\mathbf{t}_h$가 설명하는 부분을 제거하고 나머지 변동만을 담은 독립변수 행렬 $\mathbf{X}_{h + 1}$과 종속변수벡터 $\mathbf{y}_{h + 1}$을 구한다.
\[\mathbf{X}_{h + 1} \leftarrow \mathbf{X}_h - \mathbf{t}_h \mathbf{p}_h^\top, \, \mathbf{y}_{h + 1} \leftarrow \mathbf{y}_h - \mathbf{t}_h b_h\]
- **[단계 8]** $h \leftarrow h + 1$로 업데이트하고, [단계 1]로 돌아간다. [단계 1] - [단계 8]의 과정을 $A$개의 잠재변수를 얻을 때까지 반복한다.


위 NIPALS 알고리즘을 아래 `nipals_plsr`이라는 함수로 구현해보자. 이 때, 함수의 입력변수는 아래와 같다.

- `X`: 평균조정된 ($n \times k$) 행렬
- `y`: 평균조정된 종속변수 벡터
- `A`: 잠재변수 개수

```{r}
nipals_plsr <- function(X, y, A = NULL) {
  if (is_empty(A) || (A > min(dim(X)))) {
    A <- min(dim(X))
  }
  
  Th <- matrix(NA, nrow = nrow(X), ncol = A)
  Wh <- matrix(NA, nrow = ncol(X), ncol = A)
  Ph <- matrix(NA, nrow = ncol(X), ncol = A)
  bh <- matrix(NA, nrow = 1L, ncol = A)
  
  for (h in seq_len(A)) {
    # 단계 1
    Wh[, h] <- coef(lm(X ~ -1 + y))
    
    # 단계 2
    Wh[, h] <- Wh[, h] / sqrt(sum(Wh[, h] ^ 2))

    # 단계 3
    Th[, h] <- X %*% Wh[, h]
    
    # 단계 4
    Ph[, h] <- coef(lm(X ~ -1 + Th[, h]))
    
    # 단계 5
    p_size <- sqrt(sum(Ph[, h] ^ 2))
    Th[, h] <- Th[, h] * p_size
    Wh[, h] <- Wh[, h] * p_size
    Ph[, h] <- Ph[, h] / p_size
    
    # 단계 6
    bh[, h] <- coef(lm(y ~ -1 + Th[, h]))

    # 단계 7
    X <- X - Th[, h] %*% t(Ph[, h])
    y <- y - Th[, h] %*% t(bh[, h])
  }
  
  return(list(T = Th, W = Wh, P = Ph, b = bh))
}

X <- as.matrix(train_df[, c("x1", "x2", "x3")])
y <- train_df$y
nipals_fit <- nipals_plsr(X, y, A = 2)
nipals_fit
```


위 결과를 원래 독립변수 $X$와 종속변수 와 $y$에 대한 식으로 변환하는 식은 아래와 같다.

\begin{equation}
\begin{split}

\end{split}
\end{equation}


## 다수의 종속변수의 경우 {#plsr-multivariate-target}





